[{"content":"はじめに みなさん、こんにちは。今回はさまざまなAWSサービスをKubernetesから管理できるようにするAWS Controllers for Kubernetesのお話です。\nみなさんはAmazon EKSを活用してKubernetesクラスタをAWS上で動かすとなった際に、他のマネージドサービスの利用はどうされていますか。もちろんすべてKubernetes上で動かしてシステムを完結させるという選択肢もあるかと思いますが、やはり多くの方が他のAWSのマネージドサービスの併用も検討されるのではないでしょうか。その一方で、これら併用環境のコード化 (IaC、Infrastructure as Code) を実現しようとすると、Kubernetesアプリケーションの管理はHelm、AWSリソースの管理はTerraform、などという別々のツールでの管理になってしまいがちです。\nそんな悩みを解決する1つの手段がAWS Load Balancer ControllerやAWS Controllers for KubernetesといったKubernetesクラスタ機能を拡張する各種コントローラの活用です。これらのコントローラを利用することで、AWSリソースについてもKubernetesマニフェストファイルで定義できるようになり、Kubernetes側に運用管理を寄せてシンプル化することが可能です。\n今回はそのうちの1つ、さまざまなAWSサービスを管理できるようにするAWS Controllers for Kubernetesについて、簡単なサンプルを交えて紹介していきたいと思います。これからAmazon EKS上にアプリケーションを展開しようと考えている方は参考にしてみてはいかがでしょうか。\nAWS Controllers for Kubernetesとは AWS Controllers for Kubernetesは、さまざまなAWSサービスをKubernetesクラスタから管理するためのKubernetes API拡張コントローラ群の総称です。このコントローラ群を活用することで、Kubernetesクラスタから直接AWSサービスの定義、作成を行うことが可能になり、アプリケーションとその依存関係にあるデータベース、メッセージキュー、オブジェクトストレージなどのマネージドサービスを含むすべてをKubernetesにて一元管理することが可能となります。なお、現時点のAWS Controllers for Kubernetesでは、次のAWSサービス向けコントローラがディベロッパープレビュー機能として利用可能となっています。\n Amazon API Gateway V2 Amazon Application Auto Scaling Amazon DynamoDB Amazon ECR Amazon EKS Amazon ElastiCache Amazon EC2 Amazon MQ Amazon OpenSearch Service Amazon RDS Amazon SageMaker Amazon SNS AWS Step Functions Amazon S3  https://github.com/aws-controllers-k8s/community\nAWS Controllers for Kubernetesを導入してみよう Step1. 作業環境の設定 今回はAmazon EKSやAWS Controllers for Kubernetesの管理を行う環境としてAWS CloudShellを利用していきたいと思います。まずは操作に必要な各種ツールの設定をAWS CloudShellにしてきましょう。\nAWS CLIの設定 AWSリソースの操作を行えるように次のコマンドを実行し、AWS CLIの設定を行いましょう。\nAWS CLIの設定 aws configure  kubectlコマンドのインストール 次にKubernetes管理ツールのkubectlコマンドをインストールしましょう。\nhttps://docs.aws.amazon.com/ja_jp/eks/latest/userguide/install-kubectl.html#linux\nkubectlコマンドのインストール # kubectl コマンドのダウンロード curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.21.2/2021-07-05/bin/linux/amd64/kubectl  # 実行権限の付与 chmod +x kubectl  # 実行ファイルのパスを設定 mkdir -p ${HOME}/bin \u0026amp;\u0026amp; mv kubectl ${HOME}/bin \u0026amp;\u0026amp; export PATH=${PATH}:${HOME}/bin  # シェルの起動時に $HOME/bin をパスへ追加 echo \u0026#39;export PATH=${PATH}:${HOME}/bin\u0026#39; \u0026gt;\u0026gt; ~/.bashrc  # インストールが成功していることを確認 kubectl version --short --client  インストールに成功していれば出力例のようにバージョン情報の出力を確認できます。\n出力例 $ kubectl version --short --client Client Version: v1.21.2-13+d2965f0db10712  eksctlコマンドのインストール 続いてAmazon EKS管理ツールのeksctlコマンドをインストールしましょう。\nhttps://docs.aws.amazon.com/ja_jp/eks/latest/userguide/eksctl.html#linux\neksctlコマンドのインストール # eksctl の最新バージョンをダウンロード curl -L \u0026#34;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; | tar xz -C /tmp  # 実行ファイルをパスの通ったディレクトリへ移動 mv /tmp/eksctl ${HOME}/bin  # インストールが成功していることを確認 eksctl version  インストールに成功していれば出力例のようにバージョン情報の出力を確認できます。\n出力例 $ eksctl version 0.83.0  helmコマンドのインストール 最後にKubernetes上で稼働するアプリケーションを管理するためのツールであるhelmコマンドをインストールしましょう。\nhttps://docs.aws.amazon.com/ja_jp/eks/latest/userguide/helm.html\nhelmコマンドのインストール # 前提パッケージのインストール sudo yum install -y openssl  # インストールスクリプトのダウンロード curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 \u0026gt; get_helm.sh  # 実行権限の付与 chmod 700 get_helm.sh  # インストールスクリプトの実行 ./get_helm.sh  # インストールが成功していることを確認 helm version --short  インストールに成功していれば出力例のようにバージョン情報の出力を確認できます。\n出力例 $ helm version --short v3.8.0+gd141386  以上で作業環境(AWS CloudShell)の設定は完了です。\nStep2. EKSクラスタの作成 続いてAWS Controllers for Kubernetesを導入する対象のAmazon EKSクラスタを作成していきましょう。今回はあくまで検証なのでeksctlコマンドでサクッと作成していきましょう。\nhttps://docs.aws.amazon.com/ja_jp/eks/latest/userguide/getting-started-eksctl.html\nEKSクラスタの作成 # 環境変数の設定 export CLUSTER=\u0026#34;matt-tokyo-cluster\u0026#34;  # EKS クラスタの作成 eksctl create cluster --name ${CLUSTER} --version 1.21  # サービスアカウントでの IAM ロール使用を許可 eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER} --approve  ここまで終わりましたらAWS Controllers for Kubernetesを利用するための事前準備は完了です。\nStep3. コントローラのデプロイ それではAWS Controllers for KubernetesをAmazon EKSクラスタにデプロイしていきましょう。Amazon ECRパブリックギャラリーにて各AWSサービスに対応するコントローラ用Helmチャートが公開されておりますのでこちらを利用してデプロイしていきたいと思います。\nhttps://gallery.ecr.aws/aws-controllers-k8s\nインストールスクリプトの作成 AWS Controllers for Kubernetesは各AWSサービスに対応するコントローラをそれぞれ導入し、サービスアカウントの設定をしていく必要があるのですが、公式ドキュメントを見るとかなり煩雑な手順になっていることがわかります。そこでコマンド1つでインストールできるようにスクリプト化してみました。\nhttps://aws-controllers-k8s.github.io/community/docs/user-docs/install\nwarn 再実行を想定してない作りになっているのでご注意ください。\n install.sh #!/bin/bash set -eux  # 引数の確認 if [ $# -eq 0 ] \u0026amp;\u0026amp; [ -z \u0026#34;${SERVICE}\u0026#34; ]; then  echo \u0026#34;Error: usage: ./install.sh \u0026lt;SERVICE_NAME\u0026gt;\u0026#34;  exit 1 elif [ $# -eq 1 ] \u0026amp;\u0026amp; [ -n \u0026#34;$1\u0026#34; ]; then  SERVICE=$1 fi  # 環境変数の設定 export HELM_EXPERIMENTAL_OCI=1 CHART_EXPORT_PATH=\u0026#34;/tmp/chart\u0026#34; ACK_K8S_NAMESPACE=${ACK_K8S_NAMESPACE:-\u0026#34;ack-system\u0026#34;} ACK_SERVICE_CONTROLLER=\u0026#34;ack-${SERVICE}-controller\u0026#34; AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \u0026#34;Account\u0026#34; --output text) CLUSTER=${CLUSTER:-\u0026#34;my-tokyo-cluster\u0026#34;} OIDC_PROVIDER=$(aws eks describe-cluster --name ${CLUSTER} \\  --query \u0026#34;cluster.identity.oidc.issuer\u0026#34; --output text \\  | sed -e \u0026#34;s/^https:\\/\\///\u0026#34;)  # 最新リリースバージョン名の取得 RELEASE_VERSION=$(curl -sL \\  https://api.github.com/repos/aws-controllers-k8s/${SERVICE}-controller/releases/latest \\  | grep \u0026#39;\u0026#34;tag_name\u0026#34;:\u0026#39; | cut -d\u0026#39;\u0026#34;\u0026#39; -f4)  if [ -z \u0026#34;${RELEASE_VERSION}\u0026#34; ]; then  # latestリリースが作成されていない場合は一覧から最新バージョンを取得  RELEASE_VERSION=$(curl -sL \\  https://api.github.com/repos/aws-controllers-k8s/${SERVICE}-controller/releases \\  | grep \u0026#39;\u0026#34;tag_name\u0026#34;:\u0026#39; | head -n 1 | cut -d\u0026#39;\u0026#34;\u0026#39; -f4)   # リリースを何も作成されていない場合はv0.0.1を設定(SNSコントローラ向け)  RELEASE_VERSION=${RELEASE_VERSION:-v0.0.1} fi  # Helmチャートのダウンロードディレクトリの作成 mkdir -p ${CHART_EXPORT_PATH}  # Helmチャートのダウンロード helm pull oci://public.ecr.aws/aws-controllers-k8s/${SERVICE}-chart \\  --version $RELEASE_VERSION -d ${CHART_EXPORT_PATH}  # アーカイブの解凍 tar xvf ${CHART_EXPORT_PATH}/${SERVICE}-chart-${RELEASE_VERSION}.tgz \\  -C ${CHART_EXPORT_PATH}  # 解凍後のパスチェック if [ -d ${CHART_EXPORT_PATH}/${SERVICE}-chart ]; then  CHART_PATH=${CHART_EXPORT_PATH}/${SERVICE}-chart else  # SNSコントローラ向け  CHART_PATH=${CHART_EXPORT_PATH}/${ACK_SERVICE_CONTROLLER} fi  # コントローラのインストール helm install --create-namespace ${ACK_SERVICE_CONTROLLER} \\  --namespace ${ACK_K8S_NAMESPACE} \\  --set aws.region=\u0026#34;${AWS_REGION}\u0026#34; ${CHART_PATH}  # IAMロール定義ファイルの作成 cat \u0026lt;\u0026lt;EOF \u0026gt; /tmp/${ACK_SERVICE_CONTROLLER}.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Federated\u0026#34;: \u0026#34;arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;${OIDC_PROVIDER}:sub\u0026#34;: \u0026#34;system:serviceaccount:${ACK_K8S_NAMESPACE}:${ACK_SERVICE_CONTROLLER}\u0026#34; } } } ] } EOF  # IAMロールの作成 aws iam create-role \\  --role-name ${ACK_SERVICE_CONTROLLER} \\  --assume-role-policy-document file:///tmp/${ACK_SERVICE_CONTROLLER}.json  # 推奨IAMポリシーの設定 POLICY_ARN_STRINGS=$(curl -sL \\  https://raw.githubusercontent.com/aws-controllers-k8s/${SERVICE}-controller/main/config/iam/recommended-policy-arn)  if [ \u0026#34;404: Not Found\u0026#34; != \u0026#34;${POLICY_ARN_STRINGS}\u0026#34; ]; then  while IFS= read -r POLICY_ARN; do  aws iam attach-role-policy \\  --role-name \u0026#34;${ACK_SERVICE_CONTROLLER}\u0026#34; \\  --policy-arn \u0026#34;${POLICY_ARN}\u0026#34;  done \u0026lt;\u0026lt;\u0026lt; \u0026#34;${POLICY_ARN_STRINGS}\u0026#34; fi  # 推奨IAMポリシーの設定(EKSコントローラ向け) INLINE_POLICY=$(curl -sL \\  https://raw.githubusercontent.com/aws-controllers-k8s/${SERVICE}-controller/main/config/iam/recommended-inline-policy)  if [ \u0026#34;404: Not Found\u0026#34; != \u0026#34;${INLINE_POLICY}\u0026#34; ]; then  aws iam put-role-policy \\  --role-name \u0026#34;${ACK_SERVICE_CONTROLLER}\u0026#34; \\  --policy-name \u0026#34;ack-recommended-policy\u0026#34; \\  --policy-document \u0026#34;${INLINE_POLICY}\u0026#34; fi  # サービスアカウントにIAMロールを関連付け kubectl -n ${ACK_K8S_NAMESPACE} annotate serviceaccount \\  ${ACK_SERVICE_CONTROLLER} \\  eks.amazonaws.com/role-arn=$(aws iam get-role \\  --role-name=${ACK_SERVICE_CONTROLLER} --query Role.Arn --output text)  # Deploymentリソースを再起動 kubectl -n ${ACK_K8S_NAMESPACE} rollout restart deployment \\  $(kubectl -n ${ACK_K8S_NAMESPACE} get deployment \\  -o custom-columns=Name:metadata.name --no-headers \\  | grep ${ACK_SERVICE_CONTROLLER})  コントローラのインストール それではコントローラを導入していきたいと思います。本記事では例としてAmazon Simple Storage Service(S3)サービスに対応したコントローラをインストールしていきたいと思います。\nコントローラのインストール(S3の場合) # 環境変数の設定 export SERVICE=\u0026#34;s3\u0026#34; export ACK_K8S_NAMESPACE=\u0026#34;ack-system\u0026#34; export ACK_SERVICE_CONTROLLER=\u0026#34;ack-${SERVICE}-controller\u0026#34;  # コントローラのインストール bash install.sh  # 確認 helm list -n ${ACK_K8S_NAMESPACE} -o yaml -f ${ACK_SERVICE_CONTROLLER}  インストールに成功していれば出力例のようにコントローラが表示されるようになります。\n出力例 $ helm list -n ${ACK_K8S_NAMESPACE} -o yaml -f ${ACK_SERVICE_CONTROLLER} - app_version: v0.0.13  chart: s3-chart-v0.0.13  name: ack-s3-controller  namespace: ack-system  revision: \u0026#34;1\u0026#34;  status: deployed  updated: 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC  なお、今回紹介したS3以外のコントローラを導入する際は、環境変数SERVICEの値を各 AWS サービスに対応する文字列へ置換することでインストールすることが可能です。各サービスに対応する文字列は次の通りです。\n   AWS サービス名 SERVICE 値     Amazon API Gateway V2 apigatewayv2   Amazon Application Auto Scaling applicationautoscaling   Amazon DynamoDB dynamodb   Amazon ECR ecr   Amazon EKS eks   Amazon ElastiCache elasticache   Amazon EC2 ec2   Amazon MQ mq   Amazon OpenSearch Service opensearchservice   Amazon RDS rds   Amazon SageMaker sagemaker   Amazon SNS sns   AWS Step Functions sfn   Amazon S3 s3    ちなみに、全部入れるとこのような感じでコントローラだらけになってしまいます^^;\n出力例 $ helm list -n ${ACK_K8S_NAMESPACE} NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION ack-apigatewayv2-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed apigatewayv2-chart-v0.0.15 v0.0.15 ack-applicationautoscaling-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed applicationautoscaling-chart-v0.2.4 v0.2.4 ack-dynamodb-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed dynamodb-chart-v0.0.14 v0.0.14 ack-ec2-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed ec2-chart-v0.0.7 v0.0.7 ack-ecr-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed ecr-chart-v0.0.19 v0.0.19 ack-eks-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed eks-chart-v0.0.8 v0.0.8 ack-elasticache-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed elasticache-chart-v0.0.14 v0.0.14 ack-mq-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed mq-chart-v0.0.12 v0.0.12 ack-opensearchservice-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed opensearchservice-chart-v0.0.9 v0.0.9 ack-rds-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed rds-chart-v0.0.17 v0.0.17 ack-s3-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed s3-chart-v0.0.13 v0.0.13 ack-sagemaker-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed sagemaker-chart-v0.3.0 v0.3.0 ack-sfn-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed sfn-chart-v0.0.11 v0.0.11 ack-sns-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed ack-sns-controller-v0.0.1 v0.0.1  AWS Controllers for Kubernetesを使ってみよう 今回は一部のAWSサービス向けコントローラを簡単なサンプルを用いて使い方を紹介していきたいと思います。すべてのコントローラの使い方については公式ドキュメントを参照ください。\nhttps://aws-controllers-k8s.github.io/community/reference/\nAmazon ECRコントローラの使い方 AWS Controllers for Kubernetes環境では、次のサンプルのようにKubernetes Repositoryリソースを定義をすることによってECRリポジトリをプロビジョニングできます。定義可能なスペックの詳細については次の公式リファレンスドキュメントを参照ください。\nhttps://aws-controllers-k8s.github.io/community/reference/ecr/v1alpha1/repository/\nrepository.yaml（ECRリポジトリリソース定義サンプル） apiVersion: ecr.services.k8s.aws/v1alpha1 kind: Repository metadata:  name: \u0026#34;matt-ecr-repository\u0026#34; spec:  name: \u0026#34;matt-ecr-repository\u0026#34;  imageScanningConfiguration:  scanOnPush: true  それでは次のコマンドを実行してサンプルECRリポジトリリソースをデプロイしていきましょう。\nサンプルECRリポジトリのデプロイ export NAMESPACE=\u0026#34;sample\u0026#34;  # サンプルECRリポジトリ用Namespaceの作成 kubectl create namespace ${NAMESPACE}  # サンプルECRリポジトリのデプロイ kubectl apply -n ${NAMESPACE} -f repository.yaml  # サンプルECRリポジトリがデプロイされたことを確認 kubectl get -n ${NAMESPACE} repository  ECRリポジトリリソースのデプロイに成功した場合は次の出力例のようにRepositoryリソース一覧に表示されます。\n出力例 $ kubectl get -n ${NAMESPACE} repository NAME AGE matt-ecr-repository 10s  念のため、AWS CLIを実行してECRリポジトリが作成されているかを確認してみましょう。\nECRリポジトリの確認 aws ecr describe-repositories --repository-name matt-ecr-repository  AWS CLIでもECRリポジトリが確認できたら成功です。\n出力例 $ aws ecr describe-repositories --repository-name matt-ecr-repository repositories: - createdAt: \u0026#39;2022-02-xxTxx:xx:xx+00:00\u0026#39;  encryptionConfiguration:  encryptionType: AES256  imageScanningConfiguration:  scanOnPush: true  imageTagMutability: MUTABLE  registryId: \u0026#39;xxxxxxxxxxxx\u0026#39;  repositoryArn: arn:aws:ecr:ap-northeast-1:xxxxxxxxxxxx:repository/matt-ecr-repository  repositoryName: matt-ecr-repository  repositoryUri: xxxxxxxxxxxx.dkr.ecr.ap-northeast-1.amazonaws.com/matt-ecr-repository  最後にサンプルを削除して動作確認は終了です。お疲れ様でした。\nサンプルの削除 kubectl delete namespace ${NAMESPACE}  Amazon S3コントローラの使い方 AWS Controllers for Kubernetes環境では、次のサンプルのようにKubernetes Bucketリソースを定義をすることによってS3バケットをプロビジョニングできます。定義可能なスペックの詳細については次の公式リファレンスドキュメントを参照ください。\nhttps://aws-controllers-k8s.github.io/community/reference/s3/v1alpha1/bucket/\nbucket.yaml（S3バケットリソース定義サンプル） apiVersion: s3.services.k8s.aws/v1alpha1 kind: Bucket metadata:  name: \u0026#34;matt-s3-bucket\u0026#34; spec:  name: \u0026#34;matt-s3-bucket\u0026#34;  publicAccessBlock:  blockPublicACLs: true  blockPublicPolicy: true  versioning:  status: Enabled  encryption:  rules:  - bucketKeyEnabled: false  applyServerSideEncryptionByDefault:  sseAlgorithm: AES256  それでは次のコマンドを実行してサンプルS3バケットリソースをデプロイしていきましょう。\nサンプルS3バケットのデプロイ export NAMESPACE=\u0026#34;sample\u0026#34;  # サンプルS3バケット用Namespaceの作成 kubectl create namespace ${NAMESPACE}  # サンプルS3バケットのデプロイ kubectl apply -n ${NAMESPACE} -f bucket.yaml  # サンプルS3バケットがデプロイされたことを確認 kubectl get -n ${NAMESPACE} bucket  S3バケットリソースのデプロイに成功した場合は次の出力例のようにBucketリソース一覧に表示されます。\n出力例 $ kubectl get -n ${NAMESPACE} bucket NAME AGE matt-s3-bucket 1m22s  念のため、AWS CLIを実行してS3バケットが作成されているかを確認してみましょう。\nS3バケットが作成されているかを確認 aws s3 ls | grep \u0026#34;matt-s3-bucket\u0026#34;  AWS CLIでもS3バケットが確認できたら成功です。\n出力例 $ aws s3 ls | grep \u0026#34;matt-s3-bucket\u0026#34; 2022-02-xx xx:xx:xx matt-s3-bucket  最後にサンプルを削除して動作確認は終了です。お疲れ様でした。\nサンプルの削除 kubectl delete namespace ${NAMESPACE}  終わりに 今回はさまざまなAWSサービスをAmazon Elastic Kubernetes Service(EKS)上で管理できるようにするAWS Controllers for Kubernetesのご紹介でしたがいかがだったでしょうか。\n現時点ではディベロッパープレビュー機能のためプロダクション用途での活用はまだまだオススメできませんが、もしAWS Controllers for Kubernetesの活用を検討されている方は参考にしていただければ幸いです。\n以上、さまざまなAWSサービスをKubernetesから管理できるようにする「AWS Controllers for Kubernetes」のご紹介でした。\n  AWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 Kubernetes は、The Linux Foundation の米国およびその他の国における登録商標または商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。  ","permalink":"https://chacco38.github.io/posts/2022/02/aws-controllers-for-kubernetes/","summary":"はじめに みなさん、こんにちは。今回はさまざまなAWSサービスをKubernetesから管理できるようにするAWS Controllers for Kubernetesのお話です。\nみなさんはAmazon EKSを活用してKubernetesクラスタをAWS上で動かすとなった際に、他のマネージドサービスの利用はどうされていますか。もちろんすべてKubernetes上で動かしてシステムを完結させるという選択肢もあるかと思いますが、やはり多くの方が他のAWSのマネージドサービスの併用も検討されるのではないでしょうか。その一方で、これら併用環境のコード化 (IaC、Infrastructure as Code) を実現しようとすると、Kubernetesアプリケーションの管理はHelm、AWSリソースの管理はTerraform、などという別々のツールでの管理になってしまいがちです。\nそんな悩みを解決する1つの手段がAWS Load Balancer ControllerやAWS Controllers for KubernetesといったKubernetesクラスタ機能を拡張する各種コントローラの活用です。これらのコントローラを利用することで、AWSリソースについてもKubernetesマニフェストファイルで定義できるようになり、Kubernetes側に運用管理を寄せてシンプル化することが可能です。\n今回はそのうちの1つ、さまざまなAWSサービスを管理できるようにするAWS Controllers for Kubernetesについて、簡単なサンプルを交えて紹介していきたいと思います。これからAmazon EKS上にアプリケーションを展開しようと考えている方は参考にしてみてはいかがでしょうか。\nAWS Controllers for Kubernetesとは AWS Controllers for Kubernetesは、さまざまなAWSサービスをKubernetesクラスタから管理するためのKubernetes API拡張コントローラ群の総称です。このコントローラ群を活用することで、Kubernetesクラスタから直接AWSサービスの定義、作成を行うことが可能になり、アプリケーションとその依存関係にあるデータベース、メッセージキュー、オブジェクトストレージなどのマネージドサービスを含むすべてをKubernetesにて一元管理することが可能となります。なお、現時点のAWS Controllers for Kubernetesでは、次のAWSサービス向けコントローラがディベロッパープレビュー機能として利用可能となっています。\n Amazon API Gateway V2 Amazon Application Auto Scaling Amazon DynamoDB Amazon ECR Amazon EKS Amazon ElastiCache Amazon EC2 Amazon MQ Amazon OpenSearch Service Amazon RDS Amazon SageMaker Amazon SNS AWS Step Functions Amazon S3  https://github.","title":"AWS Controllers for Kubernetesを使って各種AWSサービスをマニフェストファイルで管理しよう"},{"content":"はじめに みなさん、こんにちは。以前に「複数リージョンのGKEクラスタとAnthos Service Meshでマルチクラスタメッシュ環境を構築してみた」という記事を書いたのですが、今回はその環境をTerraformを使って構築してみました。もしこれから「ASM環境をTerraformで」と検討している方は参考にしてみてはいかがでしょうか。\nとはいえ、本記事の執筆時点(2022年1月末)ではTerraform公式モジュールがASMのv1.11以降に対応しておらず正直使いモノにならなかったこともあり、やや苦しい実装になってしまっています。素直にASMの導入以降はTerraform以外を使うのが良いかと思いますが、あくまで本記事はご参考ということでその点ご承知おきいただけると幸いです。\n構築するシステムについて 次の図に示すように限定公開クラスを有効化した複数リージョンのGKEクラスタに対してAnthos Service Mesh(マネージドコントロールプレーン)を導入した環境となっています。なお、アプリケーションのコンテナについてはインフラとは異なるリポジトリで管理するのが一般的かと思うので今回は除外しています。\nTerraformのサンプルコードを書いてみた それでは今回作成したTerraformのサンプルコードを紹介していきたいと思います。まずはディレクトリ構造ですが、今回はenvironmentsディレクトリ配下へ環境ごとにサブディレクトリを作成し、Workspaceは使わずに別ファイルとして管理する形を想定した作りにしてます。\nディレクトリ構成 . |-- environments | `-- poc | |-- backend.tf | |-- main.tf | `-- variables.tf `-- modules  |-- networks  | |-- main.tf  | |-- variables.tf  | `-- outputs.tf  |-- gke  | |-- main.tf  | |-- variables.tf  | `-- outputs.tf  `-- asm  |-- main.tf  |-- variables.tf  |-- scripts  | |-- install.sh  | |-- destroy.sh  | `-- create-mesh.sh  `-- manifests  |-- istio-ingressgateway-pods  | |-- namespace.yaml  | |-- deployment.yaml  | |-- serviceaccount.yaml  | `-- role.yaml  `-- istio-ingressgateway-services  |-- multiclusterservice.yaml  |-- backendconfig.yaml  `-- multiclusteringress.yaml     No. ファイル名 概要     1 environments/poc/backend.tf PoC環境のtfstateファイル保存先定義   2 environments/poc/main.tf PoC環境の定義   3 environments/pod/variables.tf PoC環境の外部変数定義   4 modules/networks/main.tf ネットワーク設定用モジュールの定義   5 modules/networks/variables.tf ネットワーク設定用モジュールの外部変数定義   6 modules/networks/outputs.tf ネットワーク設定用モジュールのアウトプット定義   7 modules/gke/main.tf GKE設定用モジュールの定義   8 modules/gke/variables.tf GKE設定用モジュールの外部変数定義   9 modules/gke/outputs.tf GKE設定用モジュールのアウトプット定義   10 modules/asm/main.tf ASM設定用モジュールの定義   11 modules/asm/variables.tf ASM設定用モジュールの外部変数定義   12 modules/asm/scripts/install.sh ASMのインストールスクリプト   13 modules/asm/scripts/destroy.sh ASMのアンインストールスクリプト   14 modules/asm/scripts/create-mesh.sh ASMのマルチクラスタメッシュ作成スクリプト   15 modules/asm/manifests/istio-ingressgateway-pods/* Istio IngressGatewayコンテナのKubernetesマニフェストファイル群   16 modules/asm/manifests/istio-ingressgateway-services/* Istio IngressGatewayサービスのKubernetesマニフェストファイル群    PoC環境定義 environments/poc/backend.tf PoC環境のtfstateファイルをGoogle Cloud Storage(GCS)上で管理するための設定を定義しています。\n./environments/poc/backend.tf terraform {  backend \u0026#34;gcs\u0026#34; {  bucket = \u0026#34;matt-gcs-tfstate\u0026#34;  prefix = \u0026#34;multi-asm-poc\u0026#34;  } }  environments/poc/main.tf PoC環境の定義をしています。実際の処理はモジュール側で定義しており、このファイルではPoC環境固有の設定値定義がメインの役割となっています。\n./environments/poc/main.tf locals {  network = \u0026#34;matt-vpc\u0026#34;   tokyo_subnet = \u0026#34;matt-tokyo-priv-snet\u0026#34;  tokyo_subnet_ip_range = \u0026#34;172.16.0.0/16\u0026#34;  tokyo_router = \u0026#34;matt-tokyo-router\u0026#34;  tokyo_nat = \u0026#34;matt-tokyo-nat\u0026#34;   osaka_subnet = \u0026#34;matt-osaka-priv-snet\u0026#34;  osaka_subnet_ip_range = \u0026#34;172.24.0.0/16\u0026#34;  osaka_router = \u0026#34;matt-osaka-router\u0026#34;  osaka_nat = \u0026#34;matt-osaka-nat\u0026#34;   tokyo_cluster = \u0026#34;matt-tokyo-cluster-1\u0026#34;  tokyo_master_ip_range = \u0026#34;192.168.0.0/28\u0026#34;  tokyo_pod_ip_range = \u0026#34;10.16.0.0/14\u0026#34;  tokyo_service_ip_range = \u0026#34;10.20.0.0/20\u0026#34;   osaka_cluster = \u0026#34;matt-osaka-cluster-1\u0026#34;  osaka_master_ip_range = \u0026#34;192.168.8.0/28\u0026#34;  osaka_pod_ip_range = \u0026#34;10.32.0.0/14\u0026#34;  osaka_service_ip_range = \u0026#34;10.36.0.0/20\u0026#34; } module \u0026#34;networks\u0026#34; {  source = \u0026#34;../../modules/networks\u0026#34;   project_id = var.project_id  network = local.network   tokyo_subnet = local.tokyo_subnet  tokyo_subnet_ip_range = local.tokyo_subnet_ip_range  tokyo_subnet_2nd_ip_range_1 = local.tokyo_pod_ip_range  tokyo_subnet_2nd_ip_range_2 = local.tokyo_service_ip_range  tokyo_router = local.tokyo_router  tokyo_nat = local.tokyo_nat   osaka_subnet = local.osaka_subnet  osaka_subnet_ip_range = local.osaka_subnet_ip_range  osaka_subnet_2nd_ip_range_1 = local.osaka_pod_ip_range  osaka_subnet_2nd_ip_range_2 = local.osaka_service_ip_range  osaka_router = local.osaka_router  osaka_nat = local.osaka_nat } module \u0026#34;gke\u0026#34; {  source = \u0026#34;../../modules/gke\u0026#34;   project_id = var.project_id  network = module.networks.network   tokyo_cluster = local.tokyo_cluster  tokyo_subnet = local.tokyo_subnet  tokyo_master_ip_range = local.tokyo_master_ip_range   osaka_cluster = local.osaka_cluster  osaka_subnet = local.osaka_subnet  osaka_master_ip_range = local.osaka_master_ip_range } module \u0026#34;asm\u0026#34; {  source = \u0026#34;../../modules/asm\u0026#34;   project_id = var.project_id  network = module.networks.network   tokyo_cluster = module.gke.tokyo_cluster  tokyo_pod_ip_range = local.tokyo_pod_ip_range   osaka_cluster = module.gke.osaka_cluster  osaka_pod_ip_range = local.osaka_pod_ip_range }  environments/pod/variables.tf terraform plan/apply コマンド実行時に -var=\u0026quot;project_id=${PROJECT_ID}\u0026quot; のような形で外部から与える変数を定義しています。\n./environments/poc/variables.tf variable \u0026#34;project_id\u0026#34; {}  ネットワークモジュール定義 modules/networks/main.tf ネットワーク設定としてVPCおよびCloud NATの定義をしています。今回の例ではTerraform公式モジュールを活用してみました。\n./modules/networks/main.tf module \u0026#34;vpc\u0026#34; {  source = \u0026#34;terraform-google-modules/network/google\u0026#34;  version = \u0026#34;4.1.0\u0026#34;  description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/network/google/4.1.0\u0026#34;   project_id = var.project_id  network_name = var.network  shared_vpc_host = false   subnets = [  {  subnet_name = var.tokyo_subnet  subnet_ip = var.tokyo_subnet_ip_range  subnet_region = \u0026#34;asia-northeast1\u0026#34;  subnet_private_access = true  },  {  subnet_name = var.osaka_subnet  subnet_ip = var.osaka_subnet_ip_range  subnet_region = \u0026#34;asia-northeast2\u0026#34;  subnet_private_access = true  }  ]   secondary_ranges = {  (var.tokyo_subnet) = [  {  range_name = \u0026#34;${var.tokyo_subnet}-pods\u0026#34;  ip_cidr_range = var.tokyo_subnet_2nd_ip_range_1  },  {  range_name = \u0026#34;${var.tokyo_subnet}-services\u0026#34;  ip_cidr_range = var.tokyo_subnet_2nd_ip_range_2  },  ]   (var.osaka_subnet) = [  {  range_name = \u0026#34;${var.osaka_subnet}-pods\u0026#34;  ip_cidr_range = var.osaka_subnet_2nd_ip_range_1  },  {  range_name = \u0026#34;${var.osaka_subnet}-services\u0026#34;  ip_cidr_range = var.osaka_subnet_2nd_ip_range_2  },  ]  } } module \u0026#34;cloud_router_tokyo\u0026#34; {  source = \u0026#34;terraform-google-modules/cloud-router/google\u0026#34;  version = \u0026#34;1.3.0\u0026#34;  description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/cloud-router/google/1.3.0\u0026#34;   name = var.tokyo_router  project = var.project_id  region = \u0026#34;asia-northeast1\u0026#34;  network = module.vpc.network_name   nats = [{  name = var.tokyo_nat  }] } module \u0026#34;cloud_router_osaka\u0026#34; {  source = \u0026#34;terraform-google-modules/cloud-router/google\u0026#34;  version = \u0026#34;1.3.0\u0026#34;  description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/cloud-router/google/1.3.0\u0026#34;   name = var.osaka_router  project = var.project_id  region = \u0026#34;asia-northeast2\u0026#34;  network = module.vpc.network_name   nats = [{  name = var.osaka_nat  }] }  modules/networks/variables.tf ネットワークモジュールの外部変数を定義しています。\n./modules/networks/variables.tf variable \u0026#34;project_id\u0026#34; {} variable \u0026#34;network\u0026#34; {} variable \u0026#34;tokyo_subnet\u0026#34; {} variable \u0026#34;tokyo_subnet_ip_range\u0026#34; {} variable \u0026#34;tokyo_subnet_2nd_ip_range_1\u0026#34; {} variable \u0026#34;tokyo_subnet_2nd_ip_range_2\u0026#34; {} variable \u0026#34;tokyo_router\u0026#34; {} variable \u0026#34;tokyo_nat\u0026#34; {} variable \u0026#34;osaka_subnet\u0026#34; {} variable \u0026#34;osaka_subnet_ip_range\u0026#34; {} variable \u0026#34;osaka_subnet_2nd_ip_range_1\u0026#34; {} variable \u0026#34;osaka_subnet_2nd_ip_range_2\u0026#34; {} variable \u0026#34;osaka_router\u0026#34; {} variable \u0026#34;osaka_nat\u0026#34; {}  modules/networks/outputs.tf ネットワークモジュールの出力変数を定義しています。\n./modules/networks/outputs.tf output \u0026#34;network\u0026#34; {  value = module.vpc.network_name }  GKEモジュール定義 modules/gke/main.tf 東京/大阪リージョンのGKEクラスタを定義しています。こちらもネットワークモジュール同様にTerraform公式モジュールを活用してみました。\ninfo 記事執筆時点(2022年1月末)では、コントロールプレーンのグローバルアクセスを有効化するオプションがTerraform公式private-clusterサブモジュールv19.0.0(latest)になかったため、Terraform公式beta-private-clusterサブモジュールv19.0.0(latest)を活用しています。\n ./modules/gke/main.tf module \u0026#34;gke_tokyo\u0026#34; {  source = \u0026#34;terraform-google-modules/kubernetes-engine/google//modules/beta-private-cluster\u0026#34;  version = \u0026#34;19.0.0\u0026#34;  description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/kubernetes-engine/google/19.0.0/submodules/beta-private-cluster\u0026#34;   project_id = var.project_id  name = var.tokyo_cluster  region = \u0026#34;asia-northeast1\u0026#34;  network = var.network  subnetwork = var.tokyo_subnet  ip_range_pods = \u0026#34;${var.tokyo_subnet}-pods\u0026#34;  ip_range_services = \u0026#34;${var.tokyo_subnet}-services\u0026#34;  enable_private_endpoint = false  enable_private_nodes = true  master_global_access_enabled = true  master_ipv4_cidr_block = var.tokyo_master_ip_range  release_channel = var.release_channel   node_pools = [{  name = \u0026#34;default-tokyo-pool\u0026#34;  machine_type = \u0026#34;e2-standard-4\u0026#34;  min_count = 1  max_count = 3  initial_node_count = 1  }]  remove_default_node_pool = true  } module \u0026#34;gke_osaka\u0026#34; {  source = \u0026#34;terraform-google-modules/kubernetes-engine/google//modules/beta-private-cluster\u0026#34;  version = \u0026#34;19.0.0\u0026#34;  description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/kubernetes-engine/google/19.0.0/submodules/beta-private-cluster\u0026#34;   project_id = var.project_id  name = var.osaka_cluster  region = \u0026#34;asia-northeast2\u0026#34;  network = var.network  subnetwork = var.osaka_subnet  ip_range_pods = \u0026#34;${var.osaka_subnet}-pods\u0026#34;  ip_range_services = \u0026#34;${var.osaka_subnet}-services\u0026#34;  enable_private_endpoint = false  enable_private_nodes = true  master_global_access_enabled = true  master_ipv4_cidr_block = var.osaka_master_ip_range  release_channel = var.release_channel   node_pools = [{  name = \u0026#34;default-osaka-pool\u0026#34;  machine_type = \u0026#34;e2-standard-4\u0026#34;  min_count = 1  max_count = 3  initial_node_count = 1  }]  remove_default_node_pool = true  }  modules/gke/variables.tf GKEモジュールの外部変数を定義しています。\n./modules/gke/variables.tf variable \u0026#34;project_id\u0026#34; {} variable \u0026#34;network\u0026#34; {} variable \u0026#34;tokyo_cluster\u0026#34; {} variable \u0026#34;tokyo_subnet\u0026#34; {} variable \u0026#34;tokyo_master_ip_range\u0026#34; {} variable \u0026#34;osaka_cluster\u0026#34; {} variable \u0026#34;osaka_subnet\u0026#34; {} variable \u0026#34;osaka_master_ip_range\u0026#34; {} variable \u0026#34;release_channel\u0026#34; {  default = \u0026#34;STABLE\u0026#34; }  modules/gke/outputs.tf GKEモジュールの出力変数を定義しています。\n./modules/gke/outputs.tf output \u0026#34;tokyo_cluster\u0026#34; {  value = module.gke_tokyo.name } output \u0026#34;osaka_cluster\u0026#34; {  value = module.gke_osaka.name }  ASMモジュール定義 modules/asm/main.tf 東京/大阪リージョンのGKEクラスタにASMのインストール、マルチクラスタメッシ作成、Ingressゲートウェイのデプロイを定義しています、、、とはいえ、サンプルコードを書いといてなんですが大変苦しい実装になっていますので個人的には現時点では素直にTerraform以外を使用した方が良いと感じてます^^;\nwarn 記事執筆時点(2022年1月末)では、Terraform公式asmサブモジュールv19.0.0(latest)がASM v11.0以降に対応できていなかったため、Terraform公式gcloudモジュールおよびkubectl-wrapperサブモジュールv3.1.0(latest)を活用してシェルスクリプトでゴリゴリ実装しており、非常に微妙な作りになっております。\n info 今回の例ではTerraform公式firewall-rulesサブモジュールv4.1.0(latest)を活用してファイアウォールルールを定義していますが、rules内の変数定義が省略できず使い勝手はよろしくないため、google_compute_firewallリソースをそのまま定義した方が個人的には良いと感じてます。\n ./modules/asm/main.tf module \u0026#34;asm_tokyo\u0026#34; {  source = \u0026#34;terraform-google-modules/gcloud/google//modules/kubectl-wrapper\u0026#34;  version = \u0026#34;3.1.0\u0026#34;#description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/gcloud/google/3.1.0/submodules/kubectl-wrapper\u0026#34;   project_id = var.project_id  cluster_name = var.tokyo_cluster  cluster_location = var.tokyo_location  kubectl_create_command = \u0026#34;${path.module}/scripts/install.sh ${var.project_id}${var.tokyo_cluster}${var.tokyo_location}${var.release_channel}\u0026#34;  kubectl_destroy_command = \u0026#34;${path.module}/scripts/destroy.sh ${var.project_id}${var.tokyo_cluster}${var.tokyo_location}\u0026#34; } module \u0026#34;asm_osaka\u0026#34; {  source = \u0026#34;terraform-google-modules/gcloud/google//modules/kubectl-wrapper\u0026#34;  version = \u0026#34;3.1.0\u0026#34;#description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/gcloud/google/3.1.0/submodules/kubectl-wrapper\u0026#34;   project_id = var.project_id  cluster_name = var.osaka_cluster  cluster_location = var.osaka_location  kubectl_create_command = \u0026#34;${path.module}/scripts/install.sh ${var.project_id}${var.osaka_cluster}${var.osaka_location}${var.release_channel}\u0026#34;  kubectl_destroy_command = \u0026#34;${path.module}/scripts/destroy.sh ${var.project_id}${var.osaka_cluster}${var.osaka_location}\u0026#34;   module_depends_on = [module.asm_tokyo.wait] } module \u0026#34;asm_firewall_rules\u0026#34; {  source = \u0026#34;terraform-google-modules/network/google//modules/firewall-rules\u0026#34;  version = \u0026#34;4.1.0\u0026#34;#description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/network/google/4.1.0/submodules/firewall-rules\u0026#34;   project_id = var.project_id  network_name = var.network   rules = [{  name = \u0026#34;${var.network}-istio-multicluster-pods\u0026#34;  description = null  direction = \u0026#34;INGRESS\u0026#34;  priority = 900  ranges = [\u0026#34;${var.tokyo_pod_ip_range}\u0026#34;, \u0026#34;${var.osaka_pod_ip_range}\u0026#34;]  source_tags = null  source_service_accounts = null  target_tags = [\u0026#34;gke-${var.tokyo_cluster}\u0026#34;, \u0026#34;gke-${var.osaka_cluster}\u0026#34;]  target_service_accounts = null  allow = [  {  protocol = \u0026#34;tcp\u0026#34;  ports = null  },  {  protocol = \u0026#34;udp\u0026#34;  ports = null  },  {  protocol = \u0026#34;icmp\u0026#34;  ports = null  },  {  protocol = \u0026#34;esp\u0026#34;  ports = null  },  {  protocol = \u0026#34;ah\u0026#34;  ports = null  },  {  protocol = \u0026#34;sctp\u0026#34;  ports = null  }  ]  deny = []  log_config = {  metadata = \u0026#34;EXCLUDE_ALL_METADATA\u0026#34;  }  }] } module \u0026#34;asm_multi_mesh\u0026#34; {  source = \u0026#34;terraform-google-modules/gcloud/google\u0026#34;  version = \u0026#34;3.1.0\u0026#34;#description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/gcloud/google/3.1.0\u0026#34;   platform = \u0026#34;linux\u0026#34;  additional_components = [\u0026#34;kubectl\u0026#34;, \u0026#34;beta\u0026#34;]   create_cmd_entrypoint = \u0026#34;${path.module}/scripts/create-mesh.sh\u0026#34;  create_cmd_body = \u0026#34;${var.project_id}${var.project_id}/${var.tokyo_location}/${var.tokyo_cluster}${var.project_id}/${var.osaka_location}/${var.osaka_cluster}\u0026#34;   module_depends_on = [module.asm_osaka.wait] } module \u0026#34;asm_mcs_api\u0026#34; {  source = \u0026#34;terraform-google-modules/gcloud/google\u0026#34;  version = \u0026#34;3.1.0\u0026#34;#description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/gcloud/google/3.1.0\u0026#34;   platform = \u0026#34;linux\u0026#34;  additional_components = [\u0026#34;kubectl\u0026#34;, \u0026#34;beta\u0026#34;]   create_cmd_entrypoint = \u0026#34;gcloud\u0026#34;  create_cmd_body = \u0026#34;container hub ingress enable --config-membership=${var.tokyo_cluster}\u0026#34;  destroy_cmd_entrypoint = \u0026#34;gcloud\u0026#34;  destroy_cmd_body = \u0026#34;container hub ingress disable\u0026#34;   module_depends_on = [module.asm_multi_mesh.wait] } module \u0026#34;asm_tokyo_ingressgateway\u0026#34; {  source = \u0026#34;terraform-google-modules/gcloud/google//modules/kubectl-wrapper\u0026#34;  version = \u0026#34;3.1.0\u0026#34;#description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/gcloud/google/3.1.0/submodules/kubectl-wrapper\u0026#34;   project_id = var.project_id  cluster_name = var.tokyo_cluster  cluster_location = var.tokyo_location  kubectl_create_command = \u0026#34;kubectl apply -f ${path.module}/manifests/istio-ingressgateway-pods\u0026#34;  kubectl_destroy_command = \u0026#34;kubectl delete ns istio-system --ignore-not-found\u0026#34;   module_depends_on = [module.asm_mcs_api.wait] } module \u0026#34;asm_osaka_ingressgateway\u0026#34; {  source = \u0026#34;terraform-google-modules/gcloud/google//modules/kubectl-wrapper\u0026#34;  version = \u0026#34;3.1.0\u0026#34;#description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/gcloud/google/3.1.0/submodules/kubectl-wrapper\u0026#34;   project_id = var.project_id  cluster_name = var.osaka_cluster  cluster_location = var.osaka_location  kubectl_create_command = \u0026#34;kubectl apply -f ${path.module}/manifests/istio-ingressgateway-pods\u0026#34;  kubectl_destroy_command = \u0026#34;kubectl delete ns istio-system --ignore-not-found\u0026#34;   module_depends_on = [module.asm_tokyo_ingressgateway.wait] } module \u0026#34;asm_mcs_ingressgateway\u0026#34; {  source = \u0026#34;terraform-google-modules/gcloud/google//modules/kubectl-wrapper\u0026#34;  version = \u0026#34;3.1.0\u0026#34;#description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/gcloud/google/3.1.0/submodules/kubectl-wrapper\u0026#34;   project_id = var.project_id  cluster_name = var.tokyo_cluster  cluster_location = var.tokyo_location  kubectl_create_command = \u0026#34;kubectl apply -f ${path.module}/manifests/istio-ingressgateway-services\u0026#34;  kubectl_destroy_command = \u0026#34;kubectl delete -f ${path.module}/manifests/istio-ingressgateway-services --ignore-not-found\u0026#34;   module_depends_on = [module.asm_osaka_ingressgateway.wait] }  modules/asm/variables.tf ASMモジュールの外部変数を定義しています。\n./modules/asm/variables.tf variable \u0026#34;project_id\u0026#34; {} variable \u0026#34;network\u0026#34; {} variable \u0026#34;tokyo_cluster\u0026#34; {} variable \u0026#34;tokyo_location\u0026#34; {  default = \u0026#34;asia-northeast1\u0026#34; } variable \u0026#34;tokyo_pod_ip_range\u0026#34; {} variable \u0026#34;osaka_cluster\u0026#34; {} variable \u0026#34;osaka_location\u0026#34; {  default = \u0026#34;asia-northeast2\u0026#34; } variable \u0026#34;osaka_pod_ip_range\u0026#34; {} variable \u0026#34;release_channel\u0026#34; {  default = \u0026#34;STABLE\u0026#34; }  modules/asm/scripts/install.sh ASMのインストール処理を定義したスクリプトファイルです。ASM v11.0から正式ツールとなったasmcliコマンドを使用して、マネージドコントロールプレーン構成を作成しています。\n./modules/asm/scripts/install.sh #!/usr/bin/env bash  set -e  PROJECT_ID=${1} CLUSTER_NAME=${2} CLUSTER_LOCATION=${3} RELEASE_CHANNEL=${4}  curl https://storage.googleapis.com/csm-artifacts/asm/asmcli \u0026gt; asmcli chmod +x asmcli  ./asmcli install \\  --project_id ${PROJECT_ID} \\  --cluster_name ${CLUSTER_NAME} \\  --cluster_location ${CLUSTER_LOCATION} \\  --managed \\  --channel ${RELEASE_CHANNEL} \\  --enable-all  modules/asm/scripts/destroy.sh ASMの削除処理を定義したスクリプトファイルです。ASM関連のNamespaceを削除し、Anthosクラスタからの登録解除を実行しています。\n./modules/asm/scripts/destroy.sh #!/usr/bin/env bash  set -e  PROJECT_ID=${1} CLUSTER_NAME=${2} CLUSTER_LOCATION=${3}  kubectl delete ns asm-system istio-system --ignore-not-found  gcloud container hub memberships unregister ${CLUSTER_NAME} \\  --project=${PROJECT_ID} \\  --gke-cluster=${CLUSTER_LOCATION}/${CLUSTER_NAME}  modules/asm/scripts/create-mesh.sh マルチクラスタメッシュ作成処理を定義したスクリプトファイルです。\n./modules/asm/scripts/create-mesh.sh #!/usr/bin/env bash  set -e  PROJECT_ID=\u0026#34;${1}\u0026#34; CLUSTER_1=\u0026#34;${2}\u0026#34; CLUSTER_2=\u0026#34;${3}\u0026#34;  curl https://storage.googleapis.com/csm-artifacts/asm/asmcli \u0026gt; asmcli chmod +x asmcli  ./asmcli create-mesh ${PROJECT_ID} ${CLUSTER_1} ${CLUSTER_2}  modules/asm/manifests/istio-ingressgateway-pods/* Istio IngressゲートウェイコンテナのKubernetesマニフェストファイルです。GitHubにて公開されている次のサンプルをベースにしています。\nhttps://github.com/GoogleCloudPlatform/anthos-service-mesh-packages/tree/main/samples/gateways/istio-ingressgateway\n./modules/asm/manifests/istio-ingressgateway-pods/namespace.yaml apiVersion: v1 kind: Namespace metadata:  name: istio-system  labels:  istio.io/rev: asm-managed-stable  ./modules/asm/manifests/istio-ingressgateway-pods/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata:  name: istio-ingressgateway  namespace: istio-system spec:  replicas: 3  selector:  matchLabels:  app: istio-ingressgateway  istio: ingressgateway  template:  metadata:  annotations:  inject.istio.io/templates: gateway  labels:  app: istio-ingressgateway  istio: ingressgateway  spec:  containers:  - name: istio-proxy  image: auto  resources:  limits:  cpu: 2000m  memory: 1024Mi  requests:  cpu: 100m  memory: 128Mi  serviceAccountName: istio-ingressgateway --- apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata:  name: istio-ingressgateway  namespace: istio-system spec:  maxUnavailable: 1  selector:  matchLabels:  istio: ingressgateway  app: istio-ingressgateway --- apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata:  name: istio-ingressgateway  namespace: istio-system spec:  maxReplicas: 5  metrics:  - resource:  name: cpu  targetAverageUtilization: 80  type: Resource  minReplicas: 3  scaleTargetRef:  apiVersion: apps/v1  kind: Deployment  name: istio-ingressgateway  ./modules/asm/manifests/istio-ingressgateway-pods/serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata:  name: istio-ingressgateway  namespace: istio-system  ./modules/asm/manifests/istio-ingressgateway-pods/role.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata:  name: istio-ingressgateway  namespace: istio-system rules: - apiGroups: [\u0026#34;\u0026#34;]  resources: [\u0026#34;secrets\u0026#34;]  verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata:  name: istio-ingressgateway  namespace: istio-system roleRef:  apiGroup: rbac.authorization.k8s.io  kind: Role  name: istio-ingressgateway subjects: - kind: ServiceAccount  name: istio-ingressgateway  modules/asm/manifests/istio-ingressgateway-services/* Istio Ingressゲートウェイ用マルチクラスタIngress/ServiceのKubernetesマニフェストファイルです。\n./modules/asm/manifests/istio-ingressgateway-services/multiclusterservice.yaml apiVersion: networking.gke.io/v1 kind: MultiClusterService metadata:  name: istio-ingressgateway  namespace: istio-system  annotations:  cloud.google.com/backend-config: \u0026#39;{\u0026#34;default\u0026#34;: \u0026#34;ingress-backendconfig\u0026#34;}\u0026#39;  labels:  app: istio-ingressgateway  istio: ingressgateway spec:  template:  spec:  ports:  - name: status-port  port: 15021  protocol: TCP  targetPort: 15021  - name: http2  port: 80  - name: https  port: 443  selector:  istio: ingressgateway  app: istio-ingressgateway  ./modules/asm/manifests/istio-ingressgateway-services/backendconfig.yaml apiVersion: cloud.google.com/v1 kind: BackendConfig metadata:  name: ingress-backendconfig  namespace: istio-system spec:  healthCheck:  requestPath: /healthz/ready  port: 15021  type: HTTP  ./modules/asm/manifests/istio-ingressgateway-services/multiclusteringress.yaml apiVersion: networking.gke.io/v1beta1 kind: MultiClusterIngress metadata:  name: istio-ingressgateway  namespace: istio-system  labels:  app: istio-ingressgateway  istio: ingressgateway spec:  template:  spec:  backend:  serviceName: istio-ingressgateway  servicePort: 80  デプロイ用のCloud Buildパイプラインも書いてみたけれど、、、 terraform init/plan/applyコマンドを順に実行するだけですが、手動だとどんなに簡単なコマンドであってもミスが生じてしまう可能性はあるためパイプライン化してみました。環境名のブランチpocに対してPushが入ったら起動するといったイメージにしております。\n、、、と本来であればこれでもパイプラインは動くはずなのですが、残念なことにTerraform公式asmサブモジュールv19.0.0(latest)、gcloudモジュールおよびkubectl-wrapperサブモジュールv3.1.0(latest)をTerraform公式Dockerイメージ上で動かすとエラーが発生してしまいます。非常に微妙ですが、今回のサンプルコードではDockerイメージをカスタマイズするか、あきらめて手動で実行をする必要がございます(TT)\nwarn 記事執筆時点(2022年1月末)では、Terraform公式asmサブモジュールv19.0.0(latest)、gcloudモジュールおよびkubectl-wrapperサブモジュールv3.1.0(latest)をTerraform公式Dockerイメージ上で動かすとエラーになりますのでご注意ください。\n cloudbuild.yaml substitutions:  _TERRAFORM_VERSION: 1.1.4  steps:  - id: \u0026#34;terraform init\u0026#34;  name: \u0026#34;hashicorp/terraform:${_TERRAFORM_VERSION}\u0026#34;  entrypoint: \u0026#34;sh\u0026#34;  args:  - \u0026#34;-cx\u0026#34;  - |cd environments/${BRANCH_NAME} terraform init -reconfigure   - id: \u0026#34;terraform plan\u0026#34;  name: \u0026#34;hashicorp/terraform:${_TERRAFORM_VERSION}\u0026#34;  entrypoint: \u0026#34;sh\u0026#34;  args:  - \u0026#34;-cx\u0026#34;  - |cd environments/${BRANCH_NAME} terraform plan -var=\u0026#34;project_id=${PROJECT_ID}\u0026#34;   - id: \u0026#34;terraform apply\u0026#34;  name: \u0026#34;hashicorp/terraform:${_TERRAFORM_VERSION}\u0026#34;  entrypoint: \u0026#34;sh\u0026#34;  args:  - \u0026#34;-cx\u0026#34;  - |cd environments/${BRANCH_NAME} terraform apply -auto-approve -var=\u0026#34;project_id=${PROJECT_ID}\u0026#34;   エラーメッセージ出力例 module.asm.module.asm_tokyo.module.gcloud_kubectl.null_resource.additional_components[0]: Creating... module.asm.module.asm_tokyo.module.gcloud_kubectl.null_resource.additional_components[0]: Provisioning with \u0026#39;local-exec\u0026#39;... module.asm.module.asm_tokyo.module.gcloud_kubectl.null_resource.additional_components[0] (local-exec): Executing: [\u0026#34;/bin/sh\u0026#34; \u0026#34;-c\u0026#34; \u0026#34;.terraform/modules/asm.asm_tokyo/scripts/check_components.sh gcloud kubectl\u0026#34;] module.asm.module.asm_tokyo.module.gcloud_kubectl.null_resource.additional_components[0] (local-exec): /bin/sh: .terraform/modules/asm.asm_tokyo/scripts/check_components.sh: not found ╷ │ Error: local-exec provisioner error │ │ with module.asm.module.asm_tokyo.module.gcloud_kubectl.null_resource.additional_components[0], │ on .terraform/modules/asm.asm_tokyo/main.tf line 174, in resource \u0026#34;null_resource\u0026#34; \u0026#34;additional_components\u0026#34;: │ 174: provisioner \u0026#34;local-exec\u0026#34; { │ │ Error running command │ \u0026#39;.terraform/modules/asm.asm_tokyo/scripts/check_components.sh gcloud │ kubectl\u0026#39;: exit status 127. Output: /bin/sh: │ .terraform/modules/asm.asm_tokyo/scripts/check_components.sh: not found │ ╵  終わりに 今回はGKE+ASMのマルチクラスタメッシュ環境をTerraformを使って、しかも普段あまり積極的な活用はしないTerraform公式モジュールをあえて多用して^^; 構築してみましたがいかがだったでしょうか。もしこれから「ASM環境をTerraformで」と検討している方は参考にしてみてはいかがでしょうか。\nとはいえ、サンプルコードを書いといてなんですがASMの導入からはシェルスクリプトを多用した大変苦しい実装になっておりますし、個人的には現時点ではASMの導入以降は素直にTerraform以外を使用した方が良いと感じてます^^; とりあえず、苦しいことだけは伝わったかと思います。あくまで本記事はご参考ということで、その点ご承知おきいただけると幸いです。\n  Google Cloud は、Google LLC の商標または登録商標です。 Terraform は、HashiCorp, Inc. の米国およびその他の国における商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。  ","permalink":"https://chacco38.github.io/posts/2022/02/gcp-deploy-asm-with-terraform/","summary":"はじめに みなさん、こんにちは。以前に「複数リージョンのGKEクラスタとAnthos Service Meshでマルチクラスタメッシュ環境を構築してみた」という記事を書いたのですが、今回はその環境をTerraformを使って構築してみました。もしこれから「ASM環境をTerraformで」と検討している方は参考にしてみてはいかがでしょうか。\nとはいえ、本記事の執筆時点(2022年1月末)ではTerraform公式モジュールがASMのv1.11以降に対応しておらず正直使いモノにならなかったこともあり、やや苦しい実装になってしまっています。素直にASMの導入以降はTerraform以外を使うのが良いかと思いますが、あくまで本記事はご参考ということでその点ご承知おきいただけると幸いです。\n構築するシステムについて 次の図に示すように限定公開クラスを有効化した複数リージョンのGKEクラスタに対してAnthos Service Mesh(マネージドコントロールプレーン)を導入した環境となっています。なお、アプリケーションのコンテナについてはインフラとは異なるリポジトリで管理するのが一般的かと思うので今回は除外しています。\nTerraformのサンプルコードを書いてみた それでは今回作成したTerraformのサンプルコードを紹介していきたいと思います。まずはディレクトリ構造ですが、今回はenvironmentsディレクトリ配下へ環境ごとにサブディレクトリを作成し、Workspaceは使わずに別ファイルとして管理する形を想定した作りにしてます。\nディレクトリ構成 . |-- environments | `-- poc | |-- backend.tf | |-- main.tf | `-- variables.tf `-- modules  |-- networks  | |-- main.tf  | |-- variables.tf  | `-- outputs.tf  |-- gke  | |-- main.tf  | |-- variables.tf  | `-- outputs.tf  `-- asm  |-- main.tf  |-- variables.tf  |-- scripts  | |-- install.","title":"Terraformを使ってGKE+ASMのマルチクラスタメッシュ環境を構築してみた"},{"content":"はじめに みなさん、こんにちは。AWS マネジメントコンソールを使っていると、ごくごく稀に表示言語を代えたくなることはありませんか。私は日本語⇔英語を切り替えたくなるケースがたまにあります。\nただ、個人的に「設定」と言えばなんとなく右上の項目からできそうなイメージがあり、「あれ、設定ってどこから変えるんだっけ、、、アカウントへ飛んだ先にあったっけ、、、(答え、アカウントへ飛んだ先にはない)」みたいにウッカリ設定方法を忘れて途方にくれてしまうことがあります。みなさんはそんな経験ございませんかね？ということで自分への備忘も込め、今回は表示言語の設定方法について紹介していきたいと思います。\n表示言語の変更方法 では早速答えですが、言語の設定方法はAWS マネジメントコンソールの左下部分にございます。(なるほど、ここだったか、、、)\n終わりに いまさらの情報でしたがいかがだったでしょうか。こんな記事でもだれかの役に立っていただければ幸いです。以上、AWS マネジメントコンソールの表示言語を変更する方法でした。\n  AWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。  ","permalink":"https://chacco38.github.io/posts/2022/01/aws-console-language-settings/","summary":"はじめに みなさん、こんにちは。AWS マネジメントコンソールを使っていると、ごくごく稀に表示言語を代えたくなることはありませんか。私は日本語⇔英語を切り替えたくなるケースがたまにあります。\nただ、個人的に「設定」と言えばなんとなく右上の項目からできそうなイメージがあり、「あれ、設定ってどこから変えるんだっけ、、、アカウントへ飛んだ先にあったっけ、、、(答え、アカウントへ飛んだ先にはない)」みたいにウッカリ設定方法を忘れて途方にくれてしまうことがあります。みなさんはそんな経験ございませんかね？ということで自分への備忘も込め、今回は表示言語の設定方法について紹介していきたいと思います。\n表示言語の変更方法 では早速答えですが、言語の設定方法はAWS マネジメントコンソールの左下部分にございます。(なるほど、ここだったか、、、)\n終わりに いまさらの情報でしたがいかがだったでしょうか。こんな記事でもだれかの役に立っていただければ幸いです。以上、AWS マネジメントコンソールの表示言語を変更する方法でした。\n  AWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。  ","title":"AWSマネジメントコンソールの表示言語を変更する方法"},{"content":"はじめに みなさん、こんにちは。AWS CloudShellを使っているとファイルにペーストした際、勝手にインデントが入って「あー！」っとなったことの一度くらいはあるのではないでしょうか。今回はそんなときの解決方法を紹介していきたいと思います。\nインデントの自動挿入なしでペーストする方法 いくつか方法はありますが、解の1つは「ペーストモードを使う」です。編集モードへ入る前に :set paste もしくは :set paste! を実行しましょう。ペーストモードにすると出力例のようにインデントが追加されずに期待通りの動きになりますね。\n起動時に自動で設定する方法 とはいえ、エディタを起動するたびに毎回ペーストモードの設定をするのは面倒です。そんなときは vim の設定ファイル ~/.vimrc を作成しましょう。これでエディタが起動した際に自動で適用されるようになります。めでたしめでたし。\n~/.vimrc set paste  終わりに いまさらの情報でしたがいかがだったでしょうか。もちろんコードを編集するときは自動でインデントを追加してくれるのはうれしいのですが、個人的にはAWS CloudShell上ではクリップボードからコピーしてくることも結構多いのでデフォルトペーストモードにしておき、必要に応じて :set nopaste もしくは :set paste! しております。よろしければ参考にしていただければと思います。\n以上、AWS CloudShell上のviエディタでインデントの自動挿入なしでペーストする方法でした。\n  AWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。  ","permalink":"https://chacco38.github.io/posts/2021/12/aws-cloudshell-autoindent-settings/","summary":"はじめに みなさん、こんにちは。AWS CloudShellを使っているとファイルにペーストした際、勝手にインデントが入って「あー！」っとなったことの一度くらいはあるのではないでしょうか。今回はそんなときの解決方法を紹介していきたいと思います。\nインデントの自動挿入なしでペーストする方法 いくつか方法はありますが、解の1つは「ペーストモードを使う」です。編集モードへ入る前に :set paste もしくは :set paste! を実行しましょう。ペーストモードにすると出力例のようにインデントが追加されずに期待通りの動きになりますね。\n起動時に自動で設定する方法 とはいえ、エディタを起動するたびに毎回ペーストモードの設定をするのは面倒です。そんなときは vim の設定ファイル ~/.vimrc を作成しましょう。これでエディタが起動した際に自動で適用されるようになります。めでたしめでたし。\n~/.vimrc set paste  終わりに いまさらの情報でしたがいかがだったでしょうか。もちろんコードを編集するときは自動でインデントを追加してくれるのはうれしいのですが、個人的にはAWS CloudShell上ではクリップボードからコピーしてくることも結構多いのでデフォルトペーストモードにしておき、必要に応じて :set nopaste もしくは :set paste! しております。よろしければ参考にしていただければと思います。\n以上、AWS CloudShell上のviエディタでインデントの自動挿入なしでペーストする方法でした。\n  AWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。  ","title":"AWS CloudShell上のviエディタでインデントの自動挿入なしでペーストする方法"},{"content":"はじめに みなさん、こんにちは。今回はタイトルに掲げたとおり、最近の更新で Google Cloud コンソール(GUI)から Google Kubernetes Engine(GKE) Standard クラスタを作成する際に、次のようなすてきなオプションがプレビュー機能として追加されました。今回はこのオプションを使ってどのような構成が作られるのか実験したので共有したいと思います。\nいきなりですが、結論です！ In-cluster とマネージドコントロールプレーンのどちらで構築される？ 答え、マネージドコントロールプレーンにて構築されます。現時点でこの設定を変更することはできません。\nAnthos Service Mesh のバージョンは？ 答え、GKE でリリースチャンネルを採用した場合は GKE と同じチャンネルになります。GKE で静的リリースを採用した場合は Reguler チャンネルとなります。現時点でこの設定を変更することはできません。\nカスタム CA を扱うことはできるか？ 答え、扱えません。マネージドコントロールプレーンでの導入となるため、カスタム CA を扱うことができる Istio CA を選択することはできません。\n限定クラスタにした場合は別途 15017/TCP を許可する必要があるか？ 答え、不要です。ルールを追加しなくてもサイドカー自動インジェクションは問題なく動きます。\nIngress ゲートウェイはデフォルトで作られるか？ 答え、Ingress ゲートウェイはデフォルトでは作られません。別途ユーザにてデプロイする必要があります。\n終わりに 今回は GKE クラスタ作成時の Anthos Service Mesh 有効化オプションの実験結果の共有でしたがいかがだったでしょうか。\nGKE と Anthos Service Mesh で採用するリリースチャンネルを変えたい、カスタム CA を使いたいといったケースでは従来どおり CLI を利用する必要がありますが、これらの要件がなければ GUI からポチポチするだけでとっても簡単に環境を作れるようになりそうですね。\nwarn 2021 年 12 月時点ではGUI の Anthos Service Mesh 有効化オプションはプレビュー段階であり、今回紹介した挙動から変わる可能性がありますのでご注意ください。 {\n }\n  Google Cloud は、Google LLC の商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。  ","permalink":"https://chacco38.github.io/posts/2021/12/gcp-new-asm-installation-options/","summary":"はじめに みなさん、こんにちは。今回はタイトルに掲げたとおり、最近の更新で Google Cloud コンソール(GUI)から Google Kubernetes Engine(GKE) Standard クラスタを作成する際に、次のようなすてきなオプションがプレビュー機能として追加されました。今回はこのオプションを使ってどのような構成が作られるのか実験したので共有したいと思います。\nいきなりですが、結論です！ In-cluster とマネージドコントロールプレーンのどちらで構築される？ 答え、マネージドコントロールプレーンにて構築されます。現時点でこの設定を変更することはできません。\nAnthos Service Mesh のバージョンは？ 答え、GKE でリリースチャンネルを採用した場合は GKE と同じチャンネルになります。GKE で静的リリースを採用した場合は Reguler チャンネルとなります。現時点でこの設定を変更することはできません。\nカスタム CA を扱うことはできるか？ 答え、扱えません。マネージドコントロールプレーンでの導入となるため、カスタム CA を扱うことができる Istio CA を選択することはできません。\n限定クラスタにした場合は別途 15017/TCP を許可する必要があるか？ 答え、不要です。ルールを追加しなくてもサイドカー自動インジェクションは問題なく動きます。\nIngress ゲートウェイはデフォルトで作られるか？ 答え、Ingress ゲートウェイはデフォルトでは作られません。別途ユーザにてデプロイする必要があります。\n終わりに 今回は GKE クラスタ作成時の Anthos Service Mesh 有効化オプションの実験結果の共有でしたがいかがだったでしょうか。\nGKE と Anthos Service Mesh で採用するリリースチャンネルを変えたい、カスタム CA を使いたいといったケースでは従来どおり CLI を利用する必要がありますが、これらの要件がなければ GUI からポチポチするだけでとっても簡単に環境を作れるようになりそうですね。\nwarn 2021 年 12 月時点ではGUI の Anthos Service Mesh 有効化オプションはプレビュー段階であり、今回紹介した挙動から変わる可能性がありますのでご注意ください。 {","title":"GKE クラスタ作成時のオプションで Anthos Service Mesh を有効化できるようになりました"},{"content":"はじめに みなさん、こんにちは。今回は Amazon Elastic Kubernetes Service(EKS) を利用する際に併せて利用したい AWS Load Balancer Controller のお話です。\nみなさんは Amazon EKS を活用して Kubernetes クラスタを AWS 上で動かすとなった際に、他のマネージドサービスの利用はどうされていますか。もちろんすべて Kubernetes 上で動かしてシステムを完結させるという選択肢もあるかと思いますが、やはり多くの方が他の AWS のマネージドサービスの併用も検討されるのではないでしょうか。その一方で、これら併用環境のコード化 (IaC、Infrastructure as Code) を実現しようとすると、Kubernetes アプリケーションの管理は Helm で、AWS リソースの管理は Terraform で、などという別々のツールでの管理になってしまいがちです。\nそんな悩みを解決する一つの手段が AWS Load Balancer Controller や AWS Controllers for Kubernetes といった Kubernetes クラスタ機能を拡張する各種コントローラの活用です。これらのコントローラを利用することで、AWS リソースについても Kubernetes マニフェストファイルで定義できるようになり、Kubernetes 側に運用管理を寄せてシンプル化することができます。\n今回はそのうちの一つ、Elastic Load Balancing(ELB) を Kubernetes クラスタで管理できるようにする AWS Load Balancer Controller について、簡単なサンプルアプリケーションを交えて紹介していきたいと思います。これから Amazon EKS 上にアプリケーションを展開しようと考えている方は参考にしてみてはいかがでしょうか。\nAWS Load Balancer Controller とは AWS Load Balancer Controller (旧AWS ALB Ingress Controller) は、ELB を Kubernetes クラスタから管理するためのコントローラです。このコントローラを活用することで、Kubernetes Ingress リソースとして L7 ロードバランサの Application Load Balancer(ALB) を、Kuberntes Service リソースとして L4 ロードバランサの Network Load Balancer(NLB) を利用することができるようになります。\nKubernetes Service/Ingress リソースでの処理を外部ロードバランサである ELB へ切り出すことによって、ワークロードへの性能影響の低減、ノードリソースの利用効率の向上、Service/Ingress リソースのスケーリングなどを AWS 側へ任せることで運用負荷の低減といったことができる見込みです。\nhttps://github.com/kubernetes-sigs/aws-load-balancer-controller\nAWS Load Balancer Controller を導入してみよう Step1. 作業環境の設定 今回は Amazon EKS や AWS Load Balancer Controller の管理を行う環境として AWS CloudShell を利用していきたいと思います。まずは操作に必要な各種ツールの設定を AWS CloudShell にしてきましょう。\nAWS CLI の設定 AWS リソースの操作を行えるように次のコマンドを実行し、AWS CLI の設定を行いましょう。\nAWS CLIの設定 aws configure  kubectl コマンドのインストール 次に Kubernetes 管理ツールの kubectl コマンドをインストールしましょう。\nhttps://docs.aws.amazon.com/ja_jp/eks/latest/userguide/install-kubectl.html#linux\nkubectlコマンドのインストール # kubectl コマンドのダウンロード curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.21.2/2021-07-05/bin/linux/amd64/kubectl  # 実行権限の付与 chmod +x kubectl  # 実行ファイルのパスを設定 mkdir -p ${HOME}/bin \u0026amp;\u0026amp; mv kubectl ${HOME}/bin \u0026amp;\u0026amp; export PATH=${PATH}:${HOME}/bin  # シェルの起動時に $HOME/bin をパスへ追加 echo \u0026#39;export PATH=${PATH}:${HOME}/bin\u0026#39; \u0026gt;\u0026gt; ~/.bashrc  # インストールが成功していることを確認 kubectl version --short --client  インストールに成功していれば出力例のようにバージョン情報の出力を確認できます。\n出力例 $ kubectl version --short --client Client Version: v1.21.2-13+d2965f0db10712  eksctl コマンドのインストール 続いて Amazon EKS 管理ツールの eksctl コマンドをインストールしましょう。\nhttps://docs.aws.amazon.com/ja_jp/eks/latest/userguide/eksctl.html#linux\neksctlコマンドのインストール # eksctl の最新バージョンをダウンロード curl -L \u0026#34;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; | tar xz -C /tmp  # 実行ファイルをパスの通ったディレクトリへ移動 mv /tmp/eksctl ${HOME}/bin  # インストールが成功していることを確認 eksctl version  インストールに成功していれば出力例のようにバージョン情報の出力を確認できます。\n出力例 $ eksctl version 0.76.0  helm コマンドのインストール 最後に Kubernetes 上で稼働するアプリケーションを管理するためのツールである helm コマンドをインストールしましょう。\nhttps://docs.aws.amazon.com/ja_jp/eks/latest/userguide/helm.html\nhelmコマンドのインストール # 前提パッケージのインストール sudo yum install -y openssl  # インストールスクリプトのダウンロード curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 \u0026gt; get_helm.sh  # 実行権限の付与 chmod 700 get_helm.sh  # インストールスクリプトの実行 ./get_helm.sh  # インストールが成功していることを確認 helm version --short  インストールに成功していれば出力例のようにバージョン情報の出力を確認できます。\n出力例 $ helm version --short v3.7.2+g663a896  以上で作業環境(AWS CloudShell)の設定は完了です。\nStep2. EKS クラスタの作成 続いて AWS Load Balancer Controller を導入する対象の Amazon EKS クラスタを作成していきましょう。今回は Kubernetes ノードには AWS Fargate を使用していきたいと思います。それでは eksctl コマンドを実行してクラスタを作成しましょう。\nhttps://docs.aws.amazon.com/ja_jp/eks/latest/userguide/getting-started-eksctl.html\nEKSクラスタの作成 # 環境変数の設定 export CLUSTER=\u0026#34;my-tokyo-cluster\u0026#34;  # EKS クラスタの作成 eksctl create cluster --name ${CLUSTER} --version 1.21 --fargate  # サービスアカウントでの IAM ロール使用を許可 eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER} --approve  ここまで終わりましたら AWS Load Balancer Controller を利用するための事前準備は完了です。\nStep3. コントローラのデプロイ それでは AWS Load Balancer Controller を Amazon EKS クラスタにデプロイしていきましょう。\nhttps://docs.aws.amazon.com/ja_jp/eks/latest/userguide/aws-load-balancer-controller.html\nサービスアカウントの作成 まずは AWS Load Balancer Controller 用のサービスアカウントの作成を行っていきます。今回は kube-system Namespace に aws-load-balancer-controller という名前でサービスアカウントを作成して行きたいと思います。それでは次のコマンドを実行してサービスアカウントを作成しましょう。\nサービスアカウントの作成 AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \u0026#34;Account\u0026#34; --output text) POLICY_NAME=\u0026#34;myAWSLoadBalancerControllerIAMPolicy\u0026#34; SERVICE_ACCOUNT=\u0026#34;aws-load-balancer-controller\u0026#34;  # IAM ポリシー定義ファイルのダウンロード curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json  # IAM ポリシーの作成 aws iam create-policy \\  --policy-name ${POLICY_NAME} \\  --policy-document file://iam_policy.json  # サービスアカウントの作成 eksctl create iamserviceaccount \\  --name=${SERVICE_ACCOUNT} \\  --cluster=${CLUSTER} \\  --namespace=\u0026#34;kube-system\u0026#34; \\  --attach-policy-arn=arn:aws:iam::${AWS_ACCOUNT_ID}:policy/${POLICY_NAME} \\  --override-existing-serviceaccounts \\  --approve  コントローラのインストール 次に AWS Load Balancer Controller をインストールしていきましょう。なお、AWS Load Balancer Controller のインストール方法はいくつか用意されていますが、AWS Fargate 環境の場合は Helm を利用したインストール方法を選択する必要があるためご注意ください。\nコントローラのインストール # EKS 用の Helm レポジトリを追加 helm repo add eks https://aws.github.io/eks-charts  # TargetGroupBinding カスタムリソースをインストール kubectl apply -k \u0026#34;github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\u0026#34;  # Helm チャートからインストール helm install aws-load-balancer-controller eks/aws-load-balancer-controller \\  --set clusterName=${CLUSTER} \\  --set serviceAccount.create=false \\  --set vpcId=$(aws cloudformation describe-stacks \\  --stack-name \u0026#34;eksctl-${CLUSTER}-cluster\u0026#34; \\  --query \u0026#39;Stacks[0].Outputs[?OutputKey==`VPC`].OutputValue\u0026#39; \\  --output text) \\  --set serviceAccount.name=${SERVICE_ACCOUNT} \\  -n kube-system  # 確認 kubectl get deployment -n kube-system aws-load-balancer-controller  インストールに成功していれば出力例のようにコントローラが一覧に表示されるようになります。\n出力例 $ kubectl get deployment -n kube-system aws-load-balancer-controller NAME READY UP-TO-DATE AVAILABLE AGE aws-load-balancer-controller 2/2 2 2 42s  以上で AWS Load Balancer Controller の導入は完了です。\nAWS Load Balancer Controller を使ってみよう Network Load Balancer (Serviceリソース) の使い方 AWS Load Balancer Controller 環境では、次のサンプルのように Kubernetes Service リソース定義にて「LoadBalancer タイプの指定」と「アノテーションとして各種パラメータを指定」をすることによって Network Load Balancer(NLB) をプロビジョニングすることができます。\nhttps://docs.aws.amazon.com/ja_jp/eks/latest/userguide/network-load-balancing.html\nServiceリソース定義サンプル apiVersion: v1 kind: Service metadata:  name: sample-service  annotations:  service.beta.kubernetes.io/aws-load-balancer-type: external  service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing  service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip spec:  type: LoadBalancer  ports:  - port: 80  protocol: TCP  selector:  app: sample-app  アノテーションに指定する主要なパラメータとしては次の通りです。アノテーションに指定可能なパラメータの一覧につきましては次の URL を参照してください。\nhttps://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.3/guide/service/annotations/\n   アノテーション名 説明     service.beta.kubernetes.io/aws-load-balancer-type ロードバランサのプロビジョニングに使用するコントローラを指定します。AWS Load Balancer Controller の場合は \u0026ldquo;external\u0026rdquo; を指定します。   service.beta.kubernetes.io/aws-load-balancer-scheme ロードバランサのスキームを指定します。内部向けの場合は \u0026ldquo;internal\u0026quot;、インターネット向けの場合は \u0026ldquo;internet-facing\u0026rdquo; を指定します。   service.beta.kubernetes.io/aws-load-balancer-nlb-target-type バックエンドに指定するターゲットタイプを指定します。トラフィックを直接 Pod にルーティングするには \u0026ldquo;ip\u0026rdquo; を指定します。   service.beta.kubernetes.io/aws-load-balancer-healthcheck-port バックエンドのヘルスチェック用ポート(\u0026rdquo;traffic-port\u0026quot;/\u0026quot;ポート番号\u0026quot;)を指定します。デフォルトは \u0026ldquo;traffic-port\u0026rdquo; です。   service.beta.kubernetes.io/aws-load-balancer-healthcheck-protocol バックエンドのヘルスチェック用プロトコル(\u0026quot;tcp\u0026quot;/\u0026quot;http\u0026quot;/\u0026quot;https\u0026quot;)を指定します。デフォルトは \u0026ldquo;tcp\u0026rdquo; です。   service.beta.kubernetes.io/aws-load-balancer-healthcheck-path バックエンドの HTTP/HTTPS ヘルスチェック用パスを指定します。デフォルトは \u0026ldquo;/\u0026rdquo; です。    NLB Service のサンプル それではサンプルアプリケーションをデプロイして NLB Service リソースを実際に動かしてみましょう。今回は次のサンプルアプリケーションをベースに少しだけ手を加えたものを用いて動作確認をしていきたいと思います。\nhttps://github.com/istio/istio/tree/master/samples/helloworld\nhelloworld-nlb-sample.yaml（サンプルアプリケーション定義ファイル） apiVersion: v1 kind: Service metadata:  name: helloworld  labels:  app: helloworld  annotations:  service.beta.kubernetes.io/aws-load-balancer-type: external  service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing  service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip  service.beta.kubernetes.io/aws-load-balancer-healthcheck-protocol: http  service.beta.kubernetes.io/aws-load-balancer-healthcheck-path: /health spec:  type: LoadBalancer  ports:  - port: 80  targetPort: 5000  name: http  selector:  app: helloworld --- apiVersion: apps/v1 kind: Deployment metadata:  name: helloworld-v1  labels:  app: helloworld  version: v1 spec:  replicas: 1  selector:  matchLabels:  app: helloworld  version: v1  template:  metadata:  labels:  app: helloworld  version: v1  spec:  containers:  - name: helloworld  image: docker.io/istio/examples-helloworld-v1  ports:  - containerPort: 5000  それでは次のコマンドを実行してサンプルアプリケーションをデプロイしていきましょう。\nサンプルアプリケーションのデプロイ export FARGATEPROFILE=\u0026#34;sample-app\u0026#34; export NAMESPACE=\u0026#34;sample\u0026#34;  # Fargate プロファイルの作成 eksctl create fargateprofile --cluster ${CLUSTER} --name ${FARGATEPROFILE} --namespace ${NAMESPACE}  # サンプルアプリケーション用 Namespace の作成 kubectl create namespace ${NAMESPACE}  # サンプルアプリケーションのデプロイ kubectl apply -n ${NAMESPACE} -f helloworld-nlb-sample.yaml  # サービスがデプロイされたことを確認 kubectl get -n ${NAMESPACE} service helloworld  NLB Service のデプロイに成功した場合は次の出力例のように Service リソース一覧に表示されます。\n出力例 $ kubectl get -n ${NAMESPACE} service helloworld NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE helloworld LoadBalancer 10.100.114.79 k8s-sample-hellowor-xxxxxxxxxx-xxxxxxxxxxxxxxxx.elb.ap-northeast-1.amazonaws.com 80:32642/TCP 32h  NLB のヘルスチェックが正常になるまで数分待った後、NLB のパブリックエンドポイントにアクセスしてみましょう。\nサンプルアプリケーションへアクセス # NLB の DNS 名を取得 ENDPOINT=$(kubectl get -n ${NAMESPACE} service helloworld \\  -o custom-columns=HOSTNAME:status.loadBalancer.ingress[0].hostname \\  --no-headers)  # テストの実行 curl http://${ENDPOINT}/hello  期待通りのレスポンスが返ってくることが確認できたら成功です。\nサンプルアプリケーション出力例 $ curl http://${ENDPOINT}/hello Hello version: v1, instance: helloworld-v1-b9d9d6679-hdqsq  最後にサンプルアプリケーションを削除して動作確認は終了です。お疲れ様でした。\nサンプルアプリケーションの削除 kubectl delete namespace ${NAMESPACE}  Application Load Balancer (Ingressリソース) の使い方 AWS Load Balancer Controller 環境では、次のサンプルのように「Kubernetes IngressClass リソースにてコントローラの指定」を、「Kubernetes Ingress リソース定義のアノテーションとして各種パラメータを指定」をすることによって Application Load Balancer(ALB) をプロビジョニングすることができます。\nhttps://docs.aws.amazon.com/ja_jp/eks/latest/userguide/alb-ingress.html\nwarn Kubernetes Ingress リソースのアノテーションに kubernetes.io/ingress.class: alb を指定することでも作成できますが、Kubernetes 1.18 以降は kubernetes.io/ingress.class の利用は非推奨となっておりますのでご注意ください。\n Ingressリソース定義サンプル apiVersion: networking.k8s.io/v1 kind: IngressClass metadata:  name: sample-ingress-class spec:  controller: ingress.k8s.aws/alb --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata:  name: sample-ingress  annotations:  alb.ingress.kubernetes.io/scheme: internet-facing  alb.ingress.kubernetes.io/target-type: ip spec:  ingressClassName: sample-ingress-class  rules:  - http:  paths:  - path: /hello  pathType: Exact  backend:  service:  name: sample-service  port:  number: 80  アノテーションに指定する主要なパラメータとしては次の通りです。なお、ALB Ingress は AWS WAF や AWS Shield による脅威からの保護、Amazon Cognito 認証などさまざまなマネージドサービスとの連携が可能です。アノテーションに指定可能なパラメータの一覧につきましては次の URL を参照してください。\nhttps://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.3/guide/ingress/annotations/\n   アノテーション名 説明     alb.ingress.kubernetes.io/group.name 複数の Ingress リソースを 1 つの ALB に統合する際にグループ名を指定します。   alb.ingress.kubernetes.io/scheme ロードバランサのスキームを指定します。内部向けの場合は \u0026ldquo;internal\u0026quot;、インターネット向けの場合は \u0026ldquo;internet-facing\u0026rdquo; を指定します。   alb.ingress.kubernetes.io/listen-ports ロードバランサの受付ポートを指定します。デフォルトは \u0026rsquo;[{\u0026ldquo;HTTP\u0026rdquo;: 80}]\u0026rsquo; | \u0026lsquo;[{\u0026ldquo;HTTPS\u0026rdquo;: 443}]\u0026rsquo; です。   alb.ingress.kubernetes.io/target-type バックエンドに指定するターゲットタイプを指定します。トラフィックを直接 Pod にルーティングするには \u0026ldquo;ip\u0026rdquo; を指定します。   alb.ingress.kubernetes.io/healthcheck-port バックエンドのヘルスチェック用ポート(\u0026rdquo;traffic-port\u0026quot;/\u0026quot;ポート番号\u0026quot;)を指定します。デフォルトは \u0026ldquo;traffic-port\u0026rdquo; です。   alb.ingress.kubernetes.io/healthcheck-protocol バックエンドのヘルスチェック用プロトコル(\u0026quot;http\u0026quot;/\u0026quot;https\u0026quot;)を指定します。デフォルトは \u0026ldquo;http\u0026rdquo; です。   alb.ingress.kubernetes.io/healthcheck-path バックエンドの HTTP/HTTPS ヘルスチェック用パスを指定します。デフォルトは \u0026ldquo;/\u0026rdquo; です。    ALB Ingress のサンプル それではサンプルアプリケーションをデプロイして ALB Ingress リソースを実際に動かしてみましょう。今回は次のサンプルアプリケーションをベースに少しだけ手を加えたものを用いて動作確認をしていきたいと思います。\nhttps://github.com/istio/istio/tree/master/samples/helloworld\nhelloworld-alb-sample.yaml（サンプルアプリケーション定義ファイル） apiVersion: networking.k8s.io/v1 kind: IngressClass metadata:  name: alb-ingress spec:  controller: ingress.k8s.aws/alb --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata:  name: helloworld  annotations:  alb.ingress.kubernetes.io/scheme: internet-facing  alb.ingress.kubernetes.io/target-type: ip  alb.ingress.kubernetes.io/healthcheck-path: /health spec:  ingressClassName: alb-ingress  rules:  - http:  paths:  - path: /hello  pathType: Exact  backend:  service:  name: helloworld  port:  number: 80 --- apiVersion: v1 kind: Service metadata:  name: helloworld  labels:  app: helloworld  service: helloworld spec:  ports:  - port: 80  targetPort: 5000  name: http  selector:  app: helloworld --- apiVersion: apps/v1 kind: Deployment metadata:  name: helloworld-v1  labels:  app: helloworld  version: v1 spec:  replicas: 1  selector:  matchLabels:  app: helloworld  version: v1  template:  metadata:  labels:  app: helloworld  version: v1  spec:  containers:  - name: helloworld  image: docker.io/istio/examples-helloworld-v1  ports:  - containerPort: 5000  それでは次のコマンドを実行してサンプルアプリケーションをデプロイしていきましょう。\nサンプルアプリケーションのデプロイ export FARGATEPROFILE=\u0026#34;sample-app\u0026#34; export NAMESPACE=\u0026#34;sample\u0026#34;  # Fargate プロファイルの作成 eksctl create fargateprofile --cluster ${CLUSTER} --name ${FARGATEPROFILE} --namespace ${NAMESPACE}  # サンプルアプリケーション用 Namespace の作成 kubectl create namespace ${NAMESPACE}  # サンプルアプリケーションのデプロイ kubectl apply -n ${NAMESPACE} -f helloworld-alb-sample.yaml  # Ingress がデプロイされたことを確認 kubectl get -n ${NAMESPACE} ingress helloworld  ALB Ingress のデプロイに成功した場合は次の出力例のように Ingress リソース一覧に表示されます。\n出力例 $ kubectl get -n ${NAMESPACE} ingress helloworld NAME CLASS HOSTS ADDRESS PORTS AGE helloworld \u0026lt;none\u0026gt; * k8s-sample-hellowor-xxxxxxxxxx-xxxxxxxx.ap-northeast-1.elb.amazonaws.com 80 70s  DNS への登録などが反映されるまで数分待った後、ALB のパブリックエンドポイントにアクセスしてみましょう。\nサンプルアプリケーションへアクセス # NLB の DNS 名を取得 ENDPOINT=$(kubectl get -n ${NAMESPACE} ingress helloworld \\  -o custom-columns=HOSTNAME:status.loadBalancer.ingress[0].hostname \\  --no-headers)  # テストの実行 curl http://${ENDPOINT}/hello  期待通りのレスポンスが返ってくることが確認できたら成功です。\nサンプルアプリケーション出力例 $ curl http://${ENDPOINT}/hello Hello version: v1, instance: helloworld-v1-b9d9d6679-rpfzl  最後にサンプルアプリケーションを削除して動作確認は終了です。お疲れ様でした。\nサンプルアプリケーションの削除 kubectl delete namespace ${NAMESPACE}  終わりに 今回は Amazon Elastic Kubernetes Service(EKS) を利用する際に併せて利用したい AWS Load Balancer Controller のご紹介でしたが、いかがだったでしょうか？\n今回は IaC を実現する 1 つの手段として、というカットでの紹介でしたが Kubernetes Ingress/Service リソースを ELB にすることでさまざまなメリットが期待できますので、Amazon EKS をご利用の際は AWS Load Balancer Controller の併用も視野に入れてみてはいかがでしょうか。\nなお、今回は紹介しませんでしたが AWS Load Balancer Controller は使いたいけれど、ELB リソースのライフサイクルは Kubernetes からは切り離したいというケースもあるかと思います。そういった場合は AWS Load Balancer Controller の TargetGroupBinding 機能を利用することで実現可能なので参考にしていただければと思います。TargetGroupBinding 機能の詳細については公式ドキュメントを参照してください。\nhttps://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.3/guide/targetgroupbinding/targetgroupbinding/\n以上、Kubernetes Service/Ingress リソースと Elastic Load Balancing(ELB) との統合を実現する「AWS Load Balancer Controller」のご紹介でした。\n  AWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 Kubernetes は、The Linux Foundation の米国およびその他の国における登録商標または商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。  ","permalink":"https://chacco38.github.io/posts/2021/12/aws-load-balancer-controller/","summary":"はじめに みなさん、こんにちは。今回は Amazon Elastic Kubernetes Service(EKS) を利用する際に併せて利用したい AWS Load Balancer Controller のお話です。\nみなさんは Amazon EKS を活用して Kubernetes クラスタを AWS 上で動かすとなった際に、他のマネージドサービスの利用はどうされていますか。もちろんすべて Kubernetes 上で動かしてシステムを完結させるという選択肢もあるかと思いますが、やはり多くの方が他の AWS のマネージドサービスの併用も検討されるのではないでしょうか。その一方で、これら併用環境のコード化 (IaC、Infrastructure as Code) を実現しようとすると、Kubernetes アプリケーションの管理は Helm で、AWS リソースの管理は Terraform で、などという別々のツールでの管理になってしまいがちです。\nそんな悩みを解決する一つの手段が AWS Load Balancer Controller や AWS Controllers for Kubernetes といった Kubernetes クラスタ機能を拡張する各種コントローラの活用です。これらのコントローラを利用することで、AWS リソースについても Kubernetes マニフェストファイルで定義できるようになり、Kubernetes 側に運用管理を寄せてシンプル化することができます。\n今回はそのうちの一つ、Elastic Load Balancing(ELB) を Kubernetes クラスタで管理できるようにする AWS Load Balancer Controller について、簡単なサンプルアプリケーションを交えて紹介していきたいと思います。これから Amazon EKS 上にアプリケーションを展開しようと考えている方は参考にしてみてはいかがでしょうか。\nAWS Load Balancer Controller とは AWS Load Balancer Controller (旧AWS ALB Ingress Controller) は、ELB を Kubernetes クラスタから管理するためのコントローラです。このコントローラを活用することで、Kubernetes Ingress リソースとして L7 ロードバランサの Application Load Balancer(ALB) を、Kuberntes Service リソースとして L4 ロードバランサの Network Load Balancer(NLB) を利用することができるようになります。","title":"AWS Load Balancer Controller を使って ELB を Kubernetes のマニフェストファイルで管理しよう"},{"content":"はじめに みなさん、こんにちは。今回は単一リージョンに展開した複数 GKE クラスタを単一の Anthos Service Mesh 環境に追加し、GKE クラスタ間で負荷分散を行う方法についてご紹介していきたいと思います。\n複数 GKE クラスタでマルチクラスタメッシュを構築することにより、片方の GKE クラスタを先にバージョンアップし、サービスメッシュのトラフィック制御を使ってバージョンアップしたクラスタ側に少量のトラフィックを流して問題がないことを確認しながら段階的に比重をあげていく、といった「基盤部分も含めたカナリアリリース」のユースケースも容易に実現できるようになる見込みです。\nもちろん公式ドキュメントにもマルチクラスタメッシュの構築に関する記載はあるのですが、単にクラスタ間で分散されたことを確認しただけで終わっており、ルーティングの設定やメッシュの外からの通信に関する記載はなかったため、今回はここら辺も含めて一気通貫でご紹介したいと思います。もしこれから Anthos Service Mesh 環境の利用を検討している方は参考にしてみてはいかがでしょうか。\n構築するシステムについて 次の図に示すように限定公開クラスタおよび承認済みネットワーク機能を有効化した単一リージョンの複数 GKE クラスタに対して Anthos Service Mesh (マネージドコントロールプレーン)を導入し、サービスメッシュ上でサンプルアプリケーションを動かしていきたいと思います。なお、今回の例では GKE、Anthos Service Mesh のいずれのリリースチャンネルについても安定性重視の Stable チャンネルを採用しています。\nそれでは構築していきましょう 公式ドキュメントを参考にしつつ、公式ドキュメントに書かれていない部分を補足しながら構築をしていきたいと思います。\nhttps://cloud.google.com/service-mesh/docs/unified-install/gke-install-multi-cluster\nStep1. VPC ネットワークの作成 まずは GKE ノードを配置する VPC ネットワークおよび東京リージョンにサブネットを作成します。今回の例では GKE ノードからプライベートネットワーク経由で Artifact Registry などの他のマネージドサービスへアクセスできるように限定公開の Google アクセスをオンにしています。\nVPCネットワークの作成 # 環境変数の設定 export NETWORK=\u0026#34;matt-vpc\u0026#34; export SUBNET=\u0026#34;matt-private-vm\u0026#34; export LOCATION=\u0026#34;asia-northeast1\u0026#34; export IP_RANGE=\u0026#34;172.16.0.0/16\u0026#34;  # VPC ネットワークの作成 gcloud compute networks create ${NETWORK} --subnet-mode=custom  # サブネットの作成 gcloud compute networks subnets create ${SUBNET} \\  --network=${NETWORK} --range=${IP_RANGE} --region=${LOCATION} \\  --enable-private-ip-google-access  プライベートネットワーク経由でインターネット上の Docker Hub などへ接続できるよう Cloud NAT も作成しておきます。\nCloud NATの作成 # 環境変数の設定 export NAT_GATEWAY=\u0026#34;matt-tokyo-nat\u0026#34; export NAT_ROUTER=\u0026#34;matt-tokyo-router\u0026#34;  # Cloud Router の作成 (東京リージョン) gcloud compute routers create ${NAT_ROUTER} \\  --network=${NETWORK} --region=${LOCATION}  # Cloud NAT の作成 (東京リージョン) gcloud compute routers nats create ${NAT_GATEWAY} \\  --router=${NAT_ROUTER} \\  --router-region=${LOCATION} \\  --auto-allocate-nat-external-ips \\  --nat-all-subnet-ip-ranges \\  --enable-logging  Step2. GKE クラスタの作成 次に GKE クラスタを作成していきましょう。 Anthos Service Mesh の導入には次のような前提条件を満たす必要があるため、今回はこちらを満たした上で、セキュリティの観点から限定公開クラスタおよび承認済みネットワークの有効化、安定性の観点から Stable チャンネルを指定しています。\n 4 vCPU 以上を搭載したノード 合計 8 vCPU 以上を搭載したノードプール GKE Workload Identity の有効化 GKE リリースチャンネルへの登録 (※1)  ※1: Anthos Service Mesh のマネージドコントロールプレーン機能を使う場合のみ\nGKEクラスタの作成 # 環境変数の設定 export PROJECT_ID=`gcloud config list --format \u0026#34;value(core.project)\u0026#34;` export CLUSTER_1=\u0026#34;matt-tokyo-cluster-1\u0026#34; export CLUSTER_2=\u0026#34;matt-tokyo-cluster-2\u0026#34; export MASTER_IP_RANGE_1=\u0026#34;192.168.0.0/28\u0026#34; export MASTER_IP_RANGE_2=\u0026#34;192.168.8.0/28\u0026#34; export CTX_1=\u0026#34;gke_${PROJECT_ID}_${LOCATION}_${CLUSTER_1}\u0026#34; export CTX_2=\u0026#34;gke_${PROJECT_ID}_${LOCATION}_${CLUSTER_2}\u0026#34;  # GKE クラスタ #1 の作成 gcloud container clusters create ${CLUSTER_1} \\  --region=${LOCATION} \\  --machine-type=\u0026#34;e2-standard-4\u0026#34; \\  --num-nodes=\u0026#34;1\u0026#34; \\  --enable-autoscaling --min-nodes=\u0026#34;1\u0026#34; --max-nodes=\u0026#34;3\u0026#34; \\  --enable-private-nodes --master-ipv4-cidr=${MASTER_IP_RANGE_1} \\  --enable-master-global-access \\  --enable-ip-alias --network=${NETWORK} --subnetwork=${SUBNET} \\  --enable-master-authorized-networks \\  --workload-pool=\u0026#34;${PROJECT_ID}.svc.id.goog\u0026#34; \\  --release-channel=\u0026#34;stable\u0026#34;  # GKE クラスタ #2 の作成 gcloud container clusters create ${CLUSTER_2} \\  --region=${LOCATION} \\  --machine-type=\u0026#34;e2-standard-4\u0026#34; \\  --num-nodes=\u0026#34;1\u0026#34; \\  --enable-autoscaling --min-nodes=\u0026#34;1\u0026#34; --max-nodes=\u0026#34;3\u0026#34; \\  --enable-private-nodes --master-ipv4-cidr=${MASTER_IP_RANGE_2} \\  --enable-master-global-access \\  --enable-ip-alias --network=${NETWORK} --subnetwork=${SUBNET} \\  --enable-master-authorized-networks \\  --workload-pool=\u0026#34;${PROJECT_ID}.svc.id.goog\u0026#34; \\  --release-channel=\u0026#34;stable\u0026#34;  前提条件の詳細については次の公式ドキュメントをご参照ください。\nhttps://cloud.google.com/service-mesh/docs/unified-install/prerequisites\nStep3. Anthos Service Mesh のインストール 管理ツールのダウンロード 最初に Anthos Service Mesh v1.11 から正式な管理ツールとなった asmcli をダウンロードします。\nasmcliツールのダウンロード curl https://storage.googleapis.com/csm-artifacts/asm/asmcli_1.11 \u0026gt; asmcli  # 実行権限の付与 chmod +x asmcli  GKE クラスタ #1 へのインストール まずは GKE クラスタ #1 に Anthos Service Mesh をインストールしていきましょう。Kubernetes API へ接続できるように GKE コントロールプレーンの承認済みネットワークに Cloud Shell の IP アドレスを登録し、kubectl を実行できるようにクラスタ認証情報を取得します。\nクラスタ認証情報の取得(クラスタ#1) # CloudShellの承認済みネットワーク登録 gcloud container clusters update ${CLUSTER_1} \\  --region ${LOCATION} \\  --enable-master-authorized-networks \\  --master-authorized-networks \\  \u0026#34;$(dig +short myip.opendns.com @resolver1.opendns.com)/32\u0026#34;  # クラスタ認証情報の取得 gcloud container clusters get-credentials ${CLUSTER_1} \\  --region ${LOCATION}  次に asmcli を使って Anthos Service Mesh をインストールします。コマンドが完了するまでおおよそ 5 分程度かかりました。\nAnthos Service Meshのインストール(クラスタ#1) ./asmcli install \\  --project_id ${PROJECT_ID} \\  --cluster_location ${LOCATION} \\  --cluster_name ${CLUSTER_1} \\  --managed \\  --channel \u0026#34;stable\u0026#34; \\  --enable-all \\  --output_dir ${CLUSTER_1}  次のようなメッセージが出力されましたらインストールに成功です。\nインストール成功時のメッセージ出力 asmcli: Successfully installed ASM.  GKE クラスタ #2 へのインストール 同様に GKE クラスタ #2 にも Anthos Service Mesh をインストールしましょう。\nAnthos Service Meshのインストール(クラスタ#2) # CloudShellの承認済みネットワーク登録 gcloud container clusters update ${CLUSTER_2} \\  --region ${LOCATION} \\  --enable-master-authorized-networks \\  --master-authorized-networks \\  \u0026#34;$(dig +short myip.opendns.com @resolver1.opendns.com)/32\u0026#34;  # クラスタ認証情報の取得 gcloud container clusters get-credentials ${CLUSTER_2} \\  --region ${LOCATION}  # Anthos Service Mesh のインストール ./asmcli install \\  --project_id ${PROJECT_ID} \\  --cluster_location ${LOCATION} \\  --cluster_name ${CLUSTER_2} \\  --managed \\  --channel \u0026#34;stable\u0026#34; \\  --enable-all \\  --output_dir ${CLUSTER_2}  ファイアウォールルールの更新 (限定公開クラスタ時のみ) 限定公開クラスタに Anthos Service Mesh をインストールした場合は、コントロールプレーンからのポート 15017 による通信を追加で許可する必要があります。次のコマンド実行してコントロールプレーンからのポート 15017 による通信を許可します。\nファイアウォールルールの更新(限定公開クラスタ時のみ) # 既存のファイアウォールルールに 15017/TCP の許可ルールを追加 (東京リージョン) gcloud compute firewall-rules update \\  $(gcloud compute firewall-rules list \\  --filter=\u0026#34;name~${CLUSTER_1}-.*-master\u0026#34; --format=\u0026#34;value(name)\u0026#34;) \\  --allow tcp:10250,tcp:443,tcp:15017  # 既存のファイアウォールルールに 15017/TCP の許可ルールを追加 (大阪リージョン) gcloud compute firewall-rules update \\  $(gcloud compute firewall-rules list \\  --filter=\u0026#34;name~${CLUSTER_2}-.*-master\u0026#34; --format=\u0026#34;value(name)\u0026#34;) \\  --allow tcp:10250,tcp:443,tcp:15017  Step4. マルチクラスタメッシュの設定 クラスタ間通信の許可 クラスタをまたがってのサービス間通信ができるように次のコマンドを実行してファイアウォールルール \u0026quot;VPCネットワーク名\u0026quot;-istio-multicluster-pods を新たに作成します。\nクラスタ間通信の許可 # 環境変数の設定 CLUSTER_1_CIDR=$(gcloud container clusters list \\  --filter=\u0026#34;name~${CLUSTER_1}\u0026#34; --format=\u0026#39;value(clusterIpv4Cidr)\u0026#39;) CLUSTER_2_CIDR=$(gcloud container clusters list \\  --filter=\u0026#34;name~${CLUSTER_2}\u0026#34; --format=\u0026#39;value(clusterIpv4Cidr)\u0026#39;) CLUSTER_1_NETTAG=$(gcloud compute instances list \\  --filter=\u0026#34;name~${CLUSTER_1::20}\u0026#34; --format=\u0026#39;value(tags.items.[0])\u0026#39; | \\  grep ${CLUSTER_1} | sort -u) CLUSTER_2_NETTAG=$(gcloud compute instances list \\  --filter=\u0026#34;name~${CLUSTER_2::20}\u0026#34; --format=\u0026#39;value(tags.items.[0])\u0026#39; | \\  grep ${CLUSTER_2} | sort -u)  # クラスタ間通信を許可するファイアウォールルールの作成 gcloud compute firewall-rules create \u0026#34;${NETWORK}-istio-multicluster-pods\u0026#34; \\  --network=${NETWORK} \\  --allow=tcp,udp,icmp,esp,ah,sctp \\  --direction=INGRESS \\  --priority=900 \\  --source-ranges=\u0026#34;${CLUSTER_1_CIDR},${CLUSTER_2_CIDR}\u0026#34; \\  --target-tags=\u0026#34;${CLUSTER_1_NETTAG},${CLUSTER_2_NETTAG}\u0026#34;  クラスタ間サービスディスカバリの設定 次のコマンドを実行し、クラスタ間でサービスの自動検出ができるように asmcli を使って設定を行います。\nクラスタ間サービスディスカバリの設定 ./asmcli create-mesh ${PROJECT_ID} \\  ${PROJECT_ID}/${LOCATION}/${CLUSTER_1} \\  ${PROJECT_ID}/${LOCATION}/${CLUSTER_2}  シークレット情報の更新 (限定公開クラスタ時のみ) 限定公開クラスタで構築した場合は、Anthos Service Mesh コントロールプレーンから他の GKE クラスタコントロールプレーンへプライベートネットワーク経由でアクセスできるようにシークレット情報を書き換えましょう。\nシークレット情報の更新(限定公開クラスタ時のみ) # 環境変数の設定 CLUSTER_1_PRIV_IP=$(gcloud container clusters describe \u0026#34;${CLUSTER_1}\u0026#34; \\  --region \u0026#34;${LOCATION}\u0026#34; --format \u0026#34;value(privateClusterConfig.privateEndpoint)\u0026#34;) CLUSTER_2_PRIV_IP=$(gcloud container clusters describe \u0026#34;${CLUSTER_2}\u0026#34; \\  --region \u0026#34;${LOCATION}\u0026#34; --format \u0026#34;value(privateClusterConfig.privateEndpoint)\u0026#34;)  # プライベートエンドポイントに書き換えたシークレット情報の作成 (クラスタ#1) ./${CLUSTER_1}/istioctl x create-remote-secret \\  --context=${CTX_1} --name=${CLUSTER_1} \\  --server=https://${CLUSTER_1_PRIV_IP} \u0026gt; ${CTX_1}.secret  # プライベートエンドポイントに書き換えたシークレット情報の作成 (クラスタ#2) ./${CLUSTER_1}/istioctl x create-remote-secret \\  --context=${CTX_2} --name=${CLUSTER_2} \\  --server=https://${CLUSTER_2_PRIV_IP} \u0026gt; ${CTX_2}.secret  # シークレット情報の更新 (クラスタ#1) kubectl apply -f ${CTX_2}.secret --context=${CTX_1}  # シークレット情報の更新 (クラスタ#2) kubectl apply -f ${CTX_1}.secret --context=${CTX_2}  ここまで終わりましたらクラスタ間で Kubernetes サービスがロードバランシングされるようになります。\nクラスタ間ロードバランシングの動作確認 構築はまだ続きますがいったんこの状態で、Anthos Service Mesh をインストールした際に \u0026ndash;output_dir で指定したディレクトリへ格納されているサンプルアプリケーションの中から HelloWorld と Sleep というアプリケーションを使用して、クラスタ間で負荷が分散されることを実際に確認していきたいと思います。サンプルアプリケーションの詳細につきましては次の URL をご参照ください。\nhttps://github.com/istio/istio/tree/master/samples\n現時点では何もルーティング設定をしていないため、次の図のように 50% ずつトラフィックが振り分けられる状態を確認できるかと思います。\nサンプルアプリケーションのデプロイ それではサンプルアプリケーションをデプロイしていきましょう。まずは次のコマンドでサンプルアプリケーション用の Namespace を新たに作成します。\nサンプルアプリケーション用Namespaceの作成 # 環境変数の設定 export SAMPLE_NAMESPACE=\u0026#34;sample\u0026#34;  # 両クラスタにサンプルアプリケーション用 Namespace リソースの作成 for CTX in ${CTX_1} ${CTX_2} do  kubectl create --context=${CTX} namespace ${SAMPLE_NAMESPACE}  kubectl label --context=${CTX} namespace ${SAMPLE_NAMESPACE} \\  istio.io/rev=asm-managed-stable --overwrite done  次に HelloWorld および Sleep アプリケーションをデプロイしましょう。どちらのクラスタ上の Pod に振り分けられたかをわかりやすくするため、クラスタ #1 に HelloWorld アプリケーションの v1、クラスタ #2 に v2 をデプロイしています。\nサンプルアプリケーションのデプロイ # 両クラスタに HelloWorld サービスのデプロイ for CTX in ${CTX_1} ${CTX_2} do  kubectl apply --context=${CTX} -n ${SAMPLE_NAMESPACE} \\  -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\  -l service=\u0026#34;helloworld\u0026#34; done  # クラスタ #1 に HelloWorld アプリケーションの v1 をデプロイ kubectl apply --context=${CTX_1} -n ${SAMPLE_NAMESPACE} \\  -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\  -l version=\u0026#34;v1\u0026#34;  # クラスタ #2 に HelloWorld アプリケーションの v2 をデプロイ kubectl apply --context=${CTX_2} -n ${SAMPLE_NAMESPACE} \\  -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\  -l version=\u0026#34;v2\u0026#34;  # 両クラスタに Sleep サービス、アプリケーションのデプロイ for CTX in ${CTX_1} ${CTX_2} do  kubectl apply --context=${CTX} -n ${SAMPLE_NAMESPACE} \\  -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/sleep/sleep.yaml done  サービス間通信の実行 それでは Sleep アプリケーションから HelloWorld アプリケーションへのサービス間通信をしてみましょう。次のコマンドでは各クラスタ上の Sleep アプリケーションからそれぞれ 10 回ずつ HelloWorld サービスへの通信を実施しています。\nサービス間通信の実行例 for CTX in ${CTX_1} ${CTX_2} do  for x in `seq 1 10`  do  kubectl exec --context=\u0026#34;${CTX}\u0026#34; -n sample -c sleep \\  \u0026#34;$(kubectl get pod --context=\u0026#34;${CTX}\u0026#34; -n sample -l \\  app=sleep -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;)\u0026#34; \\  -- curl -sS helloworld.${SAMPLE_NAMESPACE}:5000/hello  done  echo \u0026#39;---\u0026#39; done  次の出力例のように両クラスタから v1 と v2 の Pod へランダムで 50% ずつトラフィックが振り分けられる状態を確認できるかと思います。\nメッセージ出力例 Hello version: v1, instance: helloworld-v1-776f57d5f6-62c9f Hello version: v1, instance: helloworld-v1-776f57d5f6-62c9f Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb : --- : Hello version: v1, instance: helloworld-v1-776f57d5f6-62c9f Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v1, instance: helloworld-v1-776f57d5f6-62c9f  以上でクラスタ間ロードバランシングの動作確認は完了です。\nStep5. 高度なクラスタ間ロードバランシングの設定 次にもう少し高度なクラスタ間ロードバランシングの設定をしていきましょう。ユースケースはいくつかありますが、今回は次の図のように GKE クラスタ #2 側の GKE クラスタのアップグレード後にアプリケーションの動作に影響がないかを少量のトラフィックを流して確認し、問題ないことを確認できたらその比重を段階的にあげていく、といったカナリアリリースのシナリオを想定した振り分け制御を行っていきたいと思います。\nIstio リソースのデプロイ それでは設定していきましょう。まずは Istio VirtualService リソース1および Istio DestinationRule リソース2の定義ファイルを作成しましょう。例のように VirtualService リソースにサブセットごとの振り分けの重みづけを、DestinationRule リソースにサブセットの定義をします。\nhelloworld-virtualservice.yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata:  name: helloworld spec:  hosts:  - helloworld  http:  - route:  - destination:  host: helloworld  subset: v1  weight: 80  - destination:  host: helloworld  subset: v2  weight: 20  helloworld-destinationrule.yaml apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata:  name: helloworld spec:  host: helloworld  subsets:  - name: v1  labels:  version: v1  - name: v2  labels:  version: v2  次のコマンドで両クラスタに Istio リソースをデプロイしましょう。これで設定は終わりです。\nIstioリソースのデプロイ # 両クラスタに VirtualService リソースをデプロイ for CTX in ${CTX_1} ${CTX_2} do  kubectl apply --context=${CTX} -n ${SAMPLE_NAMESPACE} \\  -f helloworld-virtualservice.yaml done  # 両クラスタに DestinationRule リソースをデプロイ for CTX in ${CTX_1} ${CTX_2} do  kubectl apply --context=${CTX} -n ${SAMPLE_NAMESPACE} \\  -f helloworld-destinationrule.yaml done  カナリアリリースの動作確認 構築はまだ続きますがいったんこの状態で、クラスタ間で負荷が設定どおりの比重で分散されることを実際に確認していきたいと思います。クラスタ間ロードバランシングの動作確認と同様に Sleep アプリケーションから HelloWorld アプリケーションへのサービス間通信をしてみましょう。\nサービス間通信の実行例 for x in `seq 1 10` do  kubectl exec --context=\u0026#34;${CTX_1}\u0026#34; -n sample -c sleep \\  \u0026#34;$(kubectl get pod --context=\u0026#34;${CTX_1}\u0026#34; -n sample -l \\  app=sleep -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;)\u0026#34; \\  -- curl -sS helloworld.${SAMPLE_NAMESPACE}:5000/hello done  10 回の実行では試行回数が少ないため誤差はあるかと思いますが、出力例のように v1 への振り分けが約 80%、v2 への振り分けが約 20% となることが確認できるかと思います。\nメッセージ出力例 Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v  以上で少し高度なクラスタ間ロードバランシング設定の動作確認もできました。\nStep6. Ingress ゲートウェイの設定 さてここからは話題がガラッと変わり、メッシュの外からの通信を受け入れるための Ingress ゲートウェイの設定をしていきたいと思います。今回は次の図のように各クラスタに配置された Ingress ゲートウェイアプリケーションを束ねるようにマルチクラスタ Ingress およびマルチクラスタ Service を配置する構成を作っていきます。\nマルチクラスタ Ingress 機能の有効化 最初に次のコマンドを実行し、マルチクラスタ Ingress 機能を有効化しましょう。なお、今回はマルチクラスタ Ingress の設定を行うメインの GKE クラスタ(=構成クラスタ)として、GKE クラスタ #1 を登録しています。\nマルチクラスタIngress機能の有効化 gcloud container hub ingress enable \\  --config-membership=${CLUSTER_1}  Ingress ゲートウェイ定義ファイルの作成 今回は Anthos Service Mesh をインストールした際に --output_dir で指定したディレクトリへ Ingress ゲートウェイのサンプル定義ファイルが配置されているのでこちらをベースに作成していきたいと思います。まずはサンプル定義ファイルを複製し、マルチクラスタ Ingress 構成向けに MultiClusterService3、BackendConfig4、MultiClusterIngress5 の 3 種類のリソース定義ファイルを追加していきましょう。\nIngressゲートウェイ定義ファイルの作成準備 # サンプル定義ファイルを複製 cp -r ${CLUSTER_1}/samples/gateways/istio-ingressgateway .  # マルチクラスタ Ingress 定義ファイルを格納するディレクトリを作成 mkdir -p istio-ingressgateway/multicluster  # Service リソース定義から MultiClusterService リソース定義ファイルに書き換え mv istio-ingressgateway/service.yaml istio-ingressgateway/multicluster/multiclusterservice.yaml  # 新たな定義ファイルを 2 種類作成 touch istio-ingressgateway/multicluster/backendconfig.yaml touch istio-ingressgateway/multicluster/multiclusteringress.yaml  それでは MultiClusterService3、BackendConfig4、MultiClusterIngress5 の 3 種類のリソース定義ファイルを編集していきましょう。MultiClusterService は Service リソースをマルチクラスタに対応させたリソースという位置づけのため、基本的に Service リソースの設定値とほぼ変わりません。今回は Ingress をフロントに配置するので LoadBalancer タイプの定義を削除し、デフォルトの Cluster IP に変更しています。\nistio-ingressgateway/multicluster/multiclusterservice.yaml（差分） - apiVersion: v1 + apiVersion: networking.gke.io/v1 - kind: Service + kind: MultiClusterService  metadata:  name: istio-ingressgateway + annotations: + cloud.google.com/backend-config: \u0026#39;{\u0026#34;default\u0026#34;: \u0026#34;ingress-backendconfig\u0026#34;}\u0026#39;  labels:  app: istio-ingressgateway  istio: ingressgateway  spec: - ports: - # status-port exposes a /healthz/ready endpoint that can be used with GKE Ingress health checks - - name: status-port - port: 15021 - protocol: TCP - targetPort: 15021 - # Any ports exposed in Gateway resources should be exposed here. - - name: http2 - port: 80 - - name: https - port: 443 - selector: - istio: ingressgateway - app: istio-ingressgateway - type: LoadBalancer + template: + spec: + ports: + # status-port exposes a /healthz/ready endpoint that can be used with GKE Ingress health checks + - name: status-port + port: 15021 + protocol: TCP + targetPort: 15021 + # Any ports exposed in Gateway resources should be exposed here. + - name: http2 + port: 80 + - name: https + port: 443 + selector: + istio: ingressgateway + app: istio-ingressgateway   BackendConfig リソースではバックエンドサービスである Ingress ゲートウェイアプリケーションのヘルスチェックに関する定義を記載します。Ingress ゲートウェイはヘルスチェック用パスとして /healthz/ready:15021 を用意しているため、こちらを設定しましょう。\nistio-ingressgateway/multicluster/backendconfig.yaml（差分） + apiVersion: cloud.google.com/v1 + kind: BackendConfig + metadata: + name: ingress-backendconfig + spec: + healthCheck: + requestPath: /healthz/ready + port: 15021 + type: HTTP   MultiClusterIngress は Ingress リソースをマルチクラスタに対応させたリソースという位置づけであり、基本的に Ingress リソースを定義するときと設定値はほぼ同じです。\nistio-ingressgateway/multicluster/multiclusteringress.yaml（差分） + apiVersion: networking.gke.io/v1beta1 + kind: MultiClusterIngress + metadata: + name: istio-ingressgateway + labels: + app: istio-ingressgateway + istio: ingressgateway + spec: + template: + spec: + backend: + serviceName: istio-ingressgateway + servicePort: 80   Ingress ゲートウェイのデプロイ まずは Ingress ゲートウェイリソースをデプロイする Namespace を新たに作成します。今回の例では istio-gateway という名前の Namespace を作成しています。\nIngress Gateway用のNamespace作成 # 環境変数の設定 export GATEWAY_NAMESPACE=\u0026#34;istio-gateway\u0026#34;  # 両クラスタにサンプルアプリケーション用 Namespace リソースの作成 for CTX in ${CTX_1} ${CTX_2} do  kubectl create --context=${CTX} namespace ${GATEWAY_NAMESPACE}  kubectl label --context=${CTX} namespace ${GATEWAY_NAMESPACE} \\  istio.io/rev=asm-managed-stable --overwrite done  次のコマンドを実行して Ingress ゲートウェイアプリケーションを両クラスタにデプロイしましょう。\nIngress Gatewayアプリケーションのデプロイ for CTX in ${CTX_1} ${CTX_2} do  kubectl apply -n ${GATEWAY_NAMESPACE} --context=${CTX} \\  -f istio-ingressgateway done  最後にマルチクラスタ Ingress リソースを構成クラスタである GKE クラスタ #1 に対してデプロイをしましょう。\nIngress Gatewayアプリケーションのデプロイ kubectl apply -n ${GATEWAY_NAMESPACE} --context=${CTX_1} \\  -f istio-ingressgateway/multicluster  以上で Ingress ゲートウェイのデプロイは終わりです。\nIstio リソースのデプロイ Ingress ゲートウェイを通じてメッシュの外から HelloWorld アプリケーションへ通信ができるように Istio リソースの定義を行っていきたいと思います。まずは Istio Gateway リソース6および Istio VirtualService リソース1の定義ファイルを作成しましょう。例のように Gateway リソースにメッシュ外から受け付けるポートとプロトコルを定義し、VirtualService リソースには Gateway に入ってきた通信のパターンマッチ条件と振り分け先バックエンドの指定をします。\nhelloworld-gateway.yaml apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata:  name: helloworld-gateway spec:  selector:  istio: ingressgateway  servers:  - port:  number: 80  name: http  protocol: HTTP  hosts:  - \u0026#34;*\u0026#34; --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata:  name: helloworld-gateway spec:  hosts:  - \u0026#34;*\u0026#34;  gateways:  - helloworld-gateway  http:  - match:  - uri:  exact: /hello  route:  - destination:  host: helloworld  port:  number: 5000  次のコマンドで両クラスタに Istio リソースをデプロイし、アプリケーションへのインバウンド通信ができるように設定しましょう。\nIstioリソースのデプロイ for CTX in ${CTX_1} ${CTX_2} do  kubectl apply -n ${SAMPLE_NAMESPACE} --context=${CTX} \\  -f helloworld-gateway.yaml done  以上でメッシュ外からのアプリケーションへのインバウンド通信もできるようになりました。\nインバウンド通信の動作確認 それではメッシュ外からのアプリケーションへのインバウンド通信ができることを確認していきましょう。実行例のように Ingress ゲートウェイの外部 IP アドレスに対して curl コマンドを実行し、アクセスをしてみましょう。\nインバウンド通信の実行 # Ingress ゲートウェイの外部 IP アドレスの取得 INGRESS_GATEWAY_IP=$(kubectl --context=${CTX_1} \\  -n ${GATEWAY_NAMESPACE} get MultiClusterIngress \\  -o custom-columns=VIP:status.VIP --no-headers)  for x in `seq 1 10` do  curl http://${INGRESS_GATEWAY_IP}/hello done  次の出力例のように両クラスタから v1 と v2 の Pod へランダムで 50% ずつトラフィックが振り分けられる状態を確認できるかと思います。\nメッセージ出力例 Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb  Step7. インバウンド通信に対する高度なロードバランシングの設定 メッシュの外からアプリケーションへのインバウンド通信に対する高度なロードバランシングの設定をしていきましょう。今回は Step5 のときとは逆に 80% を v2、20% を v1 に割り振るように設定していきたいと思います。\nIstio リソースの更新 それでは先ほど作成した Istio リソース定義ファイル helloworld-gateway.yaml の VirtualService リソース部分を編集し、サブセットごとの振り分け比重の定義を追加します。なお、サブセットの定義(DestinationRule リソース)については「Step5. 高度なクラスタ間ロードバランシングの設定」にて設定済みとなりますのでここでは省略します。\nhelloworld-gateway.yaml（差分）  apiVersion: networking.istio.io/v1alpha3  kind: VirtualService  metadata:  name: helloworld-gateway  spec:  hosts:  - \u0026#34;*\u0026#34;  gateways:  - helloworld-gateway  http:  - match:  - uri:  exact: /hello  route:  - destination:  host: helloworld  port:  number: 5000 + subset: v1 + weight: 20 + - destination: + host: helloworld + port: + number: 5000 + subset: v2 + weight: 80   それでは次のコマンドで Istio リソースを更新しましょう。これで設定は終わりです。\nIstioリソースの更新 for CTX in ${CTX_1} ${CTX_2} do  kubectl apply -n ${SAMPLE_NAMESPACE} --context=${CTX} \\  -f helloworld-gateway.yaml done  カナリアリリースの動作確認 それでは HelloWorld アプリケーションへの振り分けが設定どおりの比重で分散されることを実際に確認していきたいと思います。実行例のように Ingress ゲートウェイの外部 IP アドレスに対して curl コマンドを実行し、アクセスをしてみましょう。\nインバウンド通信の実行 # Ingress ゲートウェイの外部 IP アドレスの取得 INGRESS_GATEWAY_IP=$(kubectl --context=${CTX_1} \\  -n ${GATEWAY_NAMESPACE} get MultiClusterIngress \\  -o custom-columns=VIP:status.VIP --no-headers)  for x in `seq 1 10` do  curl http://${INGRESS_GATEWAY_IP}/hello done  10 回の実行では試行回数が少ないため誤差はあるかと思いますが、出力例のように v1 への振り分けが約 20%、v2 への振り分けが約 80% となることが確認できるかと思います。\nメッセージ出力例 Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb  以上でメッシュの外からアプリケーションへのインバウンド通信に対する高度なロードバランシングの動作確認も終了です。お疲れ様でした。\n終わりに 今回は単一リージョンに展開した複数 GKE クラスタを単一の Anthos Service Mesh 環境に追加し、GKE クラスタ間で負荷分散を行う方法についてご紹介でしたがいかがだったでしょうか。\n複数 GKE クラスタでマルチクラスタメッシュを構築することにより、片方の GKE クラスタを先にバージョンアップし、サービスメッシュのトラフィック制御を使ってバージョンアップしたクラスタ側に少量のトラフィックを流して問題がないことを確認しながら段階的に比重をあげていく、といった「基盤部分も含めたカナリアリリース」のユースケースも容易に実現できるようになる見込みです。もしこれから Anthos Service Mesh 環境の利用を検討している方はマルチクラスタメッシュ構成についても検討してみてはいかがでしょうか。\n  Google Cloud は、Google LLC の商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。    VirtualService はトラフィックの振り分け、ルーティングを定義する Istio リソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n DestinationRule は転送先サービスのサブセット化や各種トラフィックポリシーを定義する Istio リソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n MultiClusterService は Service リソースを複数のクラスタ上に展開する GKE 独自のカスタムリソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n BackendConfig はバックエンドサービスのヘルスチェックを定義する GKE 独自のカスタムリソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n MultiClusterIngress はマルチクラスタに対応した Ingress リソースを定義する GKE 独自のカスタムリソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Gateway は Ingress/Egress ゲートウェイで受け付けるポート、プロトコルを定義する Istio リソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://chacco38.github.io/posts/2021/12/gcp-multi-asm-cluster/","summary":"はじめに みなさん、こんにちは。今回は単一リージョンに展開した複数 GKE クラスタを単一の Anthos Service Mesh 環境に追加し、GKE クラスタ間で負荷分散を行う方法についてご紹介していきたいと思います。\n複数 GKE クラスタでマルチクラスタメッシュを構築することにより、片方の GKE クラスタを先にバージョンアップし、サービスメッシュのトラフィック制御を使ってバージョンアップしたクラスタ側に少量のトラフィックを流して問題がないことを確認しながら段階的に比重をあげていく、といった「基盤部分も含めたカナリアリリース」のユースケースも容易に実現できるようになる見込みです。\nもちろん公式ドキュメントにもマルチクラスタメッシュの構築に関する記載はあるのですが、単にクラスタ間で分散されたことを確認しただけで終わっており、ルーティングの設定やメッシュの外からの通信に関する記載はなかったため、今回はここら辺も含めて一気通貫でご紹介したいと思います。もしこれから Anthos Service Mesh 環境の利用を検討している方は参考にしてみてはいかがでしょうか。\n構築するシステムについて 次の図に示すように限定公開クラスタおよび承認済みネットワーク機能を有効化した単一リージョンの複数 GKE クラスタに対して Anthos Service Mesh (マネージドコントロールプレーン)を導入し、サービスメッシュ上でサンプルアプリケーションを動かしていきたいと思います。なお、今回の例では GKE、Anthos Service Mesh のいずれのリリースチャンネルについても安定性重視の Stable チャンネルを採用しています。\nそれでは構築していきましょう 公式ドキュメントを参考にしつつ、公式ドキュメントに書かれていない部分を補足しながら構築をしていきたいと思います。\nhttps://cloud.google.com/service-mesh/docs/unified-install/gke-install-multi-cluster\nStep1. VPC ネットワークの作成 まずは GKE ノードを配置する VPC ネットワークおよび東京リージョンにサブネットを作成します。今回の例では GKE ノードからプライベートネットワーク経由で Artifact Registry などの他のマネージドサービスへアクセスできるように限定公開の Google アクセスをオンにしています。\nVPCネットワークの作成 # 環境変数の設定 export NETWORK=\u0026#34;matt-vpc\u0026#34; export SUBNET=\u0026#34;matt-private-vm\u0026#34; export LOCATION=\u0026#34;asia-northeast1\u0026#34; export IP_RANGE=\u0026#34;172.16.0.0/16\u0026#34;  # VPC ネットワークの作成 gcloud compute networks create ${NETWORK} --subnet-mode=custom  # サブネットの作成 gcloud compute networks subnets create ${SUBNET} \\  --network=${NETWORK} --range=${IP_RANGE} --region=${LOCATION} \\  --enable-private-ip-google-access  プライベートネットワーク経由でインターネット上の Docker Hub などへ接続できるよう Cloud NAT も作成しておきます。","title":"単一リージョンの複数 GKE クラスタと Anthos Service Mesh でマルチクラスタメッシュ環境を構築してみた"},{"content":"はじめに みなさん、こんにちは。今回は複数のリージョンに展開する各 GKE クラスタを単一の Anthos Service Mesh 環境に追加し、GKE クラスタ間で負荷分散を行う方法についてご紹介していきたいと思います。\n複数リージョンの GKE クラスタでマルチクラスタメッシュを構築することにより、予期しない大規模災害の発生にも耐えうる高い可用性と回復力の実現、エンドユーザからより近い位置への振り分けによるレイテンシの改善といったことが期待できるようになる見込みです。\nもちろん公式ドキュメントにもマルチクラスタメッシュの構築に関する記載はあるのですが、単にクラスタ間で分散されたことを確認しただけで終わっており、ローカリティを意識したルーティングの設定やメッシュの外からの通信に関する記載はなかったため、今回はここら辺も含めて一気通貫でご紹介したいと思います。もしこれからリージョンをまたがった Anthos Service Mesh 環境の利用を検討している方は参考にしてみてはいかがでしょうか。\n構築するシステムについて 次の図に示すように限定公開クラスタおよび承認済みネットワーク機能を有効化した複数リージョンの GKE クラスタに対して Anthos Service Mesh (マネージドコントロールプレーン)を導入しています。サービスメッシュ上ではサンプルアプリケーションを動かし、ローカリティを意識した負荷分散についても設定をしていきたいと思います。なお、今回の例では GKE、Anthos Service Mesh のいずれのリリースチャンネルについても安定性重視の Stable チャンネルを採用しています。\nそれでは構築していきましょう いつも通り公式ドキュメントを参考にしつつ、公式ドキュメントに書かれていない部分を補足しながら構築をしていきたいと思います。\nhttps://cloud.google.com/service-mesh/docs/unified-install/gke-install-multi-cluster\nStep1. VPC ネットワークの作成 まずは GKE ノードを配置する VPC ネットワークおよび東京リージョンと大阪リージョンにサブネットを作成します。今回の例では GKE ノードからプライベートネットワーク経由で Artifact Registry などの他のマネージドサービスへアクセスできるように限定公開の Google アクセスをオンにしています。\nVPCネットワークの作成 # 環境変数の設定 export NETWORK=\u0026#34;matt-vpc\u0026#34; export SUBNET=\u0026#34;matt-private-vm\u0026#34; export LOCATION_1=\u0026#34;asia-northeast1\u0026#34; export LOCATION_2=\u0026#34;asia-northeast2\u0026#34; export IP_RANGE_1=\u0026#34;172.16.0.0/16\u0026#34; export IP_RANGE_2=\u0026#34;172.24.0.0/16\u0026#34;  # VPC ネットワークの作成 gcloud compute networks create ${NETWORK} --subnet-mode=custom  # サブネットの作成 (東京リージョン) gcloud compute networks subnets create ${SUBNET} \\  --network=${NETWORK} --range=${IP_RANGE_1} --region=${LOCATION_1} \\  --enable-private-ip-google-access  # サブネットの作成 (大阪リージョン) gcloud compute networks subnets create ${SUBNET} \\  --network=${NETWORK} --range=${IP_RANGE_2} --region=${LOCATION_2} \\  --enable-private-ip-google-access  プライベートネットワーク経由でインターネット上の Docker Hub などへ接続できるよう Cloud NAT も作成しておきます。\nCloud NATの作成 # 環境変数の設定 export NAT_GATEWAY_1=\u0026#34;matt-tokyo-nat\u0026#34; export NAT_GATEWAY_2=\u0026#34;matt-osaka-nat\u0026#34; export NAT_ROUTER_1=\u0026#34;matt-tokyo-router\u0026#34; export NAT_ROUTER_2=\u0026#34;matt-osaka-router\u0026#34;  # Cloud Router の作成 (東京リージョン) gcloud compute routers create ${NAT_ROUTER_1} \\  --network=${NETWORK} --region=${LOCATION_1}  # Cloud Router の作成 (大阪リージョン) gcloud compute routers create ${NAT_ROUTER_2} \\  --network=${NETWORK} --region=${LOCATION_2}  # Cloud NAT の作成 (東京リージョン) gcloud compute routers nats create ${NAT_GATEWAY_1} \\  --router=${NAT_ROUTER_1} \\  --router-region=${LOCATION_1} \\  --auto-allocate-nat-external-ips \\  --nat-all-subnet-ip-ranges \\  --enable-logging  # Cloud NAT の作成 (大阪リージョン) gcloud compute routers nats create ${NAT_GATEWAY_2} \\  --router=${NAT_ROUTER_2} \\  --router-region=${LOCATION_2} \\  --auto-allocate-nat-external-ips \\  --nat-all-subnet-ip-ranges \\  --enable-logging  Step2. GKE クラスタの作成 次に GKE クラスタを作成していきましょう。 Anthos Service Mesh の導入には次のような前提条件を満たす必要があるため、今回はこちらを満たした上で、セキュリティの観点から限定公開クラスタおよび承認済みネットワークの有効化、安定性の観点から Stable チャンネルを指定しています。\n 4 vCPU 以上を搭載したノード 合計 8 vCPU 以上を搭載したノードプール GKE Workload Identity の有効化 GKE リリースチャンネルへの登録 (※1)  ※1: Anthos Service Mesh のマネージドコントロールプレーン機能を使う場合のみ\nGKEクラスタの作成 # 環境変数の設定 export PROJECT_ID=`gcloud config list --format \u0026#34;value(core.project)\u0026#34;` export CLUSTER_1=\u0026#34;matt-tokyo-cluster-1\u0026#34; export CLUSTER_2=\u0026#34;matt-osaka-cluster-1\u0026#34; export MASTER_IP_RANGE_1=\u0026#34;192.168.0.0/28\u0026#34; export MASTER_IP_RANGE_2=\u0026#34;192.168.8.0/28\u0026#34; export CTX_1=\u0026#34;gke_${PROJECT_ID}_${LOCATION_1}_${CLUSTER_1}\u0026#34; export CTX_2=\u0026#34;gke_${PROJECT_ID}_${LOCATION_2}_${CLUSTER_2}\u0026#34;  # GKE クラスタ(東京リージョン)の作成 gcloud container clusters create ${CLUSTER_1} \\  --region=${LOCATION_1} \\  --machine-type=\u0026#34;e2-standard-4\u0026#34; \\  --num-nodes=\u0026#34;1\u0026#34; \\  --enable-autoscaling --min-nodes=\u0026#34;1\u0026#34; --max-nodes=\u0026#34;3\u0026#34; \\  --enable-private-nodes --master-ipv4-cidr=${MASTER_IP_RANGE_1} \\  --enable-master-global-access \\  --enable-ip-alias --network=${NETWORK} --subnetwork=${SUBNET} \\  --enable-master-authorized-networks \\  --workload-pool=\u0026#34;${PROJECT_ID}.svc.id.goog\u0026#34; \\  --release-channel=\u0026#34;stable\u0026#34;  # GKE クラスタ(大阪リージョン)の作成 gcloud container clusters create ${CLUSTER_2} \\  --region=${LOCATION_2} \\  --machine-type=\u0026#34;e2-standard-4\u0026#34; \\  --num-nodes=\u0026#34;1\u0026#34; \\  --enable-autoscaling --min-nodes=\u0026#34;1\u0026#34; --max-nodes=\u0026#34;3\u0026#34; \\  --enable-private-nodes --master-ipv4-cidr=${MASTER_IP_RANGE_2} \\  --enable-master-global-access \\  --enable-ip-alias --network ${NETWORK} --subnetwork=${SUBNET} \\  --enable-master-authorized-networks \\  --workload-pool=\u0026#34;${PROJECT_ID}.svc.id.goog\u0026#34; \\  --release-channel=\u0026#34;stable\u0026#34;  前提条件の詳細については次の公式ドキュメントをご参照ください。\nhttps://cloud.google.com/service-mesh/docs/unified-install/prerequisites\nStep3. Anthos Service Mesh のインストール 管理ツールのダウンロード 最初に Anthos Service Mesh v1.11 から正式な管理ツールとなった asmcli をダウンロードします。\nasmcliツールのダウンロード curl https://storage.googleapis.com/csm-artifacts/asm/asmcli_1.11 \u0026gt; asmcli  # 実行権限の付与 chmod +x asmcli  東京 GKE クラスタへのインストール まずは東京リージョンの GKE クラスタに Anthos Service Mesh をインストールしていきましょう。Kubernetes API へ接続できるように GKE コントロールプレーンの承認済みネットワークに Cloud Shell の IP アドレスを登録し、kubectl を実行できるようにクラスタ認証情報を取得します。\nクラスタ認証情報の取得(東京リージョン) # CloudShellの承認済みネットワーク登録 gcloud container clusters update ${CLUSTER_1} \\  --region ${LOCATION_1} \\  --enable-master-authorized-networks \\  --master-authorized-networks \\  \u0026#34;$(dig +short myip.opendns.com @resolver1.opendns.com)/32\u0026#34;  # クラスタ認証情報の取得 gcloud container clusters get-credentials ${CLUSTER_1} \\  --region ${LOCATION_1}  次に asmcli を使って Anthos Service Mesh をインストールします。コマンドが完了するまでおおよそ 5 分程度かかりました。\nAnthos Service Meshのインストール(東京リージョン) ./asmcli install \\  --project_id ${PROJECT_ID} \\  --cluster_location ${LOCATION_1} \\  --cluster_name ${CLUSTER_1} \\  --managed \\  --channel \u0026#34;stable\u0026#34; \\  --enable-all \\  --output_dir ${CLUSTER_1}  次のようなメッセージが出力されましたらインストールに成功です。\nインストール成功時のメッセージ出力 asmcli: Successfully installed ASM.  大阪 GKE クラスタへのインストール 同様に大阪リージョンの GKE クラスタにもインストールをしましょう。\nAnthos Servic eMeshのインストール(大阪リージョン) # CloudShellの承認済みネットワーク登録 gcloud container clusters update ${CLUSTER_2} \\  --region ${LOCATION_2} \\  --enable-master-authorized-networks \\  --master-authorized-networks \\  \u0026#34;$(dig +short myip.opendns.com @resolver1.opendns.com)/32\u0026#34;  # クラスタ認証情報の取得 gcloud container clusters get-credentials ${CLUSTER_2} \\  --region ${LOCATION_2}  # Anthos Service Mesh のインストール ./asmcli install \\  --project_id ${PROJECT_ID} \\  --cluster_location ${LOCATION_2} \\  --cluster_name ${CLUSTER_2} \\  --managed \\  --channel \u0026#34;stable\u0026#34; \\  --enable-all \\  --output_dir ${CLUSTER_2}  ファイアウォールルールの更新 (限定公開クラスタ時のみ) 限定公開クラスタに Anthos Service Mesh をインストールした場合は、コントロールプレーンからのポート 15017 による通信を追加で許可する必要があります。次のコマンド実行してコントロールプレーンからのポート 15017 による通信を許可します。\nファイアウォールルールの更新 # 既存のファイアウォールルールに 15017/TCP の許可ルールを追加 (東京リージョン) gcloud compute firewall-rules update \\  `gcloud compute firewall-rules list --filter=\u0026#34;name~${CLUSTER_1}-.*-master\u0026#34; --format=\u0026#34;value(name)\u0026#34;` \\  --allow tcp:10250,tcp:443,tcp:15017  # 既存のファイアウォールルールに 15017/TCP の許可ルールを追加 (大阪リージョン) gcloud compute firewall-rules update \\  `gcloud compute firewall-rules list --filter=\u0026#34;name~${CLUSTER_2}-.*-master\u0026#34; --format=\u0026#34;value(name)\u0026#34;` \\  --allow tcp:10250,tcp:443,tcp:15017  Step4. マルチクラスタメッシュの設定 クラスタ間通信の許可 クラスタをまたがってのサービス間通信ができるように次のコマンドを実行してファイアウォールルール \u0026quot;VPCネットワーク名\u0026quot;-istio-multicluster-pods を新たに作成します。\nクラスタ間通信の許可 # 環境変数の設定 CLUSTER_1_CIDR=$(gcloud container clusters list \\  --filter=\u0026#34;name~${CLUSTER_1}\u0026#34; --format=\u0026#39;value(clusterIpv4Cidr)\u0026#39;) CLUSTER_2_CIDR=$(gcloud container clusters list \\  --filter=\u0026#34;name~${CLUSTER_2}\u0026#34; --format=\u0026#39;value(clusterIpv4Cidr)\u0026#39;) CLUSTER_1_NETTAG=$(gcloud compute instances list \\  --filter=\u0026#34;name~${CLUSTER_1::16}\u0026#34; --format=\u0026#39;value(tags.items.[0])\u0026#39; | \\  grep ${CLUSTER_1} | sort -u) CLUSTER_2_NETTAG=$(gcloud compute instances list \\  --filter=\u0026#34;name~${CLUSTER_2::16}\u0026#34; --format=\u0026#39;value(tags.items.[0])\u0026#39; | \\  grep ${CLUSTER_2} | sort -u)  # クラスタ間通信を許可するファイアウォールルールの作成 gcloud compute firewall-rules create \u0026#34;${NETWORK}-istio-multicluster-pods\u0026#34; \\  --network=${NETWORK} \\  --allow=tcp,udp,icmp,esp,ah,sctp \\  --direction=INGRESS \\  --priority=900 \\  --source-ranges=\u0026#34;${CLUSTER_1_CIDR},${CLUSTER_2_CIDR}\u0026#34; \\  --target-tags=\u0026#34;${CLUSTER_1_NETTAG},${CLUSTER_2_NETTAG}\u0026#34;  承認済みネットワークの追加 (In-cluster かつ承認済みネットワーク有効時のみ) 今回はマネージドコントロールプレーンを利用しているため設定は不要ですが、Anthos Service Mesh を In-cluster で構築した場合は Anthos Service Mesh コントロールプレーンから他 GKE クラスタのコントロールプレーンにアクセスする必要があるため承認済みネットワークを更新します。\n承認済みネットワークの追加（In-cluster時のみ） # 環境変数の設定 POD_IP_CIDR_1=$(gcloud container clusters describe ${CLUSTER_1} \\  --region ${LOCATION_1} --format \u0026#34;value(ipAllocationPolicy.clusterIpv4CidrBlock)\u0026#34;) POD_IP_CIDR_2=$(gcloud container clusters describe ${CLUSTER_2} \\  --region ${LOCATION_2} --format \u0026#34;value(ipAllocationPolicy.clusterIpv4CidrBlock)\u0026#34;)  # 大阪リージョンの Pod アドレス範囲を、東京リージョンの承認済みネットワークに追加 gcloud container clusters update ${CLUSTER_1} \\  --region ${LOCATION_1} \\  --enable-master-authorized-networks \\  --master-authorized-networks \\  \u0026#34;${POD_IP_CIDR_2},$(dig +short myip.opendns.com @resolver1.opendns.com)/32\u0026#34;  # 東京リージョンの Pod アドレス範囲を、大阪リージョンの承認済みネットワークに追加 gcloud container clusters update ${CLUSTER_2} \\  --region ${LOCATION_2} \\  --enable-master-authorized-networks \\  --master-authorized-networks \\  \u0026#34;${POD_IP_CIDR_1},$(dig +short myip.opendns.com @resolver1.opendns.com)/32\u0026#34;  クラスタ間サービスディスカバリの設定 クラスタ間でサービスの自動検出ができるように asmcli を使って設定を行います。\nクラスタ間サービスディスカバリの設定 ./asmcli create-mesh ${PROJECT_ID} \\  ${PROJECT_ID}/${LOCATION_1}/${CLUSTER_1} \\  ${PROJECT_ID}/${LOCATION_2}/${CLUSTER_2}  シークレット情報の更新 (限定公開クラスタ時のみ) 限定公開クラスタで構築した場合は、Anthos Service Mesh コントロールプレーンから他の GKE クラスタコントロールプレーンへプライベートネットワーク経由でアクセスできるようにシークレット情報を書き換えましょう。\nシークレット情報の更新(限定公開クラスタ時のみ) # 環境変数の設定 CLUSTER_1_PRIV_IP=$(gcloud container clusters describe \u0026#34;${CLUSTER_1}\u0026#34; \\  --region \u0026#34;${LOCATION_1}\u0026#34; --format \u0026#34;value(privateClusterConfig.privateEndpoint)\u0026#34;) CLUSTER_2_PRIV_IP=$(gcloud container clusters describe \u0026#34;${CLUSTER_2}\u0026#34; \\  --region \u0026#34;${LOCATION_2}\u0026#34; --format \u0026#34;value(privateClusterConfig.privateEndpoint)\u0026#34;)  # プライベートエンドポイントに書き換えたシークレット情報の作成 (東京リージョン) ./${CLUSTER_1}/istioctl x create-remote-secret \\  --context=${CTX_1} --name=${CLUSTER_1} \\  --server=https://${CLUSTER_1_PRIV_IP} \u0026gt; ${CTX_1}.secret  # プライベートエンドポイントに書き換えたシークレット情報の作成 (大阪リージョン) ./${CLUSTER_1}/istioctl x create-remote-secret \\  --context=${CTX_2} --name=${CLUSTER_2} \\  --server=https://${CLUSTER_2_PRIV_IP} \u0026gt; ${CTX_2}.secret  # シークレット情報の更新 (東京リージョン) kubectl apply -f ${CTX_2}.secret --context=${CTX_1}  # シークレット情報の更新 (大阪リージョン) kubectl apply -f ${CTX_1}.secret --context=${CTX_2}  ここまで終わりましたらクラスタ間で Kubernetes サービスがロードバランシングされるようになります。\nクラスタ間ロードバランシングの動作確認 構築はまだ続きますがいったんこの状態で、Anthos Service Mesh をインストールした際に \u0026ndash;output_dir で指定したディレクトリへ格納されているサンプルアプリケーションの中から HelloWorld と Sleep というアプリケーションを使用して、クラスタ間で負荷が分散されることを実際に確認していきたいと思います。サンプルアプリケーションの詳細につきましては次の URL をご参照ください。\nhttps://github.com/istio/istio/tree/master/samples\n現時点では何もルーティング設定をしていないため、次の図のように 50% ずつトラフィックが振り分けられる状態を確認できるかと思います。\nサンプルアプリケーションのデプロイ それではサンプルアプリケーションをデプロイしていきましょう。まずは次のコマンドでサンプルアプリケーション用の Namespace を新たに作成します。\nサンプルアプリケーション用Namespaceの作成 # 環境変数の設定 export SAMPLE_NAMESPACE=\u0026#34;sample\u0026#34;  # 両クラスタにサンプルアプリケーション用 Namespace リソースの作成 for CTX in ${CTX_1} ${CTX_2} do  kubectl create --context=${CTX} namespace ${SAMPLE_NAMESPACE}  kubectl label --context=${CTX} namespace ${SAMPLE_NAMESPACE} \\  istio.io/rev=asm-managed-stable --overwrite done  次に HelloWorld および Sleep アプリケーションをデプロイしましょう。どちらのクラスタ上の Pod に振り分けられたかをわかりやすくするため、東京 GKE クラスタに HelloWorld アプリケーションの v1、大阪 GKE クラスタに v2 をデプロイしています。\nサンプルアプリケーションのデプロイ # 両クラスタに HelloWorld サービスのデプロイ for CTX in ${CTX_1} ${CTX_2} do  kubectl apply --context=${CTX} -n ${SAMPLE_NAMESPACE} \\  -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\  -l service=\u0026#34;helloworld\u0026#34; done  # 東京 GKE クラスタに HelloWorld アプリケーションの v1 をデプロイ kubectl apply --context=${CTX_1} -n ${SAMPLE_NAMESPACE} \\  -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\  -l version=\u0026#34;v1\u0026#34;  # 大阪 GKE クラスタに HelloWorld アプリケーションの v2 をデプロイ kubectl apply --context=${CTX_2} -n ${SAMPLE_NAMESPACE} \\  -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\  -l version=\u0026#34;v2\u0026#34;  # 両クラスタに Sleep サービス、アプリケーションのデプロイ for CTX in ${CTX_1} ${CTX_2} do  kubectl apply --context=${CTX} -n ${SAMPLE_NAMESPACE} \\  -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/sleep/sleep.yaml done  サービス間通信の実行 それでは Sleep アプリケーションから HelloWorld アプリケーションへのサービス間通信をしてみましょう。次のコマンドでは各クラスタ上の Sleep アプリケーションからそれぞれ 10 回ずつ HelloWorld サービスへの通信を実施しています。\nサービス間通信の実行例 for CTX in ${CTX_1} ${CTX_2} do  echo \u0026#34;--- ${CTX}---\u0026#34;  for x in `seq 1 10`  do  kubectl exec --context=\u0026#34;${CTX}\u0026#34; -n sample -c sleep \\  \u0026#34;$(kubectl get pod --context=\u0026#34;${CTX}\u0026#34; -n sample -l \\  app=sleep -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;)\u0026#34; \\  -- curl -sS helloworld.${SAMPLE_NAMESPACE}:5000/hello \\  -w \u0026#34;time_total:%{time_total}\\n\u0026#34;  done done  次の出力例のように両クラスタから v1 と v2 の Pod へランダムで約 50% ずつトラフィックが振り分けられる状態を確認できるかと思います。また、応答時間については若干ですが東京からのアクセスは東京に配置される v1 の方が良く、大阪については v2 の方が良いことも確認できるかと思います。\nメッセージ出力例 --- gke_${PROJECT_ID}_asia-northeast1_matt-tokyo-cluster-1 --- Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.135174 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.132497 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.175281 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.127934 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.157005 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.141171 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.132390 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.141507 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.129160 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.141614 --- gke_${PROJECT_ID}_asia-northeast2_matt-osaka-cluster-1 --- Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.171108 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.157021 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.137668 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.131152 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.135779 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.141066 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.135495 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.131946 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.135268 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.131380  以上でクラスタ間ロードバランシングの動作確認は完了です。\nStep5. ローカリティロードバランシングの設定 次にもう少し高度なクラスタ間ロードバランシングの設定をしていきましょう。ユースケースはいくつかありますが、今回は次の図のようにサービス間通信は基本的にリージョン内のアプリケーションへルーティングされるようにローカリティを意識した振り分け制御を行っていきたいと思います。\nIstio リソースのデプロイ それでは設定していきましょう。まずは Istio DestinationRule リソース1の定義ファイルを作成しましょう。例のように DestinationRule リソースにて Outlier Detection(外れ値検知)の設定をすることでローカリティロードバランシングを有効化できます。\nhelloworld-destinationrule.yaml apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata:  name: helloworld spec:  host: helloworld  trafficPolicy:  outlierDetection:  consecutive5xxErrors: 5  interval: 10s  baseEjectionTime: 30s  次のコマンドで両クラスタに Istio リソースをデプロイしましょう。これで設定は終わりです。\nIstioリソースのデプロイ # 両クラスタに DestinationRule リソースをデプロイ for CTX in ${CTX_1} ${CTX_2} do  kubectl apply --context=${CTX} -n ${SAMPLE_NAMESPACE} \\  -f helloworld-destinationrule.yaml done  for CTX in ${CTX_1} ${CTX_2} do  kubectl delete --context=${CTX} -n ${SAMPLE_NAMESPACE} \\  -f helloworld-destinationrule.yaml done  ローカリティロードバランシングの動作確認 構築はまだ続きますがいったんこの状態で、クラスタ間で負荷が設定どおりの比重で分散されることを実際に確認していきたいと思います。クラスタ間ロードバランシングの動作確認と同様に Sleep アプリケーションから HelloWorld アプリケーションへのサービス間通信をしてみましょう。\nサービス間通信の実行例 for CTX in ${CTX_1} ${CTX_2} do  echo \u0026#34;--- ${CTX}---\u0026#34;  for x in `seq 1 5`  do  kubectl exec --context=\u0026#34;${CTX}\u0026#34; -n sample -c sleep \\  \u0026#34;$(kubectl get pod --context=\u0026#34;${CTX}\u0026#34; -n sample -l \\  app=sleep -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;)\u0026#34; \\  -- curl -sS helloworld.${SAMPLE_NAMESPACE}:5000/hello \\  -w \u0026#34;time_total:%{time_total}\\n\u0026#34;  done done  出力例のように東京リージョンの Sleep アプリケーションからのアクセスは v1 のみ、大阪リージョンからのアクセスは v2 のみに振り分けられることが確認できるかと思います。応答時間についてはあまり大きな差は見られないものの、ローカリティを意識しない場合と比較すると若干バラツキが少なくなったように感じます。\nメッセージ出力例 --- gke_${PROJECT_ID}_asia-northeast1_matt-tokyo-cluster-1 --- Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.132195 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.134340 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.127857 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.142889 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.128125 --- gke_${PROJECT_ID}_asia-northeast2_matt-osaka-cluster-1 --- Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.136323 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.131857 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.130870 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.130531 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.131932  以上でローカリティロードバランシング設定の動作確認もできました。\nStep6. Ingress ゲートウェイの設定 さてここからは話題がガラッと変わり、メッシュの外からの通信を受け入れるための Ingress ゲートウェイの設定をしていきたいと思います。今回は次の図のように各クラスタに配置された Ingress ゲートウェイアプリケーションを束ねるようにマルチクラスタ Ingress およびマルチクラスタ Service を配置する構成を作っていきます。\nマルチクラスタ Ingress 機能の有効化 最初に次のコマンドを実行し、マルチクラスタ Ingress 機能を有効化しましょう。なお、今回はマルチクラスタ Ingress の設定を行うメインの GKE クラスタ(=構成クラスタ)として、GKE クラスタ #1 を登録しています。\nマルチクラスタIngress機能の有効化 gcloud container hub ingress enable \\  --config-membership=${CLUSTER_1}  Ingress ゲートウェイ定義ファイルの作成 今回は Anthos Service Mesh をインストールした際に --output_dir で指定したディレクトリへ Ingress ゲートウェイのサンプル定義ファイルが配置されているのでこちらをベースに作成していきたいと思います。まずはサンプル定義ファイルを複製し、マルチクラスタ Ingress 構成向けに MultiClusterService2、BackendConfig3、MultiClusterIngress4 の 3 種類のリソース定義ファイルを追加していきましょう。\nIngressゲートウェイ定義ファイルの作成準備 # サンプル定義ファイルを複製 cp -r ${CLUSTER_1}/samples/gateways/istio-ingressgateway .  # マルチクラスタ Ingress 定義ファイルを格納するディレクトリを作成 mkdir -p istio-ingressgateway/multicluster  # Service リソース定義から MultiClusterService リソース定義ファイルに書き換え mv istio-ingressgateway/service.yaml istio-ingressgateway/multicluster/multiclusterservice.yaml  # 新たな定義ファイルを 2 種類作成 touch istio-ingressgateway/multicluster/backendconfig.yaml touch istio-ingressgateway/multicluster/multiclusteringress.yaml  それでは MultiClusterService2、BackendConfig3、MultiClusterIngress4 の 3 種類のリソース定義ファイルを編集していきましょう。MultiClusterService は Service リソースをマルチクラスタに対応させたリソースという位置づけのため、基本的に Service リソースの設定値とほぼ変わりません。今回は Ingress をフロントに配置するので LoadBalancer タイプの定義を削除し、デフォルトの Cluster IP に変更しています。\nistio-ingressgateway/multicluster/multiclusterservice.yaml（差分） - apiVersion: v1 + apiVersion: networking.gke.io/v1 - kind: Service + kind: MultiClusterService  metadata:  name: istio-ingressgateway + annotations: + cloud.google.com/backend-config: \u0026#39;{\u0026#34;default\u0026#34;: \u0026#34;ingress-backendconfig\u0026#34;}\u0026#39;  labels:  app: istio-ingressgateway  istio: ingressgateway  spec: - ports: - # status-port exposes a /healthz/ready endpoint that can be used with GKE Ingress health checks - - name: status-port - port: 15021 - protocol: TCP - targetPort: 15021 - # Any ports exposed in Gateway resources should be exposed here. - - name: http2 - port: 80 - - name: https - port: 443 - selector: - istio: ingressgateway - app: istio-ingressgateway - type: LoadBalancer + template: + spec: + ports: + # status-port exposes a /healthz/ready endpoint that can be used with GKE Ingress health checks + - name: status-port + port: 15021 + protocol: TCP + targetPort: 15021 + # Any ports exposed in Gateway resources should be exposed here. + - name: http2 + port: 80 + - name: https + port: 443 + selector: + istio: ingressgateway + app: istio-ingressgateway   BackendConfig リソースではバックエンドサービスである Ingress ゲートウェイアプリケーションのヘルスチェックに関する定義を記載します。Ingress ゲートウェイはヘルスチェック用パスとして /healthz/ready:15021 を用意しているため、こちらを設定しましょう。\nistio-ingressgateway/multicluster/backendconfig.yaml（差分） + apiVersion: cloud.google.com/v1 + kind: BackendConfig + metadata: + name: ingress-backendconfig + spec: + healthCheck: + requestPath: /healthz/ready + port: 15021 + type: HTTP   MultiClusterIngress は Ingress リソースをマルチクラスタに対応させたリソースという位置づけであり、基本的に Ingress リソースを定義するときと設定値はほぼ同じです。\nistio-ingressgateway/multicluster/multiclusteringress.yaml（差分） + apiVersion: networking.gke.io/v1beta1 + kind: MultiClusterIngress + metadata: + name: istio-ingressgateway + labels: + app: istio-ingressgateway + istio: ingressgateway + spec: + template: + spec: + backend: + serviceName: istio-ingressgateway + servicePort: 80   Ingress ゲートウェイのデプロイ まずは Ingress ゲートウェイリソースをデプロイする Namespace を新たに作成します。今回の例では istio-gateway という名前の Namespace を作成しています。\nIngress Gateway用のNamespace作成 # 環境変数の設定 export GATEWAY_NAMESPACE=\u0026#34;istio-gateway\u0026#34;  # 両クラスタにサンプルアプリケーション用 Namespace リソースの作成 for CTX in ${CTX_1} ${CTX_2} do  kubectl create --context=${CTX} namespace ${GATEWAY_NAMESPACE}  kubectl label --context=${CTX} namespace ${GATEWAY_NAMESPACE} \\  istio.io/rev=asm-managed-stable --overwrite done  次のコマンドを実行して Ingress ゲートウェイアプリケーションを両クラスタにデプロイしましょう。\nIngress Gatewayアプリケーションのデプロイ for CTX in ${CTX_1} ${CTX_2} do  kubectl apply -n ${GATEWAY_NAMESPACE} --context=${CTX} \\  -f istio-ingressgateway done  最後にマルチクラスタ Ingress リソースを構成クラスタである GKE クラスタ #1 に対してデプロイをしましょう。\nIngress Gatewayアプリケーションのデプロイ kubectl apply -n ${GATEWAY_NAMESPACE} --context=${CTX_1} \\  -f istio-ingressgateway/multicluster  以上で Ingress ゲートウェイのデプロイは終わりです。\nIstio リソースのデプロイ Ingress ゲートウェイを通じてメッシュの外から HelloWorld アプリケーションへ通信ができるように Istio リソースの定義を行っていきたいと思います。まずは Istio Gateway リソース5および Istio VirtualService リソース1の定義ファイルを作成しましょう。例のように Gateway リソースにメッシュ外から受け付けるポートとプロトコルを定義し、VirtualService リソースには Gateway に入ってきた通信のパターンマッチ条件と振り分け先バックエンドの指定をします。\nhelloworld-gateway.yaml apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata:  name: helloworld-gateway spec:  selector:  istio: ingressgateway  servers:  - port:  number: 80  name: http  protocol: HTTP  hosts:  - \u0026#34;*\u0026#34; --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata:  name: helloworld-gateway spec:  hosts:  - \u0026#34;*\u0026#34;  gateways:  - helloworld-gateway  http:  - match:  - uri:  exact: /hello  route:  - destination:  host: helloworld  port:  number: 5000  次のコマンドで両クラスタに Istio リソースをデプロイし、アプリケーションへのインバウンド通信ができるように設定しましょう。\nIstioリソースのデプロイ for CTX in ${CTX_1} ${CTX_2} do  kubectl apply -n ${SAMPLE_NAMESPACE} --context=${CTX} \\  -f helloworld-gateway.yaml done  以上でメッシュ外からのアプリケーションへのインバウンド通信もできるようになりました。\nインバウンド通信の動作確認 それではメッシュ外からのアプリケーションへのインバウンド通信ができることを確認していきましょう。ローカリティロードバランシングの設定は「Step5. ローカリティロードバランシングの設定」にて実施済みですので次のような挙動となることが想定されます。\nパブリックエンドポイントの取得 まずは Ingress ゲートウェイの外部 IP アドレスを取得しましょう。\nIngressゲートウェイの外部IPアドレスの取得 kubectl --context=${CTX_1} \\  -n ${GATEWAY_NAMESPACE} get MultiClusterIngress \\  -o custom-columns=VIP:status.VIP --no-headers  インバウンド通信の実行 では東京リージョン上に仮想マシンなどを立てて Ingress ゲートウェイの外部 IP アドレスに対して curl コマンドを実行し、アクセスをしてみましょう。実行例のように v1 に振り分けが 100% されることを確認できるかと思います。\n東京からの実行例（ローカリティロードバランシング有効） $ for x in `seq 1 10`; do curl http://\u0026lt;Ingress Gateway\u0026#39;s External IP\u0026gt;/hello -w \u0026#34;time_total:%{time_total}\\n\u0026#34;; done Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.139677 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.139758 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.138518 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.142124 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.137289 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.151358 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.133507 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.139144 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.133560 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.135621  次に大阪リージョン上に仮想マシンなどを立てて Ingress ゲートウェイの外部 IP アドレスに対して curl コマンドを実行し、アクセスをしてみましょう。先ほどとは異なり、実行例のように v2 に振り分けが 100% されることを確認できるかと思います。\n大阪からの実行例（ローカリティロードバランシング有効） $ for x in `seq 1 10`; do curl http://\u0026lt;Ingress Gateway\u0026#39;s External IP\u0026gt;/hello -w \u0026#34;time_total:%{time_total}\\n\u0026#34;; done Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.139 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.133 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.134 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.130 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.129 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.141 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.133 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.176 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.137 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.138  ご参考、ローカリティロードバランシング無効時の動作 ローカリティロードバランシングを無効(=DestinationRule リソースを削除)にした状態で、各リージョンからアクセスした結果も取得してみました。メッシュ内の通信では差が見えにくかったのですが、インバウンド通信にするとローカリティによるレスポンスの改善がはっきりとわかる結果になりました。\n東京からの実行例（ローカリティロードバランシング無効） $ for x in `seq 1 10`; do curl http://\u0026lt;Ingress Gateway\u0026#39;s External IP\u0026gt;/hello -w \u0026#34;time_total:%{time_total}\\n\u0026#34;; done Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.221164 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.142753 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.224596 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.160720 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.188251 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.145336 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.173262 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.138860 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.139937 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.170250  大阪からの実行例（ローカリティロードバランシング無効） $ for x in `seq 1 10`; do curl http://\u0026lt;Ingress Gateway\u0026#39;s External IP\u0026gt;/hello -w \u0026#34;time_total:%{time_total}\\n\u0026#34;; done Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 response_time:0.147 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t response_time:0.225 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t response_time:0.211 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 response_time:0.141 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 response_time:0.134 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t response_time:0.182 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 response_time:0.133 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 response_time:0.134 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 response_time:0.143 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t response_time:0.187  以上でメッシュの外からアプリケーションへのインバウンド通信に対する動作確認も終了です。お疲れ様でした。\n終わりに 今回は複数リージョンに展開した複数 GKE クラスタを単一の Anthos Service Mesh 環境に追加し、GKE クラスタ間で負荷分散を行う方法についてご紹介でしたがいかがだったでしょうか。\n複数リージョンの GKE クラスタでマルチクラスタメッシュを構築することにより、予期しない大規模災害の発生にも耐えうる高い可用性と回復力の実現、エンドユーザからより近い位置への振り分けによるレイテンシの改善といったことが期待できるようになる見込みです。もしこれから Anthos Service Mesh 環境の利用を検討している方はマルチクラスタメッシュ構成についても検討してみてはいかがでしょうか。\n  Google Cloud は、Google LLC の商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。    DestinationRule は転送先サービスのサブセット化や各種トラフィックポリシーを定義する Istio リソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n MultiClusterService は Service リソースを複数のクラスタ上に展開する GKE 独自のカスタムリソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n BackendConfig はバックエンドサービスのヘルスチェックを定義する GKE 独自のカスタムリソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n MultiClusterIngress はマルチクラスタに対応した Ingress リソースを定義する GKE 独自のカスタムリソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Gateway は Ingress/Egress ゲートウェイで受け付けるポート、プロトコルを定義する Istio リソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://chacco38.github.io/posts/2021/12/gcp-multi-region-asm-cluster/","summary":"はじめに みなさん、こんにちは。今回は複数のリージョンに展開する各 GKE クラスタを単一の Anthos Service Mesh 環境に追加し、GKE クラスタ間で負荷分散を行う方法についてご紹介していきたいと思います。\n複数リージョンの GKE クラスタでマルチクラスタメッシュを構築することにより、予期しない大規模災害の発生にも耐えうる高い可用性と回復力の実現、エンドユーザからより近い位置への振り分けによるレイテンシの改善といったことが期待できるようになる見込みです。\nもちろん公式ドキュメントにもマルチクラスタメッシュの構築に関する記載はあるのですが、単にクラスタ間で分散されたことを確認しただけで終わっており、ローカリティを意識したルーティングの設定やメッシュの外からの通信に関する記載はなかったため、今回はここら辺も含めて一気通貫でご紹介したいと思います。もしこれからリージョンをまたがった Anthos Service Mesh 環境の利用を検討している方は参考にしてみてはいかがでしょうか。\n構築するシステムについて 次の図に示すように限定公開クラスタおよび承認済みネットワーク機能を有効化した複数リージョンの GKE クラスタに対して Anthos Service Mesh (マネージドコントロールプレーン)を導入しています。サービスメッシュ上ではサンプルアプリケーションを動かし、ローカリティを意識した負荷分散についても設定をしていきたいと思います。なお、今回の例では GKE、Anthos Service Mesh のいずれのリリースチャンネルについても安定性重視の Stable チャンネルを採用しています。\nそれでは構築していきましょう いつも通り公式ドキュメントを参考にしつつ、公式ドキュメントに書かれていない部分を補足しながら構築をしていきたいと思います。\nhttps://cloud.google.com/service-mesh/docs/unified-install/gke-install-multi-cluster\nStep1. VPC ネットワークの作成 まずは GKE ノードを配置する VPC ネットワークおよび東京リージョンと大阪リージョンにサブネットを作成します。今回の例では GKE ノードからプライベートネットワーク経由で Artifact Registry などの他のマネージドサービスへアクセスできるように限定公開の Google アクセスをオンにしています。\nVPCネットワークの作成 # 環境変数の設定 export NETWORK=\u0026#34;matt-vpc\u0026#34; export SUBNET=\u0026#34;matt-private-vm\u0026#34; export LOCATION_1=\u0026#34;asia-northeast1\u0026#34; export LOCATION_2=\u0026#34;asia-northeast2\u0026#34; export IP_RANGE_1=\u0026#34;172.16.0.0/16\u0026#34; export IP_RANGE_2=\u0026#34;172.24.0.0/16\u0026#34;  # VPC ネットワークの作成 gcloud compute networks create ${NETWORK} --subnet-mode=custom  # サブネットの作成 (東京リージョン) gcloud compute networks subnets create ${SUBNET} \\  --network=${NETWORK} --range=${IP_RANGE_1} --region=${LOCATION_1} \\  --enable-private-ip-google-access  # サブネットの作成 (大阪リージョン) gcloud compute networks subnets create ${SUBNET} \\  --network=${NETWORK} --range=${IP_RANGE_2} --region=${LOCATION_2} \\  --enable-private-ip-google-access  プライベートネットワーク経由でインターネット上の Docker Hub などへ接続できるよう Cloud NAT も作成しておきます。","title":"複数リージョンの GKE クラスタと Anthos Service Mesh でマルチクラスタメッシュ環境を構築してみた"},{"content":"はじめに みなさん、こんにちは。今回は GitHub Enterprise Cloud(GHEC) の監査ログ(Audit Log) の取得方法についてのお話です。\nGHEC 監査ログの取得方法としてはいくつか方法はあるのですが、この記事では GHEC の監査ログを取得するためのコマンドラインインタフェースである GHEC Audit Log CLI を使った方法をご紹介していきたいと思います。\nhttps://github.com/github/ghec-audit-log-cli\nGHEC Audit Log CLI を使ってみよう 今回は Linux(AWS CloudShell) 上に環境を作って試しに実行してみるところからはじめて、定期的に監査ログ取得を行う自動化フローの構築まで紹介していきたいと思います。\nローカル環境で実行してみよう Step1. 前提パッケージのインストール GHEC Audit Log CLI の前提パッケージとして Node.js が必要となります。GitHub からソースコードを入手する必要があるため、git コマンドと併せてインストールしましょう。\n前提パッケージの導入 $ curl --silent --location https://rpm.nodesource.com/setup_16.x | sudo bash - $ sudo yum install -y nodejs git  Step2. GHEC Audit Log CLI のインストール GitHub からソースコードを取得し、npm コマンドを使って GHEC Audit Log CLI をインストールします。最後の ghce-audit-log-cli -v コマンドにてバージョン情報が出力されれば CLI のインストールは完了です。\nソースコード取得とインストール $ git clone https://github.com/github/ghec-audit-log-cli.git $ cd ghec-audit-log-cli $ sudo npm link $ ghec-audit-log-cli -v 2.1.2 $ ghec-audit-log-cli --help Usage: ghec-audit-log-cli [options]  Options:  -v, --version Output the current version  -t, --token \u0026lt;string\u0026gt; the token to access the API (mandatory)  -o, --org \u0026lt;string\u0026gt; the organization we want to extract the audit log from  -cfg, --config \u0026lt;string\u0026gt; location for the config yaml file. Default \u0026#34;.ghec-audit-log\u0026#34;  (default: \u0026#34;./.ghec-audit-log\u0026#34;)  -p, --pretty prints the json data in a readable format (default: false)  -l, --limit \u0026lt;number\u0026gt; a maximum limit on the number of items retrieved  -f, --file \u0026lt;string\u0026gt; the output file where the result should be printed  -a, --api \u0026lt;string\u0026gt; the version of GitHub API to call (default: \u0026#34;v4\u0026#34;)  -at, --api-type \u0026lt;string\u0026gt; Only if -a is v3. API type to bring, either all, web or git  (default: \u0026#34;all\u0026#34;)  -c, --cursor \u0026lt;string\u0026gt; if provided, this cursor will be used to query the newest  entries from the cursor provided. If not present, the result  will contain all the audit log from the org  -s, --source \u0026lt;string\u0026gt; the source of the audit log. The source can be either a  GitHub Enterprise or a GitHub Enterprise Organization.  Accepts the following values: org | enterprise. Defaults to  org (default: \u0026#34;org\u0026#34;)  -h, --help display help for command  Step3. GitHub アクセストークンの取得 次に GitHub のサイトへ移り、GitHub API へアクセスする際に利用するアクセストークンを作成します。Organization の Owner 権限を持つユーザで、Settings \u0026gt; Developer settings \u0026gt; Personal access tokens \u0026gt; Generate new tokens から作成しましょう。付与するスコープについては筆者の環境では次の 4 つの権限を付与することで動作を確認することができました。\n public_repo admin:org read:user admin:enterprise  Step4. GHEC Audit Log CLI の動作確認 ここまで来たら実際にコマンドを実行して監査ログが取得できるか試してみましょう。次のコマンドを実行して監査ログが出力されれば成功です。\n監査ログの取得 $ export ORG_NAME=\u0026#34;Your GitHub organization account name\u0026#34; $ export AUDIT_LOG_TOKEN=\u0026#34;Your GitHub access token\u0026#34;  $ ghec-audit-log-cli --org ${ORG_NAME} --token ${AUDIT_LOG_TOKEN} --api \u0026#34;v3\u0026#34; --pretty  なお、GitHub アクセストークンに付与した権限が足りない場合は次のようなメッセージを出力してコマンドがエラー終了します。エラーメッセージに不足している権限についての情報がありますので Step3 へ戻り、アクセストークンへ不足する権限を追加して再度 CLI を実行してください。\nコマンド失敗時の出力エラーメッセージ例 GraphqlError: Your token has not been granted the required scopes to execute this query. The \u0026#39;organizationBillingEmail\u0026#39; field requires one of the following scopes: [\u0026#39;admin:org\u0026#39;], but your token has only been granted the: [\u0026#39;admin:enterprise\u0026#39;, \u0026#39;read:org\u0026#39;] scopes. Please modify your token\u0026#39;s scopes at: https://github.com/settings/tokens.  以上で無事にローカル環境で GHEC Audit Log CLI を実行することができました。\n監査ログを取得するフローを自動化しよう 次は定期的に監査ログ取得を行う自動化フローの構築を行っていきたいと思います。今回はサンプルとして用意されているワークフロー定義を少し改造し、「1 時間ごと GHEC Audit Log CLI を使って監査ログを取得し、監査ログ格納用のリポジトリに追加する」といったフローを GitHub Actions を使って自動化していきたいと思います。\nStep1. 監査ログ格納用リポジトリの作成 まずは対象 Organization に監査ログを取得、格納するためのリポジトリを作成しましょう。今回は ghec-audit-log-cli という名前のリポジトリを作成しています。\nStep2. シークレットの登録 次にアクセストークンなどの機密性の高い情報をワークフローに渡すために Organization 設定にてシークレットの登録を行います。シークレットは次の 3 種類を登録しましょう。\n   シークレット名 説明     ORG_NAME 監査ログを取得する対象の Organization アカウント名を指定   AUDIT_LOG_TOKEN GitHub Audit Log API へアクセスする際に利用するアクセストークンを指定   COMMITTER_EMAIL 監査ログなどをリポジトリに Push するアカウントのメールアドレスを指定    Step3. GHEC Audit Log CLI のソースコード登録 GHEC Audit Log CLI のソースコードを取得し、新しく作成したリポジトリへソースコードを登録をしましょう。\nソースコードの登録 $ export ORG_NAME=\u0026#34;Your GitHub organization account name\u0026#34;  $ git clone https://github.com/github/ghec-audit-log-cli.git $ cd ghec-audit-log-cli  $ git remote add logging https://github.com/${ORG_NAME}/ghec-audit-log-cli.git $ git push -u logging main  Step4. ワークフロー定義の作成 次にサンプルのワークフロー定義が workflows ディレクトリに用意されていますので、こちらをベースにワークフロー定義を作成していきたいと思います。なお、v3 と v4 の 2 種類のサンプルが用意されていますが記事の執筆時点では ​v4 に不具合があったため v3 を利用しています。\nまずはサンプル定義ファイルを GitHub Actions 所定のディレクトリ .github/workflows へ複製します。\nサンプルワークフローを複製 $ cp workflows/forward-v3-workflow.yml .github/workflows/ghec-audit-log.yml  次に複製した .github/workflows/ghec-audit-log.yml ファイルを編集します。サンプルでは取得した監査ログを指定した URL へ POST するように定義されていますが、今回はこちらのリポジトリへコミットするように書き換えています。\n.github/workflows/ghec-audit-log.yml（差分）  ############################################  # Github Action Workflow to poll and aggregate logs #  ############################################  name: POLL/POST Audit Log Data from v3 API   ##############################################  # Run once an hour and when pushed to main #  ##############################################  on:  push:  branches: main  schedule:  - cron: \u0026#39;59 * * * *\u0026#39;   #################  # Build the job #  #################  jobs:  poll:  runs-on: ubuntu-latest   strategy:  matrix:  node-version: [12.x]   steps:  # Clone source code  - name: Checkout source code  uses: actions/checkout@v2   # Install congiure NodeJS  - name: Use Node.js ${{ matrix.node-version }}  uses: actions/setup-node@v1  with:  node-version: ${{ matrix.node-version }}   # Need to install NPM  - name: NPM Install  run: npm install   # If this is the first time we poll, then do a fresh poll. If not, poll from latest cursor.  - name: Poll from Cursor  run: | + FILE_SUFFIIX=$(date +%Y%m%d-%H%M) + mkdir -p audit-logs  if [ -f \u0026#34;.last-v3-cursor-update\u0026#34; ]; then  LAST_CURSOR=$(cat .last-v3-cursor-update)  fi   if [ -z \u0026#34;$LAST_CURSOR\u0026#34; ]; then  echo \u0026#34;FIRST TIME RUNNING AUDIT LOG POLL\u0026#34; - npm start -- --token ${{secrets.AUDIT_LOG_TOKEN}} --org ${{secrets.ORG_NAME}} --api \u0026#39;v3\u0026#39; --api-type \u0026#39;all\u0026#39; --file \u0026#34;audit-log-output.json\u0026#34; + npm start -- --token ${{secrets.AUDIT_LOG_TOKEN}} \\ + --org ${{secrets.ORG_NAME}} --api \u0026#39;v3\u0026#39; --api-type \u0026#39;all\u0026#39; \\ + --file \u0026#34;audit-logs/audit-log-output-${FILE_SUFFIIX}.json\u0026#34; --pretty  else  echo \u0026#34;RUNNING AUDIT LOG POLL FROM $LAST_CURSOR\u0026#34; - npm start -- --token ${{secrets.AUDIT_LOG_TOKEN}} --org ${{secrets.ORG_NAME}} --api \u0026#39;v3\u0026#39; --api-type \u0026#39;all\u0026#39; --cursor $LAST_CURSOR --file \u0026#34;audit-log-output.json\u0026#34; + npm start -- --token ${{secrets.AUDIT_LOG_TOKEN}} \\ + --org ${{secrets.ORG_NAME}} --api \u0026#39;v3\u0026#39; --api-type \u0026#39;all\u0026#39; \\ + --cursor $LAST_CURSOR \\ + --file \u0026#34;audit-logs/audit-log-output-${FILE_SUFFIIX}.json\u0026#34; --pretty  fi - curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d @audit-log-output.json ${{secrets.WEBHOOK_URL}}   # Commit the cursor back to source  - name: Commit cursor  uses: EndBug/add-and-commit@v5  with:  author_name: Audit Log Integration  author_email: ${{ secrets.COMMITTER_EMAIL }}  message: \u0026#34;Updating cursor for audit log\u0026#34;  add: \u0026#34;.last-v3-cursor-update --force\u0026#34;  env:  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} + - name: Commit audit log + uses: EndBug/add-and-commit@v5 + with: + author_name: Audit Log Integration + author_email: ${{ secrets.COMMITTER_EMAIL }} + message: \u0026#34;Adding audit log\u0026#34; + add: \u0026#34;audit-logs/audit-log-output-*.json --force\u0026#34; + env: + GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}   最後に定義ファイルをコミットして、リモートリポジトリへ登録しましょう。\nリポジトリにワークフローの登録 $ git add .github/workflows/ghec-audit-log.yml $ git commit -m \u0026#34;add ghec-audit-log workflow\u0026#34; $ git push -u logging main  このリポジトリへの Push をトリガーに GitHub Actions が起動しますので、この後はワークフローが期待通り動作しているかを確認していきましょう。\nStep5. ワークフローの動作確認 まずは GitHub Actions 画面へ遷移し、ワークフローの起動および処理が成功していることを確認します。出力例のように GitHub Actions のワークフロー実行履歴にて成功が記録されていれば OK です。\n次に監査ログが正しく格納されているか確認しましょう。出力例のように監査ログの格納が確認できれば OK です。\n以上でワークフローの設定も完了です。お疲れ様でした。以降はワークフローに定義したスケジュールに沿って 1 時間ごとに監査ログがエクスポートされるようになるかと思います。\n終わりに 今回は GitHub Enterprise Cloud (GHEC) の監査ログを取得する方法でした。GHEC の監査ログを長期(90 日以上)保存したい方はエクスポートが必須となってきますので参考にしてみてはいかがでしょうか。\n  GitHub は、GitHub Inc. の商標または登録商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。  ","permalink":"https://chacco38.github.io/posts/2021/12/ghec-audit-log-cli/","summary":"はじめに みなさん、こんにちは。今回は GitHub Enterprise Cloud(GHEC) の監査ログ(Audit Log) の取得方法についてのお話です。\nGHEC 監査ログの取得方法としてはいくつか方法はあるのですが、この記事では GHEC の監査ログを取得するためのコマンドラインインタフェースである GHEC Audit Log CLI を使った方法をご紹介していきたいと思います。\nhttps://github.com/github/ghec-audit-log-cli\nGHEC Audit Log CLI を使ってみよう 今回は Linux(AWS CloudShell) 上に環境を作って試しに実行してみるところからはじめて、定期的に監査ログ取得を行う自動化フローの構築まで紹介していきたいと思います。\nローカル環境で実行してみよう Step1. 前提パッケージのインストール GHEC Audit Log CLI の前提パッケージとして Node.js が必要となります。GitHub からソースコードを入手する必要があるため、git コマンドと併せてインストールしましょう。\n前提パッケージの導入 $ curl --silent --location https://rpm.nodesource.com/setup_16.x | sudo bash - $ sudo yum install -y nodejs git  Step2. GHEC Audit Log CLI のインストール GitHub からソースコードを取得し、npm コマンドを使って GHEC Audit Log CLI をインストールします。最後の ghce-audit-log-cli -v コマンドにてバージョン情報が出力されれば CLI のインストールは完了です。","title":"GHEC Audit Log CLI を使って GitHub Enterprise Cloud の監査ログを取得してみた"},{"content":"はじめに みなさん、こんにちは。今回は Google Cloud が提供するマネージドサービスメッシュサービスの Anthos Service Mesh に関するお話です。\nAnthos Service Mesh はここ半年で「マネージドコントロールプレーン機能の一般公開」、「マネージドデータプレーン機能のプレビュー公開」と Google マネージドの範囲を徐々に広げてきましたが、2021 年 11 月 19 日の更新でプレビュー段階ではありますが「Google Kubernetes Engine(GKE) Autopilot 上でも Anthos Service Mesh を利用できる」ようになりました。\n今回はそんなプレビュー公開されたばかりの GKE Autopilot と Anthos Service Mesh(ASM) を使った Kubernetes 部分も含めてフルマネージドなサービスメッシュ環境を構築していきたいと思います。\n構築するシステムについて 次の図に示すように限定公開クラスタおよび承認済みネットワーク機能を有効化した GKE Autopilot クラスタに対して Anthos Service Mesh を導入し、サービスメッシュ上でサンプルアプリケーションを動かしていきたいと思います。\nそれでは構築していきましょう いつも通り公式ドキュメントを参考にしつつ、公式ドキュメントに書かれていない部分を補足しながら構築をしていきたいと思います。\nhttps://cloud.google.com/service-mesh/docs/unified-install/managed-asmcli-experimental\nStep1. VPC ネットワークの作成 まずは GKE ノードを配置する VPC ネットワークおよび東京リージョンにサブネットを作成します。今回の例では GKE ノードからプライベートネットワーク経由で Artifact Registry などの他のマネージドサービスへアクセスできるように限定公開の Google アクセスをオンにしています。\nプライベートネットワークからインターネット上の Docker Hub などへ接続できるよう Cloud NAT リソースも作成しておきたいと思います。\nStep2. GKE Autopilot クラスタの作成 次に GKE Autopilot クラスタを作成していきます。Google Cloud コンソールから操作する場合は GKE クラスタ画面の「+作成」ボタンをクリックし、GKE Autopilot クラスタの作成画面へと遷移します。\nAutopilot クラスタの作成画面では、クラスタを展開するネットワークの選択とリリースチャンネルの選択をしていきます。今回はセキュリティの観点から限定公開クラスタに設定し、GKE Autopilot リリースチャンネルには Anthos Service Mesh が現時点で唯一サポート対象としている Rapid チャンネルを指定しています。作成ボタンを押してからクラスタの作成が完了するまで 5 分程度かかりました。\nなお、Anthos Service Mesh がサポートする GKE Autopilot リリースチャンネルについては近い将来変更が生じる可能性が高いため、構築を行う際には最新のサポート状況を公式ドキュメントから確認するようにしましょう。\nhttps://cloud.google.com/service-mesh/docs/supported-features-mcp\nStep3. Anthos Service Mesh のインストール ここからは Cloud Shell から操作を実施していきたいと思います。まずは Kubernetes API へ接続できるように GKE コントロールプレーンの承認済みネットワークに Cloud Shell の IP アドレスを登録し、kubectl を実行できるようにクラスタ認証情報を取得します。\nクラスタ認証情報の取得 export PROJECT_ID=`gcloud config list --format \u0026#34;value(core.project)\u0026#34;` export CLUSTER_NAME=\u0026#34;matt-tokyo-autopilot-cluster-001\u0026#34; export CLUSTER_LOCATION=\u0026#34;asia-northeast1\u0026#34;  # CloudShellの承認済みネットワーク登録 gcloud container clusters update ${CLUSTER_NAME} \\  --region ${CLUSTER_LOCATION} \\  --enable-master-authorized-networks \\  --master-authorized-networks \\  \u0026#34;`dig +short myip.opendns.com @resolver1.opendns.com`/32\u0026#34;  # クラスタ認証情報の取得 gcloud container clusters get-credentials ${CLUSTER_NAME} \\  --region ${CLUSTER_LOCATION}  次に Anthos Service Mesh v1.11 から正式な管理ツールとなった asmcli をダウンロードします。\nasmcliツールのダウンロード curl https://storage.googleapis.com/csm-artifacts/asm/asmcli_1.11 \u0026gt; asmcli  # 実行権限の付与 chmod +x asmcli  asmcli を使って GKE Autopilot クラスタに Rapid チャンネル(※1)の Anthos Service Mesh をインストールします。コマンドが完了するまでおおよそ 5 分程度かかりました。\n※1: 現時点では GKE Autopilot は Anthos Service Mesh の Rapid チャンネルでのみサポートされているため\nAnthosServiceMeshのインストール ./asmcli x install \\  --project_id ${PROJECT_ID} \\  --cluster_location ${CLUSTER_LOCATION} \\  --cluster_name ${CLUSTER_NAME} \\  --managed \\  --use_managed_cni \\  --channel \u0026#34;rapid\u0026#34; \\  --enable-all \\  --output_dir ${CLUSTER_NAME}  インストールに成功した場合は次のようなメッセージが出力されます。\nインストール成功時のメッセージ出力 asmcli: Successfully installed ASM.  Step4. ファイアウォールルールの更新 (限定公開クラスタ時のみ) 限定公開クラスタに Anthos Service Mesh をインストールした場合は、コントロールプレーンからのポート 15017 による通信を追加で許可する必要があります。まず次のコマンドを実行し、既存のファイアウォールルール名を取得します。\nファイアウォールルールの確認 gcloud compute firewall-rules list --filter=\u0026#34;name~${CLUSTER_NAME}-.*-master\u0026#34;  次のコマンドのファイアウォールルール名 ${FIREWALL_RULE_NAME} を前のコマンドで取得した名前に置き替え、コマンド実行してコントロールプレーンからのポート 15017 による通信を許可します。\nファイアウォールルールの更新 gcloud compute firewall-rules update ${FIREWALL_RULE_NAME} \\  --allow tcp:10250,tcp:443,tcp:15017  Step5. マネージドデータプレーンの有効化 (Namespace 毎に設定) マネージドデータプレーンは Kubernetes Namespace リソースごとにアノテーションを設定して有効化を行います。次のコマンドの ${NAMESPACE_NAME} を設定対象の Namespace 名に置き替え、コマンド実行してマネージドデータプレーンを有効化します。\nマネージドデータプレーンの有効化 kubectl annotate --overwrite namespace ${NAMESPACE_NAME} \\  mesh.cloud.google.com/proxy=\u0026#39;{\u0026#34;managed\u0026#34;:\u0026#34;true\u0026#34;}\u0026#39;  Step6. Ingress Gateway のデプロイ asmcli でインストールした場合は自動で Ingress Gateway はデプロイされないため、メッシュ外からのアクセスをさせるためには Ingress Gateway をデプロイする必要があります。まず次のコマンドで istio-gateway Namespace を新たに作成します。\nIngressGateway用のNamespace作成 export GATEWAY_NAMESPACE=\u0026#34;istio-gateway\u0026#34;  kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Namespace metadata: name: ${GATEWAY_NAMESPACE} annotations: # マネージドデータプレーン有効化の設定 mesh.cloud.google.com/proxy: \u0026#39;{\u0026#34;managed\u0026#34;:\u0026#34;true\u0026#34;}\u0026#39; labels: # 自動サイドカーインジェクション(Rapid)有効化の設定 istio.io/rev: asm-managed-rapid EOF  次のコマンドを実行して Ingress Gateway をデプロイします。なお、今回は Anthos Service Mesh をインストールした際に --output_dir で指定したディレクトリに Ingress Gateway の定義ファイルが配置されているのでこちらをそのまま利用しています。\nIngressGatewayのデプロイ kubectl apply -n ${GATEWAY_NAMESPACE} \\  -f ${CLUSTER_NAME}/samples/gateways/istio-ingressgateway  なお、今回利用した Ingress Gateway の詳細については次の URL をご参照ください。\nhttps://github.com/GoogleCloudPlatform/anthos-service-mesh-packages/tree/main/samples/gateways/istio-ingressgateway\n以上でアプリケーションをデプロイする準備が整いました。\nStep7. サンプルアプリケーションのデプロイ 最後にサンプルアプリケーションをデプロイし、環境に問題がないかを確認していきましょう。まず次のコマンドでサンプルアプリケーション用の Namespace を新たに作成します。\nNamespaceの作成 export SAMPLE_NAMESPACE=\u0026#34;sample\u0026#34;  # サンプルアプリケーション用 Namespace リソースの作成 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Namespace metadata: annotations: # マネージドデータプレーン有効化の設定 mesh.cloud.google.com/proxy: \u0026#39;{\u0026#34;managed\u0026#34;:\u0026#34;true\u0026#34;}\u0026#39; labels: # 自動サイドカーインジェクション(Rapid)有効化の設定 istio.io/rev: asm-managed-rapid EOF  サンプルアプリケーションをデプロイします。今回は Anthos Service Mesh をインストールした際に --output_dir で指定したディレクトリに格納されているサンプルアプリケーションの中から HelloWorld というサンプルアプリケーションを使用しています。\nHelloWorldアプリケーションのデプロイ # Kubernetes Service リソースのデプロイ kubectl apply -n ${SAMPLE_NAMESPACE} \\  -f ${CLUSTER_NAME}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\  -l service=helloworld  # Kubernetes Deployment リソースのデプロイ kubectl apply -n ${SAMPLE_NAMESPACE} \\  -f ${CLUSTER_NAME}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\  -l version=v1  # Istio Gateway/VirtualService リソースのデプロイ kubectl apply -n ${SAMPLE_NAMESPACE} \\  -f ${CLUSTER_NAME}/istio-1.11.2-asm.17/samples/helloworld/helloworld-gateway.yaml  アプリケーションのデプロイが終わったので Ingress Gateway 経由でアプリケーションにアクセスできるかを確認していきましょう。まず次のコマンドを実行して Ingress Gateway の External IP アドレスを取得します。\nIngressGatewayの設定 kubectl -n ${GATEWAY_NAMESPACE} get service istio-ingressgateway  External IP アドレスが取得できたら curl コマンドなどで http://\u0026lt;EXTERNAL_IP\u0026gt;/hello にアクセスしてみましょう。次のようなメッセージが表示されていれば成功です。\n成功時のメッセージ Hello version: v1, instance: helloworld-v1-xxxxxxxxxx-xxxxx  以上で構築は終わりです。お疲れ様でした。\n終わりに さて今回はプレビュー公開されたばかりの Google Kubernetes Engine(GKE) Autopilot と Anthos Service Mesh(ASM) を使ったフルマネージドなサービスメッシュ環境を構築する方法のご紹介でした。いかがだったでしょうか。\nこれで晴れて Kubernetes 部分も含めフルマネージドなサービスメッシュ環境が実現できるようになり、こちらの活用により従来よりも運用負荷を大きく軽減できる未来が近づいてきましたね。\nもちろん安定性、保守性といった観点からプロダクション用途へのプレビュー段階の機能の適用は、現時点ではお勧めできませんが、もしこれから検証目的で試してみたいという方は参考にしてみてはいかがでしょうか。\n  Google Cloud は、Google LLC の商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。  ","permalink":"https://chacco38.github.io/posts/2021/12/gcp-asm-with-gke-autopilot/","summary":"はじめに みなさん、こんにちは。今回は Google Cloud が提供するマネージドサービスメッシュサービスの Anthos Service Mesh に関するお話です。\nAnthos Service Mesh はここ半年で「マネージドコントロールプレーン機能の一般公開」、「マネージドデータプレーン機能のプレビュー公開」と Google マネージドの範囲を徐々に広げてきましたが、2021 年 11 月 19 日の更新でプレビュー段階ではありますが「Google Kubernetes Engine(GKE) Autopilot 上でも Anthos Service Mesh を利用できる」ようになりました。\n今回はそんなプレビュー公開されたばかりの GKE Autopilot と Anthos Service Mesh(ASM) を使った Kubernetes 部分も含めてフルマネージドなサービスメッシュ環境を構築していきたいと思います。\n構築するシステムについて 次の図に示すように限定公開クラスタおよび承認済みネットワーク機能を有効化した GKE Autopilot クラスタに対して Anthos Service Mesh を導入し、サービスメッシュ上でサンプルアプリケーションを動かしていきたいと思います。\nそれでは構築していきましょう いつも通り公式ドキュメントを参考にしつつ、公式ドキュメントに書かれていない部分を補足しながら構築をしていきたいと思います。\nhttps://cloud.google.com/service-mesh/docs/unified-install/managed-asmcli-experimental\nStep1. VPC ネットワークの作成 まずは GKE ノードを配置する VPC ネットワークおよび東京リージョンにサブネットを作成します。今回の例では GKE ノードからプライベートネットワーク経由で Artifact Registry などの他のマネージドサービスへアクセスできるように限定公開の Google アクセスをオンにしています。\nプライベートネットワークからインターネット上の Docker Hub などへ接続できるよう Cloud NAT リソースも作成しておきたいと思います。","title":"GKE Autopilot と Anthos Service Mesh を使ってフルマネージドなサービスメッシュ環境を構築してみた"},{"content":"はじめに みなさん、こんにちは。今回は GitHub Enterprise Cloud(GHEC) の各種ログを SIEM1 マネージドサービスである Microsoft Sentinel (旧称 Azure Sentinel) に集約し、サイバー攻撃の兆候を検知できるようにする方法をご紹介していきたいと思います。\nMicrosoft Sentinel と GHEC との連携方法はいくつかあるのですが、この記事では Microsoft Sentinel コミュニティで開発している Azure Logic Apps(ロジックアプリ)、Azure Functions(関数アプリ)の 2 種類のカスタムデータコネクタの内、「Azure Logic Apps コネクタ」を使った方法をご紹介していきたいと思います。\nこれから GHEC の利用を検討している方や、GHEC は利用しているけれど SIEM システムの導入まではしていないという方は、セキュリティ強化策のひとつとして参考にしてみてはいかがでしょうか。\n構築するシステムについて 今回は Azure Logic Apps(ロジックアプリ)を定期的に起動し、GHEC から監査ログなどを取得して Microsoft Sentinel ワークスペースへ格納、格納されたログに対して Microsoft Sentinel が自動的に相関分析をかけていく、といった流れで処理を行うシステムを構築していきたいと思います。\nAzure Sentinel とのコネクタについては、今回は Microsoft Sentinel コミュニティで公開されている次のカスタムデータコネクタを利用していきます。\nhttps://github.com/Azure/Azure-Sentinel/tree/master/DataConnectors/GitHub\n本カスタムコネクタをデプロイすると、次の 3 種類の Azure Logic Apps リソースが動作するようになります。\n   リソース種別 説明     Audit Playbook 監査ログを定期的に収集する自動ワークフロー (デフォルト 5 分間隔)   Repo Playbook 各リポジトリに対するフォーク、クローン、コミットなどの操作ログを定期的に収集する自動ワークフロー (デフォルト 1 時間間隔)   Vulnerability Alert Playbook 各リポジトリに対するセキュリティ脆弱性診断ログを定期的に収集する自動ワークフロー (デフォルト 1 日間隔)    また、本カスタムコネクタで取得した各種ログデータについては、Log Analytics ワークスペースの次のカスタムテーブルへ格納されるようになります。\n   テーブル名 説明     GitHub_CL 監査ログのデータを格納するテーブル   GitHubRepoLogs_CL 各リポジトリに対するフォーク、クローン、コミットなどの操作ログやリポジトリに対するセキュリティ脆弱性診断ログのデータを格納するテーブル    それでは構築していきましょう 今回は GitHub Enterprise Cloud → Microsoft Sentinel ワークスペース → カスタムコネクタ → Microsoft Sentinel の順で設定していきます。\nGitHub Enterprise Cloud を設定しよう ここではログを収集する先の GitHub Organization の作成と、Azure Logic Apps から GitHub API へアクセスする際に利用するアクセストークンの作成を実施していきます。\nStep1. Organization(Enterprise)の作成 監査ログなどを取得する対象の Organization を作成しましょう。今回は GitHub Enterprise Cloud とするため課金プランは「Enterprise」を選択します。\nStep2. アクセストークンの作成 次に Azure Logic Apps から GitHub API へアクセスする際に利用するアクセストークンを作成します。Organization の Owner 権限を持つユーザで、Settings \u0026gt; Developer settings \u0026gt; Personal access tokens \u0026gt; Generate new tokens から作成しましょう。\n付与するスコープについては Microsoft Sentinel コミュニティでは言及されておりませんが、筆者の環境では必要最低限のスコープとして次の 5 つの権限を付与することで動作を確認することができました。\n public_repo admin:org read:user user:email admin:enterprise  作成後はデータコネクタをデプロイする際に利用するためトークン情報をコピーしておきます。\n注意: トークン情報が漏れると大変なことになりますので絶対に漏らさないように注意しましょう。\nMicrosoft Sentinel ワークスペースを設定しよう 次に Microsoft Sentinel の設定です。ここでは Microsoft Sentinel 用の Log Analytics ワークスペースの作成および Microsoft Sentinel リソースの作成を実施していきます。\nStep1. ワークスペースの作成 Azure ポータルなどから Microsoft Sentinel および Microsoft Sentinel 用の Log Analytics ワークスペースを作成しましょう。\nStep2. ワークスペース情報の取得 Microsoft Sentinel リソースの作成後は、データコネクタをデプロイする際に利用するため Log Analytics ワークスペース画面のエージェント管理などから「ワークスペース ID」と「主キー」をコピーしておきます。\nカスタムコネクタを設定しよう ここでは GitHub 用カスタムデータコネクタを Azure Resource Manager(ARM) テンプレートからデプロイして、対象 GitHub Organization からログデータを取得できるようにデータコネクタの諸設定まで実施していきます。\nStep1. ARM テンプレートのデプロイ それでは GitHub 用カスタムデータコネクタをデプロイしていきましょう。データコネクタの「Readme」に記載されている Deploy to Azureボタンをクリックします。\nAzure のカスタムテンプレートのデプロイ画面へ遷移したらパラメータ値を入力して作成をしましょう。\n   パラメータ 説明     Personal Access Token GitHub のアクセストークンを指定する   User Name GitHub アクセストークンなどを格納する Azure Key Vault キーコンテナへのアクセスを許可する Azure AD ユーザ名を指定する   Principal Id 上記 Azure AD ユーザのオブジェクト ID を指定する   Workspace Id Microsoft Sentinel ワークスペースのワークスペース ID を指定する   Workspace Key Microsoft Sentinel ワークスペースの主キーを指定する    デプロイにかかる時間は環境により誤差はあると思いますがおおよそ 2 分でした。デプロイが終わると Azure Logic Apps 以外にもストレージアカウントや Azure Key Vault などのリソースも作成されていることがわかります。\nStep2. Azure Key Vault API 接続の設定 Azure Logic Apps から Azure Key Vault キーコンテナに格納された GitHub アクセストークンへのアクセスを可能とするため、keyvault-GitHubPlaybooks リソースの設定画面から API 接続への承認処理を行います。\nStep3. 設定ファイルの作成 データコネクタの 2 種類の設定ファイル ORGS.json と lastrun-Audit.json を作成します。各ファイルの概要およびフォーマットは次の通りです。\n   ファイル名 説明     ORGS.json ログ取得対象の GitHub Organization を定義するファイル   lastrun-Audit.json 最終実行時刻を管理するファイル(新しいレコードのみを収集するために利用される)    ORGS.json [  {​​​​​​​  \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your 1st organization account name\u0026gt;\u0026#34;  }​​​​​​​,  {​​​​​​​​​​​​​​​​​​​​​  \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your 2nd organization account name\u0026gt;\u0026#34;  }​​​​​​​​​​​​​​​​​​​​​,  {​​​​​​​​​​​​​​​​​​​​​  \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your Nth organization account name\u0026gt;\u0026#34;  }​​​​​​​​​​​​​​​​​​​​​ ]  lastrun-Audit.json {​​​​​​​​​​​​​​​​​​​​​  \u0026#34;lastContext\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;lastRun\u0026#34;: \u0026#34;\u0026#34; }​​​​​​​​​​​​​​​​​​​​​  ORGS.json ファイルを編集し、org パラメータ値をログ取得対象の GitHub Organization アカウント名に置きかえます。フォーマットのように、複数の GitHub Organization を指定することもできますので、実際の GitHub Organization の数にあわせて増減させてください。\nlastrun-Audit.json ファイルの各パラメータ値については、カスタムコネクタが動作すると自動的に値が更新されるためユーザ側で意識する必要はありません。\nStep4. 設定ファイルの配置 2 種類のファイルを作成したら、ARM テンプレートから作成されたストレージアカウントの githublogicapp コンテナにアップロードしましょう。\nStep5. Azure Blob Storage API 接続の設定 Azure Logic Apps からストレージアカウントに設定ファイルへのアクセスを可能とするため、ストレージアカウントのアクセスキー情報を取得し、azureblob-GitHubPlaybooks リソースの設定画面から API 接続を行います。\nStep6. Azure Logic Apps の有効化 最後に ARM テンプレートで作成された 3 つの Azure Logic Apps を有効化していきましょう。\nここまでで一通りの設定は終わりましたので、あとはスケジュールに沿ってログの取得が行われるようになっていることを確認していきましょう。\nStep7. 動作確認 まずは Azure Logic Apps から見ていきましょう。Azure Logic Apps では各アプリケーションが正しくスケジュール実行されていること、実行されたアプリケーションが正常終了していることを確認します。出力例のように実行の履歴にて成功と記録されていることが確認できれば OK です。\n次に Log Analytics ワークスペースを確認していきます。ここではカスタムログに GitHub Enterprise Cloud から取得したログが格納されていることを確認します。出力例のように「GitHub_CL」と「GitHubRepoLogs_CL」の 2 種類のカスタムテーブルが作成されていれば OK です。\n最後に Microsoft Sentinel を確認してみましょう。出力例のように「GitHub_CL」および「GitHubRepoLogs_CL」からのイベントが記録されていれば OK です。\n以上でカスタムコネクタのデプロイは終わりです。お疲れ様でした。\nMicrosoft Sentinel を設定しよう さてログデータが収集できるようになりましたので、ここからは収集したログデータを自動で分析して脅威を検出できるように Microsoft Sentinel の設定をしていきましょう。今回は Microsoft Sentinel が標準で用意している GitHub 固有の分析ルールを有効化し、次のイベントを自動で検出できるようにしていきたいと思います。\n GitHub アカウントへのブルートフォース攻撃 (※1) 複数の異なるロケーションからのサインインバースト (※1) 新しい国からのアクティビティ TI インジケータに登録した IP アドレスからのアクセス (※2) 二要素認証の無効化 リポジトリ内にセキュリティ脆弱性  ※1: Azure Active Directory(Azure AD) とのシングルサインオン(SSO)設定、Azure AD ログの収集が必要\n※2: Microsoft Sentinel の脅威インテリジェンス設定にて TI インジケータの事前登録が必要\nStep1. データの解析および正規化 まずは GitHub_CL テーブルおよび GitHubRepoLogs_CL テーブルに格納されているデータを、Microsoft Sentinel で分析しやすいように加工をしていきます。なお、Microsoft Sentinel コミュニティにてデータ加工用のパーサ関数(GitHubAudit 関数、GitHubRepo 関数)が公開されておりますので、今回はこちらを利用していきます。\nhttps://github.com/Azure/Azure-Sentinel/tree/master/Parsers/GitHub\nGitHubAudit関数 GitHub_CL | project TimeGenerated=node_createdAt_t,  Organization=columnifexists(\u0026#39;node_organizationName_s\u0026#39;, \u0026#34;\u0026#34;),  Action=node_action_s,  OperationType=node_operationType_s,  Repository=columnifexists(\u0026#39;node_repositoryName_s\u0026#39;,\u0026#34;\u0026#34;),  Actor=columnifexists(\u0026#39;node_actorLogin_s\u0026#39;, \u0026#34;\u0026#34;),  IPaddress=columnifexists(\u0026#39;node_actorIp_s\u0026#39;, \u0026#34;\u0026#34;),  City=columnifexists(\u0026#39;node_actorLocation_city_s\u0026#39;, \u0026#34;\u0026#34;),  Country=columnifexists(\u0026#39;node_actorLocation_country_s\u0026#39;, \u0026#34;\u0026#34;),  ImpactedUser=columnifexists(\u0026#39;node_userLogin_s\u0026#39;, \u0026#34;\u0026#34;),  ImpactedUserEmail=columnifexists(\u0026#39;node_user_email_s\u0026#39;, \u0026#34;\u0026#34;),  InvitedUserPermission=columnifexists(\u0026#39;node_permission_s\u0026#39;, \u0026#34;\u0026#34;),  Visibility=columnifexists(\u0026#39;node_visibility_s\u0026#39;, \u0026#34;\u0026#34;),  OauthApplication=columnifexists(\u0026#39;node_oauthApplicationName_s\u0026#39;, \u0026#34;\u0026#34;),  OauthApplicationUrl=columnifexists(\u0026#39;node_applicationUrl_s\u0026#39;, \u0026#34;\u0026#34;),  OauthApplicationState=columnifexists(\u0026#39;node_state_s\u0026#39;, \u0026#34;\u0026#34;),  UserCanInviteCollaborators=columnifexists(\u0026#39;node_canInviteOutsideCollaboratorsToRepositories_b\u0026#39;, \u0026#34;\u0026#34;),  MembershipType=columnifexists(\u0026#39;node_membershipTypes_s\u0026#39;, \u0026#34;\u0026#34;),  CurrentPermission=columnifexists(\u0026#39;node_permission_s\u0026#39;, \u0026#34;\u0026#34;),  PreviousPermission=columnifexists(\u0026#39;node_permissionWas_s\u0026#39;, \u0026#34;\u0026#34;),  TeamName=columnifexists(\u0026#39;node_teamName_s\u0026#39;, \u0026#34;\u0026#34;),  Reason=columnifexists(\u0026#39;node_reason_s\u0026#39;, \u0026#34;\u0026#34;),  BlockedUser=columnifexists(\u0026#39;node_blockedUserName_s\u0026#39;, \u0026#34;\u0026#34;),  CanCreateRepositories=columnifexists(\u0026#39;canCreateRepositories_b\u0026#39;, \u0026#34;\u0026#34;)  GitHubRepo関数 GitHubRepoLogs_CL | project TimeGenerated = columnifexists(\u0026#39;DateTime_t\u0026#39;, \u0026#34;\u0026#34;),  Organization=columnifexists(\u0026#39;Organization_s\u0026#39;, \u0026#34;\u0026#34;),  Repository=columnifexists(\u0026#39;Repository_s\u0026#39;,\u0026#34;\u0026#34;),  Action=columnifexists(\u0026#39;LogType_s\u0026#39;,\u0026#34;\u0026#34;),  Actor=coalesce(login_s, owner_login_s),  ActorType=coalesce(owner_type_s, type_s),  IsPrivate=columnifexists(\u0026#39;private_b\u0026#39;,\u0026#34;\u0026#34;),  ForksUrl=columnifexists(\u0026#39;forks_url_s\u0026#39;,\u0026#34;\u0026#34;),  PushedAt=columnifexists(\u0026#39;pushed_at_t\u0026#39;,\u0026#34;\u0026#34;),  IsDisabled=columnifexists(\u0026#39;disabled_b\u0026#39;,\u0026#34;\u0026#34;),  AdminPermissions=columnifexists(\u0026#39;permissions_admin_b\u0026#39;,\u0026#34;\u0026#34;),  PushPermissions=columnifexists(\u0026#39;permissions_push_b\u0026#39;,\u0026#34;\u0026#34;),  PullPermissions=columnifexists(\u0026#39;permissions_pull_b\u0026#39;,\u0026#34;\u0026#34;),  ForkCount=columnifexists(\u0026#39;forks_count_d\u0026#39;,\u0026#34;\u0026#34;),  Count=columnifexists(\u0026#39;count_d,\u0026#39;,\u0026#34;\u0026#34;),  UniqueUsersCount=columnifexists(\u0026#39;uniques_d\u0026#39;,\u0026#34;\u0026#34;),  DismmisedAt=columnifexists(\u0026#39;dismissedAt_t\u0026#39;,\u0026#34;\u0026#34;),  Reason=columnifexists(\u0026#39;dismissReason_s\u0026#39;,\u0026#34;\u0026#34;),  vulnerableManifestFilename = columnifexists(\u0026#39;vulnerableManifestFilename_s\u0026#39;,\u0026#34;\u0026#34;),  Description=columnifexists(\u0026#39;securityAdvisory_description_s\u0026#39;,\u0026#34;\u0026#34;),  Link=columnifexists(\u0026#39;securityAdvisory_permalink_s\u0026#39;,\u0026#34;\u0026#34;),  PublishedAt=columnifexists(\u0026#39;securityAdvisory_publishedAt_t \u0026#39;,\u0026#34;\u0026#34;),  Severity=columnifexists(\u0026#39;securityAdvisory_severity_s\u0026#39;,\u0026#34;\u0026#34;),  Summary=columnifexists(\u0026#39;securityAdvisory_summary_s\u0026#39;,\u0026#34;\u0026#34;)  次の画像を参考に Microsoft Sentinel のログ画面から 2 つのパーサ関数を登録しましょう。なお、「従来のカテゴリ」の値については任意の文字列で大丈夫です。\nStep2. 分析ルールの追加 次に GitHub 固有の脅威を検出できるように分析ルールをテンプレートから追加していきしょう。Microsoft Sentinel では GitHub 固有の脅威に対する分析ルールのテンプレートが用意されていますので、今回はこちらを活用して分析ルールの追加をしてきたいと思います。出力例のように検索キーワードに「github」と入力することで目的のテンプレートを見つけることができます。\nなお、2021 年 11 月時点で用意されているテンプレートは次の 6 種類となります。\n   No. テンプレート名 説明     1 GitHub Two Factor Auth Disable 二要素認証の無効化イベントを検知するルール、デフォルトでは 1 日ごとにクエリを実行   2 GitHub Activites from a New Country 新しい国からのアクティビティを検知するルール、デフォルトでは過去 7 日分を学習データに利用して 1 日ごとにクエリを実行   3 Brute Force Attack against GitHub Account GitHub アカウントへのブルートフォース攻撃を検知するルール、デフォルトでは 1 日ごとにクエリを実行   4 GitHub Signin Burst from Multiple Locations 複数の異なるロケーションからのサインインバーストを検知するルール、デフォルトでは 1 時間ごとにクエリを実行   5 TI map IP entity to GitHub_CL TI インジケータに登録した IP アドレスからのアクセスを検知するルール、デフォルトでは 1 時間ごとにクエリを実行   6 GitHub Security Vulnerability in Repository リポジトリ内にセキュリティ脆弱性が含まれていることを検知するルール、デフォルトでは 1 時間ごとにクエリを実行    それではテンプレートを用いた分析ルールの追加をしていきましょう。まず追加したいルールを選択し、「ルールの作成」ボタンをクリックします。\nルールの追加画面ではウィザードに従ってルールの作成を行っていきます。各パラメータにはテンプレートによってデフォルト値が入力されていますので特に値を変更する要件がなければそのまま作成していきます。\n分析ルールが追加されたテンプレートについては次の出力例のように「使用中」アイコンが付与されます。先ほどと同様の手順を繰り返し、他のテンプレートに対しても分析ルールの追加をしていきましょう。出力例のように 6 種類すべてに使用中アイコンが付与されれば脅威を自動検知するルールの設定も終わりです。お疲れ様でした。\n補足、もう一歩踏み込んだ脅威分析を行うには Microsoft Sentinel では今回紹介した標準の分析ルールを利用する以外に、カスタム分析ルールを自作してより高度なイベントの検知をすることが可能です。サンプルとして次のようなカスタム分析ルールが Microsoft 技術ブログでも紹介されていますのでぜひ参考にしてみてください。\n リポジトリに対する異常数のクローン操作 リポジトリの一括削除 リポジトリをプライベートからパブリックに変更 リポジトリに対する部外者からのフォーク操作 GitHub Organization へのユーザ招待およびユーザ追加 ユーザへのアクセス許可の追加付与　など  https://techcommunity.microsoft.com/t5/microsoft-sentinel-blog/protecting-your-github-assets-with-azure-sentinel/ba-p/1457721\n終わりに 今回は GitHub Enterprise Cloud の各種ログを SIEM マネージドサービスである Microsoft Sentinel (旧称 Azure Sentinel) に集約し、サイバー攻撃の兆候を検知できるようにする方法のご紹介でしたがいかがだったでしょうか？\nGitHub Enterprise Cloud に限らず様々な監査ログを収集し、長期保存されている方は多いと思います。ただ、せっかく取得するのであればただ集めて長期保存しておくだけではなく、Microsoft Sentinel を活用してもう一歩進んだセキュアな環境を実現してみる、というのも \u0026ldquo;有り\u0026rdquo; なのではないでしょうか。\n以上、「Microsoft Sentinel を使って GitHub Enterprise Cloud のセキュリティを強化しよう (Azure Logic Apps 編)」でした。\n  Microsoft Azure は，Microsoft Corporation の商標または登録商標です。 GitHub は、GitHub Inc. の商標または登録商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。    SIEM(Security Information and Event Management) は、様々なログを一元的に集約、相関分析をしてサイバー攻撃などの異常を自動的に検出するソリューションです。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://chacco38.github.io/posts/2021/11/azure-sentinel-logicapps-data-connector-for-ghec/","summary":"はじめに みなさん、こんにちは。今回は GitHub Enterprise Cloud(GHEC) の各種ログを SIEM1 マネージドサービスである Microsoft Sentinel (旧称 Azure Sentinel) に集約し、サイバー攻撃の兆候を検知できるようにする方法をご紹介していきたいと思います。\nMicrosoft Sentinel と GHEC との連携方法はいくつかあるのですが、この記事では Microsoft Sentinel コミュニティで開発している Azure Logic Apps(ロジックアプリ)、Azure Functions(関数アプリ)の 2 種類のカスタムデータコネクタの内、「Azure Logic Apps コネクタ」を使った方法をご紹介していきたいと思います。\nこれから GHEC の利用を検討している方や、GHEC は利用しているけれど SIEM システムの導入まではしていないという方は、セキュリティ強化策のひとつとして参考にしてみてはいかがでしょうか。\n構築するシステムについて 今回は Azure Logic Apps(ロジックアプリ)を定期的に起動し、GHEC から監査ログなどを取得して Microsoft Sentinel ワークスペースへ格納、格納されたログに対して Microsoft Sentinel が自動的に相関分析をかけていく、といった流れで処理を行うシステムを構築していきたいと思います。\nAzure Sentinel とのコネクタについては、今回は Microsoft Sentinel コミュニティで公開されている次のカスタムデータコネクタを利用していきます。\nhttps://github.com/Azure/Azure-Sentinel/tree/master/DataConnectors/GitHub\n本カスタムコネクタをデプロイすると、次の 3 種類の Azure Logic Apps リソースが動作するようになります。\n   リソース種別 説明     Audit Playbook 監査ログを定期的に収集する自動ワークフロー (デフォルト 5 分間隔)   Repo Playbook 各リポジトリに対するフォーク、クローン、コミットなどの操作ログを定期的に収集する自動ワークフロー (デフォルト 1 時間間隔)   Vulnerability Alert Playbook 各リポジトリに対するセキュリティ脆弱性診断ログを定期的に収集する自動ワークフロー (デフォルト 1 日間隔)    また、本カスタムコネクタで取得した各種ログデータについては、Log Analytics ワークスペースの次のカスタムテーブルへ格納されるようになります。","title":"Microsoft Sentinel を使って GitHub Enterprise Cloud のセキュリティを強化しよう (Azure Logic Apps コネクタ編)"},{"content":"はじめに みなさん、こんにちは。今回は GitHub Enterprise Cloud(GHEC) の各種ログを SIEM1 マネージドサービスである Microsoft Sentinel (旧称 Azure Sentinel) に集約し、サイバー攻撃の兆候を検知できるようにする方法をご紹介していきたいと思います。\nMicrosoft Sentinel と GHEC との連携方法はいくつかあるのですが、この記事では Microsoft Sentinel コミュニティで開発している Azure Logic Apps(ロジックアプリ)、Azure Functions(関数アプリ)の 2 種類のカスタムデータコネクタの内、「Azure Functions コネクタ」を使った方法をご紹介していきたいと思います。\nこれから GHEC の利用を検討している方や、GHEC は利用しているけれど SIEM システムの導入まではしていないという方は、セキュリティ強化策のひとつとして参考にしてみてはいかがでしょうか。\n構築するシステムについて 今回は Azure Functions(関数アプリ)を定期的に起動し、GHEC から監査ログなどを取得して Microsoft Sentinel ワークスペースへ格納、格納されたログに対して Microsoft Sentinel が自動的に相関分析をかけていく、といった流れで処理を行うシステムを構築していきたいと思います。\nAzure Sentinel とのコネクタについては、今回は Microsoft Sentinel コミュニティで公開されている次のカスタムデータコネクタを利用していきます。\nhttps://github.com/Azure/Azure-Sentinel/blob/master/DataConnectors/GithubFunction\n本カスタムコネクタで取得した各種ログデータについては、Log Analytics ワークスペースの次のカスタムテーブルへ格納されるようになります。\n   テーブル名 説明     GitHub_CL 監査ログのデータを格納するテーブル   GitHubRepoLogs_CL 各リポジトリに対するフォーク、クローン、コミットなどの操作ログやリポジトリに対するセキュリティ脆弱性診断ログのデータを格納するテーブル    それでは構築していきましょう 今回は GitHub Enterprise Cloud → Microsoft Sentinel ワークスペース → カスタムコネクタ → Microsoft Sentinel の順で設定していきます。\nGitHub Enterprise Cloud を設定しよう ここではログを収集する先の GitHub Organization の作成と、Azure Functions から GitHub API へアクセスする際に利用するアクセストークンの作成を実施していきます。\nStep1. Organization(Enterprise) の作成 監査ログなどを取得する対象の Organization を作成しましょう。今回は GitHub Enterprise Cloud とするため課金プランは「Enterprise」を選択します。\nStep2. アクセストークンの作成 次に Azure Logic Apps から GitHub API へアクセスする際に利用するアクセストークンを作成します。Organization の Owner 権限を持つユーザで、Settings \u0026gt; Developer settings \u0026gt; Personal access tokens \u0026gt; Generate new tokens から作成しましょう。\n付与するスコープについては Microsoft Sentinel コミュニティでは言及されておりませんが、筆者の環境では必要最低限のスコープとして次の 5 つの権限を付与することで動作を確認することができました。\n public_repo admin:org read:user user:email admin:enterprise  作成後はデータコネクタをデプロイする際に利用するためトークン情報をコピーしておきます。 注意: トークン情報が漏れると大変なことになりますので絶対に漏らさないように注意しましょう。\nMicrosoft Sentinel ワークスペースを設定しよう 次に Microsoft Sentinel ワークスペースの設定です。ここでは Log Analytics ワークスペースの作成および Log Analytics ワークスペースへの Microsoft Sentinel 機能の追加を実施していきます。\nStep1. ワークスペースの作成 Azure ポータルなどから Log Analytics ワークスペースの作成および作成したワークスペースへの Microsoft Sentinel 機能の追加を次の例のように実施していきます。\nStep2. ワークスペース情報の取得 Microsoft Sentinel ワークスペースの作成後は、データコネクタをデプロイする際に利用するため Log Analytics ワークスペース画面のエージェント管理などから「ワークスペース ID」と「主キー」をコピーしておきます。\nカスタムコネクタを設定しよう ここでは GitHub 用カスタムデータコネクタを ARM テンプレートからデプロイして、対象 GitHub Organization からログデータを取得できるようにデータコネクタの諸設定まで実施していきます。\nStep1. ARM テンプレートのデプロイ それでは GitHub 用カスタムデータコネクタをデプロイしていきましょう。データコネクタの「Readme」に記載されている Deploy to Azureボタンをクリックします。\nAzure のカスタムテンプレートのデプロイ画面へ遷移したらパラメータ値を入力して作成をしましょう。\n   パラメータ 説明     Personal Access Token GitHub のアクセストークンを指定する   Workspace Id Microsoft Sentinel ワークスペースのワークスペース ID を指定する   Workspace Key Microsoft Sentinel ワークスペースの主キーを指定する   Function Schedule Azure Functions の起動スケジュールを「\u0026ldquo;秒\u0026rdquo; \u0026ldquo;分\u0026rdquo; \u0026ldquo;時\u0026rdquo; \u0026ldquo;日付\u0026rdquo; \u0026ldquo;月\u0026rdquo; \u0026ldquo;曜日\u0026rdquo;」で指定する(デフォルト 10 分間隔)    デプロイにかかる時間は環境により誤差はあると思いますがおおよそ 3 分でした。デプロイが終わると Azure Functions 以外にもストレージアカウントや Azure Key Vault などのリソースも作成されていることがわかります。\nStep2. 設定ファイルの作成 データコネクタの 2 種類の設定ファイル ORGS.json と lastrun-Audit.json を作成します。各ファイルの概要およびフォーマットは次の通りです。\n   ファイル名 説明     ORGS.json ログ取得対象の GitHub Organization を定義するファイル   lastrun-Audit.json 最終実行時刻を管理するファイル(新しいレコードのみを収集するために利用される)    ORGS.json [  {  \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your 1st organization account name\u0026gt;\u0026#34;  },  {  \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your 2nd organization account name\u0026gt;\u0026#34;  },  {  \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your Nth organization account name\u0026gt;\u0026#34;  } ]  lastrun-Audit.json [  {  \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your 1st organization account name\u0026gt;\u0026#34;  \u0026#34;lastContext\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;lastRun\u0026#34;: \u0026#34;\u0026#34;  },  {  \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your 2nd organization account name\u0026gt;\u0026#34;  \u0026#34;lastContext\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;lastRun\u0026#34;: \u0026#34;\u0026#34;  },  {  \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your Nth organization account name\u0026gt;\u0026#34;  \u0026#34;lastContext\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;lastRun\u0026#34;: \u0026#34;\u0026#34;  } ]  各ファイルを編集し、org パラメータ値をログ取得対象の GitHub Organization アカウント名に置きかえます。フォーマットのように、複数の GitHub Organization を指定することもできますので、実際の GitHub Organization の数にあわせて増減させてください。\nなお、lastrun-Audit.json ファイルには org 以外のパラメータもありますが、これらはカスタムコネクタが動作すると自動的に値が更新されるため、ユーザ側で意識する必要はありません。\nStep3. 設定ファイルの配置 2 種類のファイルを作成したら、ARM テンプレートから作成されたストレージアカウントの github-repo-logs コンテナにアップロードしましょう。\nここまでで一通りの設定は終わりましたので、あとはスケジュールに沿ってログの取得が行われるようになっていることを確認していきましょう。\nStep4. 動作確認 まずは Azure Functions から見ていきましょう。Azure Functions では、関数が正しくスケジュール実行されていること、実行された関数が正常終了していることを確認します。出力例のように実行回数と成功回数がカウントされていれば OK です。\n次に Log Analytics ワークスペースを確認していきます。ここではカスタムログに GitHub Enterprise Cloud から取得したログが格納されていることを確認します。出力例のように「GitHub_CL」と「GitHubRepoLogs_CL」の 2 種類のカスタムテーブルが作成されていれば OK です。\n最後に Microsoft Sentinel を確認してみましょう。出力例のように「GitHub_CL」および「GitHubRepoLogs_CL」からのイベントが記録されていれば OK です。\n以上でカスタムコネクタのデプロイは終わりです。お疲れ様でした。\nMicrosoft Sentinel を設定しよう さてログデータが収集できるようになりましたので、ここからは収集したログデータを自動で分析して脅威を検出できるように Microsoft Sentinel の設定をしていきましょう。今回は Microsoft Sentinel が標準で用意している GitHub 固有の分析ルールを有効化し、次のイベントを自動で検出できるようにしていきたいと思います。\n GitHub アカウントへのブルートフォース攻撃 (※1) 複数の異なるロケーションからのサインインバースト (※1) 新しい国からのアクティビティ TI インジケータに登録した IP アドレスからのアクセス (※2) 二要素認証の無効化 リポジトリ内にセキュリティ脆弱性  ※1: Azure Active Directory(Azure AD) とのシングルサインオン(SSO)設定、Azure AD ログの収集が必要 ※2: Microsoft Sentinel の脅威インテリジェンス設定にて TI インジケータの事前登録が必要\nStep1. データの解析および正規化 まずは GitHub_CL テーブルおよび GitHubRepoLogs_CL テーブルに格納されているデータを、Microsoft Sentinel で分析しやすいように加工をしていきます。なお、Microsoft Sentinel コミュニティにてデータ加工用のパーサ関数(GitHubAudit 関数、GitHubRepo 関数)が公開されておりますので、今回はこちらを利用していきます。\nhttps://github.com/Azure/Azure-Sentinel/tree/master/Parsers/GitHub\nGitHubAudit関数 GitHub_CL | project TimeGenerated=node_createdAt_t,  Organization=columnifexists(\u0026#39;node_organizationName_s\u0026#39;, \u0026#34;\u0026#34;),  Action=node_action_s,  OperationType=node_operationType_s,  Repository=columnifexists(\u0026#39;node_repositoryName_s\u0026#39;,\u0026#34;\u0026#34;),  Actor=columnifexists(\u0026#39;node_actorLogin_s\u0026#39;, \u0026#34;\u0026#34;),  IPaddress=columnifexists(\u0026#39;node_actorIp_s\u0026#39;, \u0026#34;\u0026#34;),  City=columnifexists(\u0026#39;node_actorLocation_city_s\u0026#39;, \u0026#34;\u0026#34;),  Country=columnifexists(\u0026#39;node_actorLocation_country_s\u0026#39;, \u0026#34;\u0026#34;),  ImpactedUser=columnifexists(\u0026#39;node_userLogin_s\u0026#39;, \u0026#34;\u0026#34;),  ImpactedUserEmail=columnifexists(\u0026#39;node_user_email_s\u0026#39;, \u0026#34;\u0026#34;),  InvitedUserPermission=columnifexists(\u0026#39;node_permission_s\u0026#39;, \u0026#34;\u0026#34;),  Visibility=columnifexists(\u0026#39;node_visibility_s\u0026#39;, \u0026#34;\u0026#34;),  OauthApplication=columnifexists(\u0026#39;node_oauthApplicationName_s\u0026#39;, \u0026#34;\u0026#34;),  OauthApplicationUrl=columnifexists(\u0026#39;node_applicationUrl_s\u0026#39;, \u0026#34;\u0026#34;),  OauthApplicationState=columnifexists(\u0026#39;node_state_s\u0026#39;, \u0026#34;\u0026#34;),  UserCanInviteCollaborators=columnifexists(\u0026#39;node_canInviteOutsideCollaboratorsToRepositories_b\u0026#39;, \u0026#34;\u0026#34;),  MembershipType=columnifexists(\u0026#39;node_membershipTypes_s\u0026#39;, \u0026#34;\u0026#34;),  CurrentPermission=columnifexists(\u0026#39;node_permission_s\u0026#39;, \u0026#34;\u0026#34;),  PreviousPermission=columnifexists(\u0026#39;node_permissionWas_s\u0026#39;, \u0026#34;\u0026#34;),  TeamName=columnifexists(\u0026#39;node_teamName_s\u0026#39;, \u0026#34;\u0026#34;),  Reason=columnifexists(\u0026#39;node_reason_s\u0026#39;, \u0026#34;\u0026#34;),  BlockedUser=columnifexists(\u0026#39;node_blockedUserName_s\u0026#39;, \u0026#34;\u0026#34;),  CanCreateRepositories=columnifexists(\u0026#39;canCreateRepositories_b\u0026#39;, \u0026#34;\u0026#34;)  GitHubRepo関数 GitHubRepoLogs_CL | project TimeGenerated = columnifexists(\u0026#39;DateTime_t\u0026#39;, \u0026#34;\u0026#34;),  Organization=columnifexists(\u0026#39;Organization_s\u0026#39;, \u0026#34;\u0026#34;),  Repository=columnifexists(\u0026#39;Repository_s\u0026#39;,\u0026#34;\u0026#34;),  Action=columnifexists(\u0026#39;LogType_s\u0026#39;,\u0026#34;\u0026#34;),  Actor=coalesce(login_s, owner_login_s),  ActorType=coalesce(owner_type_s, type_s),  IsPrivate=columnifexists(\u0026#39;private_b\u0026#39;,\u0026#34;\u0026#34;),  ForksUrl=columnifexists(\u0026#39;forks_url_s\u0026#39;,\u0026#34;\u0026#34;),  PushedAt=columnifexists(\u0026#39;pushed_at_t\u0026#39;,\u0026#34;\u0026#34;),  IsDisabled=columnifexists(\u0026#39;disabled_b\u0026#39;,\u0026#34;\u0026#34;),  AdminPermissions=columnifexists(\u0026#39;permissions_admin_b\u0026#39;,\u0026#34;\u0026#34;),  PushPermissions=columnifexists(\u0026#39;permissions_push_b\u0026#39;,\u0026#34;\u0026#34;),  PullPermissions=columnifexists(\u0026#39;permissions_pull_b\u0026#39;,\u0026#34;\u0026#34;),  ForkCount=columnifexists(\u0026#39;forks_count_d\u0026#39;,\u0026#34;\u0026#34;),  Count=columnifexists(\u0026#39;count_d,\u0026#39;,\u0026#34;\u0026#34;),  UniqueUsersCount=columnifexists(\u0026#39;uniques_d\u0026#39;,\u0026#34;\u0026#34;),  DismmisedAt=columnifexists(\u0026#39;dismissedAt_t\u0026#39;,\u0026#34;\u0026#34;),  Reason=columnifexists(\u0026#39;dismissReason_s\u0026#39;,\u0026#34;\u0026#34;),  vulnerableManifestFilename = columnifexists(\u0026#39;vulnerableManifestFilename_s\u0026#39;,\u0026#34;\u0026#34;),  Description=columnifexists(\u0026#39;securityAdvisory_description_s\u0026#39;,\u0026#34;\u0026#34;),  Link=columnifexists(\u0026#39;securityAdvisory_permalink_s\u0026#39;,\u0026#34;\u0026#34;),  PublishedAt=columnifexists(\u0026#39;securityAdvisory_publishedAt_t \u0026#39;,\u0026#34;\u0026#34;),  Severity=columnifexists(\u0026#39;securityAdvisory_severity_s\u0026#39;,\u0026#34;\u0026#34;),  Summary=columnifexists(\u0026#39;securityAdvisory_summary_s\u0026#39;,\u0026#34;\u0026#34;)  次の画像を参考に Microsoft Sentinel のログ画面から 2 つのパーサ関数を登録しましょう。なお、「従来のカテゴリ」の値については任意の文字列で大丈夫です。\nStep2. 分析ルールの追加 次に GitHub 固有の脅威を検出できるように分析ルールをテンプレートから追加していきしょう。Microsoft Sentinel では GitHub 固有の脅威に対する分析ルールのテンプレートが用意されていますので、今回はこちらを活用して分析ルールの追加をしてきたいと思います。出力例のように検索キーワードに「github」と入力することで目的のテンプレートを見つけることができます。\nなお、2021 年 11 月時点で用意されているテンプレートは次の 6 種類となります。\n   No. テンプレート名 説明     1 GitHub Two Factor Auth Disable 二要素認証の無効化イベントを検知するルール、デフォルトでは 1 日ごとにクエリを実行   2 GitHub Activites from a New Country 新しい国からのアクティビティを検知するルール、デフォルトでは過去 7 日分を学習データに利用して 1 日ごとにクエリを実行   3 Brute Force Attack against GitHub Account GitHub アカウントへのブルートフォース攻撃を検知するルール、デフォルトでは 1 日ごとにクエリを実行   4 GitHub Signin Burst from Multiple Locations 複数の異なるロケーションからのサインインバーストを検知するルール、デフォルトでは 1 時間ごとにクエリを実行   5 TI map IP entity to GitHub_CL TI インジケータに登録した IP アドレスからのアクセスを検知するルール、デフォルトでは 1 時間ごとにクエリを実行   6 GitHub Security Vulnerability in Repository リポジトリ内にセキュリティ脆弱性が含まれていることを検知するルール、デフォルトでは 1 時間ごとにクエリを実行    それではテンプレートを用いた分析ルールの追加をしていきましょう。まず追加したいルールを選択し、「ルールの作成」ボタンをクリックします。\nルールの追加画面ではウィザードに従ってルールの作成を行っていきます。各パラメータにはテンプレートによってデフォルト値が入力されていますので特に値を変更する要件がなければそのまま作成していきます。\n分析ルールが追加されたテンプレートについては次の出力例のように「使用中」アイコンが付与されます。先ほどと同様の手順を繰り返し、他のテンプレートに対しても分析ルールの追加をしていきましょう。出力例のように 6 種類すべてに使用中アイコンが付与されれば脅威を自動検知するルールの設定も終わりです。お疲れ様でした。\n補足、もう一歩踏み込んだ脅威分析を行うには Microsoft Sentinel では今回紹介した標準の分析ルールを利用する以外に、カスタム分析ルールを自作してより高度なイベントの検知をすることが可能です。サンプルとして次のようなカスタム分析ルールが Microsoft 技術ブログでも紹介されていますのでぜひ参考にしてみてください。\n リポジトリに対する異常数のクローン操作 リポジトリの一括削除 リポジトリをプライベートからパブリックに変更 リポジトリに対する部外者からのフォーク操作 GitHub Organization へのユーザ招待およびユーザ追加 ユーザへのアクセス許可の追加付与　など  https://techcommunity.microsoft.com/t5/microsoft-sentinel-blog/protecting-your-github-assets-with-azure-sentinel/ba-p/1457721\n終わりに 今回は GitHub Enterprise Cloud の各種ログを SIEM マネージドサービスである Microsoft Sentinel (旧称 Azure Sentinel) に集約し、サイバー攻撃の兆候を検知できるようにする方法のご紹介でしたがいかがだったでしょうか？\nGitHub Enterprise Cloud に限らず様々な監査ログを収集し、長期保存されている方は多いと思います。ただ、せっかく取得するのであればただ集めて長期保存しておくだけではなく、Microsoft Sentinel を活用してもう一歩進んだセキュアな環境を実現してみる、というのも \u0026ldquo;有り\u0026rdquo; なのではないでしょうか。\n以上、「Microsoft Sentinel を使って GitHub Enterprise Cloud のセキュリティを強化しよう (Azure Functions コネクタ編)」でした。\n  Microsoft Azure は，Microsoft Corporation の商標または登録商標です。 GitHub は、GitHub Inc. の商標または登録商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。    SIEM(Security Information and Event Management) は、様々なログを一元的に集約、相関分析をしてサイバー攻撃などの異常を自動的に検出するソリューションです。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://chacco38.github.io/posts/2021/11/azure-sentinel-functions-data-connector-for-ghec/","summary":"はじめに みなさん、こんにちは。今回は GitHub Enterprise Cloud(GHEC) の各種ログを SIEM1 マネージドサービスである Microsoft Sentinel (旧称 Azure Sentinel) に集約し、サイバー攻撃の兆候を検知できるようにする方法をご紹介していきたいと思います。\nMicrosoft Sentinel と GHEC との連携方法はいくつかあるのですが、この記事では Microsoft Sentinel コミュニティで開発している Azure Logic Apps(ロジックアプリ)、Azure Functions(関数アプリ)の 2 種類のカスタムデータコネクタの内、「Azure Functions コネクタ」を使った方法をご紹介していきたいと思います。\nこれから GHEC の利用を検討している方や、GHEC は利用しているけれど SIEM システムの導入まではしていないという方は、セキュリティ強化策のひとつとして参考にしてみてはいかがでしょうか。\n構築するシステムについて 今回は Azure Functions(関数アプリ)を定期的に起動し、GHEC から監査ログなどを取得して Microsoft Sentinel ワークスペースへ格納、格納されたログに対して Microsoft Sentinel が自動的に相関分析をかけていく、といった流れで処理を行うシステムを構築していきたいと思います。\nAzure Sentinel とのコネクタについては、今回は Microsoft Sentinel コミュニティで公開されている次のカスタムデータコネクタを利用していきます。\nhttps://github.com/Azure/Azure-Sentinel/blob/master/DataConnectors/GithubFunction\n本カスタムコネクタで取得した各種ログデータについては、Log Analytics ワークスペースの次のカスタムテーブルへ格納されるようになります。\n   テーブル名 説明     GitHub_CL 監査ログのデータを格納するテーブル   GitHubRepoLogs_CL 各リポジトリに対するフォーク、クローン、コミットなどの操作ログやリポジトリに対するセキュリティ脆弱性診断ログのデータを格納するテーブル    それでは構築していきましょう 今回は GitHub Enterprise Cloud → Microsoft Sentinel ワークスペース → カスタムコネクタ → Microsoft Sentinel の順で設定していきます。","title":"Microsoft Sentinel を使って GitHub Enterprise Cloud のセキュリティを強化しよう (Azure Functions コネクタ編)"},{"content":"はじめに みなさん、こんにちは。今回は TaskCat というオープンソースを利用した AWS CloudFormation (CFn) テンプレートの自動テストについてのお話です。\nCFn テンプレートを扱っていると構文エラーチェックはパスしたものの、いざ動かしてみたらスタックの作成でエラーになってしまうといった経験をすることがあるかと思います。TaskCat は多くの方にとってあまり馴染みのないツールだと思いますが、実際に使ってみるととても手軽に CFn テンプレートの自動テストをすることができます。\n今回は Linux 上に開発環境を作るところからはじめて、簡素なサンプルを用いたテストの実行、テスト自動化を組み込んだシンプルな CI/CD パイプラインの構築まで紹介していきたいと思います。これから CFn テンプレート開発されている方で自動テストをやりたいと考えている方は参考にしてみてはいかがでしょうか。\nTaskCat とは TaskCat とは、AWS CloudFormation (CFn) テンプレートの自動テストを行う Python 製のテストツールです。\nこのツールを利用することで、指定した各リージョンに CFn テンプレートから環境を一時的にデプロイ、各リージョンでのデプロイ可否結果のレポート生成、テストで一時的に作成した環境を削除、といった一連の流れを自動化することができます。\nなお、AWS TackCat はローカルでテストを実行する際に Docker が必要となるため、Docker をサポートしていない AWS CloudShell では利用することができないのでご注意ください。\nhttps://github.com/aws-quickstart/taskcat\nTaskCat を使ってみよう 「はじめに」で既に述べたとおり、今回は Linux 上に開発環境を作るところからはじめて、簡素なサンプルを用いたテストの実行、テスト自動化を組み込んだシンプルな CI/CD パイプラインの構築まで紹介していきたいと思います。\nまずは開発環境の設定から それでは Linux 上に開発環境を作っていきたいと思います。まず TaskCat をインストールする事前準備として Python の仮想環境を作成していきましょう。なお、今回の例で使用している Linux ディストリビューションは Amazon Linux 2 です。\n$ sudo yum install -y python3 $ python3 --version Python 3.7.10 $ python3 -m venv venv37 $ . venv37/bin/activate 次に、TaskCat をインストールします。\n$ python3 -m pip install taskcat $ taskcat --version  _ _ _ | |_ __ _ ___| | _____ __ _| |_ | __/ _` / __| |/ / __/ _` | __| | || (_| \\__ \\  \u0026lt; (_| (_| | |_  \\__\\__,_|___/_|\\_\\___\\__,_|\\__|  version 0.9.25 0.9.25 TaskCat のテストに必要な docker サービスやこの後のステップで利用する git を追加で設定します。\n$ sudo yum install -y docker git $ sudo systemctl start docker 最後に、AWS CLI の設定をして開発環境の構築は完了です。\n$ aws configure 手動でテストを実行してみよう では簡単なサンプルを用いて TaskCat を使ったテストを行っていきましょう。今回は、東京リージョン(ap-northeast-1)と大阪リージョン(ap-northeast-3)の 2 つのリージョンに対して、同じテンプレートを使ってスタックの作成ができるかを確認していきたいと思います。\nStep1. テスト対象のテンプレートを作成しよう 今回は VPC を作るだけのとてもシンプルなテンプレートを用意しました。\nAWSTemplateFormatVersion: \u0026#34;2010-09-09\u0026#34; Description: Sample CloudFormation Template  Parameters:  vpcIpv4CicdBlock:  Type: String  Default: 10.0.0.0/16  vpcNameTag:  Type: String  Resources:  myVPC:  Type: AWS::EC2::VPC  Properties:  CidrBlock: !Ref vpcIpv4CicdBlock  EnableDnsSupport: true  EnableDnsHostnames: true  Tags:  - Key: Name  Value: !Ref vpcNameTag  Outputs:  myVpcId:  Description: VPC ID  Value: !Ref myVPC  Export:  Name: myVpcId Step2. TaskCat のテスト定義ファイルを作成しよう 次に TaskCat のテスト定義ファイル .taskcat.yml を作成します。今回の例では東京リージョン(ap-northeast-1)と大阪リージョン(ap-northeast-3)でテストを実施するように定義しています。\nproject:  name: sample-taskcat-project  regions:  - ap-northeast-1  - ap-northeast-3 tests:  test-my-vpc:  parameters:  vpcIpv4CicdBlock: 10.255.0.0/16  vpcNameTag: test-vpc  template: my-vpc.yml Step3. テストを実行してみよう では次のコマンドでテストを実行していきましょう。テストを実行すると、スタック作成が東京リージョンと大阪リージョンに対して並列で実行され、各リージョンでのスタック作成の成否結果の収集、スタック削除まで自動的に行われます。\n$ taskcat test run  _ _ _ | |_ __ _ ___| | _____ __ _| |_ | __/ _` / __| |/ / __/ _` | __| | || (_| \\__ \\  \u0026lt; (_| (_| | |_  \\__\\__,_|___/_|\\_\\___\\__,_|\\__|  version 0.9.25 [INFO ] : Linting passed for file: /root/aws-taskcat-sample/my-vpc.yaml [S3: -\u0026gt; ] s3://tcat-sample-taskcat-project-XXXXXXX/sample-taskcat-project/my-vpc.yaml [INFO ] : ┏ stack Ⓜ tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f [INFO ] : ┣ region: ap-northeast-1 [INFO ] : ┗ status: CREATE_COMPLETE [INFO ] : ┏ stack Ⓜ tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f [INFO ] : ┣ region: ap-northeast-3 [INFO ] : ┗ status: CREATE_COMPLETE [INFO ] : Reporting on arn:aws:cloudformation:ap-northeast-1:\u0026lt;AWSアカウント名\u0026gt;:stack/tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f/XXXXXXX [INFO ] : Reporting on arn:aws:cloudformation:ap-northeast-3:\u0026lt;AWSアカウント名\u0026gt;:stack/tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f/XXXXXXX [INFO ] : Deleting stack: arn:aws:cloudformation:ap-northeast-3:\u0026lt;AWSアカウント名\u0026gt;:stack/tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f/XXXXXXX [INFO ] : Deleting stack: arn:aws:cloudformation:ap-northeast-1:\u0026lt;AWSアカウント名\u0026gt;:stack/tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f/XXXXXXX [INFO ] : ┏ stack Ⓜ tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f [INFO ] : ┣ region: ap-northeast-1 [INFO ] : ┗ status: DELETE_COMPLETE [INFO ] : ┏ stack Ⓜ tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f [INFO ] : ┣ region: ap-northeast-3 [INFO ] : ┗ status: DELETE_COMPLETE 実行後は taskcat_outputs ディレクトリにリージョンごとの実行ログが出力されますので、こちらからスタック作成の成否結果を確認することができます。\n$ tree -a taskcat_outputs/ taskcat_outputs/ ├── index.html ├── tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f-ap-northeast-1-cfnlogs.txt └── tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f-ap-northeast-3-cfnlogs.txt  0 directories, 3 files $ cat taskcat_outputs/*-ap-northeast-3-*.txt ----------------------------------------------------------------------------- Region: ap-northeast-3 StackName: tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f ***************************************************************************** ResourceStatusReason: Stack launch was successful ***************************************************************************** : 以上、ローカル環境で CFn テンプレートのテストを実行する方法のご紹介でした。\nCI/CD パイプラインに組み込んでみよう 次はもう一歩進んで、ソースコードの更新をトリガーに自動でテストを実行する CI/CD パイプラインを構築し、実際に動かすところまで行ってみたいと思います。なお、今回は AWS リソース作成を簡略化するため AWS クイックスタートを利用して CI/CD パイプラインを構築していきたいと思います。\nhttps://aws.amazon.com/jp/quickstart/architecture/cicd-taskcat/\nなお、今回作成するパイプラインでは開発ブランチ(例では develop ブランチ)に対して更新が入ると、それをトリガーに AWS TackCat を活用した自動テストを実行し、テストに成功したら特定ブランチ(例では main ブランチ)へマージするという一連の流れを自動化しています。\nStep1. GitHub にリポジトリを作ろう まずは枠だけ作っておきましょう。中身は後続のステップで入れます。\nStep2. GitHub でアクセストークンを作ろう GitHub \u0026gt; Setting \u0026gt; Developer settings \u0026gt; Personal access tokens \u0026gt; Generate new tokens から、指定の通り「repo」と「admin:repo_hook」のスコープを選択したトークンを作ります。\n作成に成功したら次のようにトークンが表示されます。スタック作成時に利用しますのでコピーしておきましょう。なお、トークン情報が漏れると大変なことになりますので絶対に漏らさないように注意しましょう。\nStep3. クイックスタートを起動します ではクイックスタートを活用して環境を構築していきましょう。まずはクイックスタートサイトの「クイックスタートを起動します」をクリックします。\nリンクをクリックすると自動的にスタックの作成画面へ遷移します。作成先のリージョンがデフォルトだとオレゴンなので適宜変更して「次へ」をクリックします。\n次にパラメータを入力していきます。\n必要に応じてタグなどの設定を実施し、確認画面で入力ミスがないかを確認したら「スタックの作成」を実行します。\nあとはステータスが「CREATE_COMPLETE」になるまで待つだけです。\n以上で、CI/CD パイプラインに必要な AWS リソースの構築まで完了しました。\nStep4. CI/CD パイプラインの動作確認 ではソースコードを GitHub に登録して CI/CD パイプラインが動作することを確認していきましょう。登録するソースコードは先ほど作成したこちらのファイルです。\n$ tree -a . ├── my-vpc.yaml └── .taskcat.yml  0 directories, 2 files スタック作成時に指定した GitHub の監視対象ブランチ(例では develop ブランチ)へソースコードをプッシュします。\n$ git init . $ git remote add origin \u0026lt;GitHub上のリポジトリ\u0026gt; $ git pull origin main $ git branch develop $ git checkout develop $ git add . $ git commit -m \u0026#34;initial commit\u0026#34; $ git push origin develop さてここからは再び AWS マネジメントコンソールへ移ります。先ほどのプッシュをトリガーにパイプラインが起動していることを確認するため、まずは CodePipeline 画面へ行きましょう。次のように CI/CD パイプラインが動作していることが確認できるかと思います。\nここからドリルダウンでビルドログなどを見ることができます。Build ステージの CodeBuild アクションボックスの「詳細」ボタンをクリックし、ビルドログを確認しましょう。出力例では指定したリージョンでのテストに成功していることがわかります。\n以上、TaskCat によるテストを組み込んだ CI/CD パイプラインの作成のご紹介でした。\n終わりに TaskCat はいかがだったでしょうか？\n今回はクイックスタートを使って環境を構築しましたが、既にパイプラインを構築されている方はリポジトリに .taskcat.yml を追加して、pip install tackcat と taskcat test run をステップに追加するだけで簡単に CFn テンプレートの自動テストを組み込むことができます。気になった方はぜひ触ってみていただければと思います。\n以上、AWS CloudFormation テンプレートの自動テストを実現する「TaskCat」のご紹介でした。\n  AWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 GitHub は、GitHub Inc. の商標または登録商標です。 Linux は、Linus Torvalds 氏 の日本およびその他の国における登録商標または商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。  ","permalink":"https://chacco38.github.io/posts/2021/10/aws-taskcat/","summary":"はじめに みなさん、こんにちは。今回は TaskCat というオープンソースを利用した AWS CloudFormation (CFn) テンプレートの自動テストについてのお話です。\nCFn テンプレートを扱っていると構文エラーチェックはパスしたものの、いざ動かしてみたらスタックの作成でエラーになってしまうといった経験をすることがあるかと思います。TaskCat は多くの方にとってあまり馴染みのないツールだと思いますが、実際に使ってみるととても手軽に CFn テンプレートの自動テストをすることができます。\n今回は Linux 上に開発環境を作るところからはじめて、簡素なサンプルを用いたテストの実行、テスト自動化を組み込んだシンプルな CI/CD パイプラインの構築まで紹介していきたいと思います。これから CFn テンプレート開発されている方で自動テストをやりたいと考えている方は参考にしてみてはいかがでしょうか。\nTaskCat とは TaskCat とは、AWS CloudFormation (CFn) テンプレートの自動テストを行う Python 製のテストツールです。\nこのツールを利用することで、指定した各リージョンに CFn テンプレートから環境を一時的にデプロイ、各リージョンでのデプロイ可否結果のレポート生成、テストで一時的に作成した環境を削除、といった一連の流れを自動化することができます。\nなお、AWS TackCat はローカルでテストを実行する際に Docker が必要となるため、Docker をサポートしていない AWS CloudShell では利用することができないのでご注意ください。\nhttps://github.com/aws-quickstart/taskcat\nTaskCat を使ってみよう 「はじめに」で既に述べたとおり、今回は Linux 上に開発環境を作るところからはじめて、簡素なサンプルを用いたテストの実行、テスト自動化を組み込んだシンプルな CI/CD パイプラインの構築まで紹介していきたいと思います。\nまずは開発環境の設定から それでは Linux 上に開発環境を作っていきたいと思います。まず TaskCat をインストールする事前準備として Python の仮想環境を作成していきましょう。なお、今回の例で使用している Linux ディストリビューションは Amazon Linux 2 です。\n$ sudo yum install -y python3 $ python3 --version Python 3.","title":"TaskCatを使ってCloudFormationテンプレートの自動テストをしよう"},{"content":"はじめに みなさん、こんにちは。今回は「AWS Chalice」を活用したサーバレスアプリケーション開発についてのお話です。AWS Summit Online Japan 2021 の日立製作所の講演で軽く触れたこともあり、どこかで紹介したいと思っておりました。\nさて、AWS Chalice は多くの方にとってあまり馴染みのないツールだと思いますが、実際に使ってみるととても手軽に Amazon API Gateway と AWS Lambda を使用するサーバレスアプリケーションを作成してデプロイすることができます。もちろん、どんなツールにも得手不得手はあるので「すべてのプロジェクトで AWS Chalice の活用が最適か？」と言われればもちろん「No！」なのですが、ちょっと試しに動く Web API をサッと手軽に作りたい、といったケースにはとても良いソリューションだと思います。\n今回は AWS CloudShell 上に開発環境を作るところからはじめて、簡素なサンプルを用いた一連の開発の流れ、テスト自動化を組み込んだシンプルな CI/CD パイプラインの構築まで紹介していきたいと思います。これから実際に動くサーバレスアプリケーションを手軽に作りたいと思われている方は参考にしてみてはいかがでしょうか。\n改めて AWS Chalice とは AWS Chalice とは、Amazon AWS Gateway や AWS Lambda を用いたサーバレスアプリケーションを、お手軽に開発できるようにする Python 製のサーバレスアプリケーションフレームワークです。具体的には、Web API をシンプルで直感的なコードで実装できるようにする機能や、作成したコードからアプリケーションの作成やデプロイを実行するコマンドラインインタフェース(CLI)といった開発者にやさしい機能を提供してくれます。\n普段から Python に触れている方であれば似たような機能として Flask や Bottle をイメージされるかと思いますが、これらにサーバレス環境へデプロイする機能が追加で付与されたものが AWS Chalice、とイメージしていただくと良いのかなと思います。\nhttps://github.com/aws/chalice\nではさっそく AWS Chalice を使ってみよう 「はじめに」で既に述べたとおり、今回は AWS CloudShell 上に開発環境を作るところからはじめて、簡素なサンプルを用いた一連の開発の流れ、テスト自動化を組み込んだシンプルな CI/CD パイプラインの構築まで紹介していきたいと思います。\nまずは開発環境の構築から 今回は AWS CloudShell 上に開発環境を作っていきたいと思います。 それではまず AWS Chalice をインストールする事前準備として Python の仮想環境を作成していきましょう。\n$ sudo yum install -y python3 $ python3 --version Python 3.7.10 $ python3 -m venv venv37 $ . venv37/bin/activate 次に、AWS Chalice をインストールします。\n$ python3 -m pip install chalice $ chalice --version chalice 1.26.0, python 3.7.10, linux 4.14.243-185.433.amzn2.x86_64 最後に、AWS CLI の設定をして開発環境の構築は完了です。\n$ aws configure 簡単なアプリケーションを動かしてみよう ここでは AWS Chalice を使ったアプリケーションの作成からデプロイまでの流れを、次のような簡単なサンプルを用いて紹介していきたいと思います。\nStep1. 新規プロジェクトの作成 まずは chalice new-project コマンドを実行して新しいプロジェクトを作成します。出力例のようにプロジェクトを新しく作るとデフォルトでサンプルプログラムも生成されます。\n$ chalice new-project \u0026lt;任意のプロジェクト名\u0026gt; $ cd \u0026lt;任意のプロジェクト名\u0026gt; $ tree -a . ├── app.py # APIの実装を行うファイル ├── .chalice │ └── config.json # Chaliceの設定を行うファイル ├── .gitignore └── requirements.txt # 利用するライブラリの定義を行うファイル  1 directory, 4 file Step2. ソースコードの編集 今回はデフォルトで作られたソースコードにちょっとだけ手を加えました。修正後のファイルの中身は次の通りです。実際の例を見ていただけるとわかる通り、AWS Chalice を用いた実装は Python を普段使わないという方でも直感的でわかりやすい構文になっているのではないでしょうか。\nfrom chalice import Chalice  app = Chalice(app_name=\u0026#39;\u0026lt;任意のプロジェクト名\u0026gt;\u0026#39;) app.log.setLevel(logging.INFO)  @app.route(\u0026#39;/hello\u0026#39;) def hello():  app.log.debug(\u0026#34;Invoking from function hello\u0026#34;)  return {\u0026#39;hello\u0026#39;: \u0026#39;world\u0026#39;}  @app.route(\u0026#39;/hello\u0026#39;, methods=[\u0026#39;POST\u0026#39;], content_types=[\u0026#39;application/json\u0026#39;], cors=True) def hello_post():  app.log.debug(\u0026#34;Invoking from function hello_post\u0026#34;)  request = app.current_request  return {\u0026#39;result\u0026#39;: request.json_body[\u0026#39;payload\u0026#39;]} chalice {  \u0026#34;version\u0026#34;: \u0026#34;2.0\u0026#34;,  \u0026#34;app_name\u0026#34;: \u0026#34;\u0026lt;任意のプロジェクト名\u0026gt;\u0026#34;,  \u0026#34;stages\u0026#34;: {  \u0026#34;dev\u0026#34;: {  \u0026#34;api_gateway_stage\u0026#34;: \u0026#34;api\u0026#34;  }  } } Step3. ローカル環境で動作確認 AWS 上へデプロイする前にローカル環境で軽く動作をしたい場合は、chalice local コマンドを実行します。 実行例のように期待通りのレスポンスが返ってくるようであれば、いよいよ AWS 上へデプロイをしていきます。\n$ chalice local Serving on http://127.0.0.1:8000 $ curl http://127.0.0.1:8000/hello {\u0026#34;hello\u0026#34;:\u0026#34;world\u0026#34;} $ curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;payload\u0026#34;:\u0026#34;hello, world\u0026#34;}\u0026#39; http://127.0.0.1:8000/hello {\u0026#34;result\u0026#34;:\u0026#34;hello, world\u0026#34;} Step4. AWS 上へデプロイ AWS 上へアプリケーションのデプロイをする場合は、chalice deploy コマンドを実行します。実行例ではメッセージから新たに Lambda 関数、API Gateway と IAM ロールが作られていることがわかります。\n$ chalice deploy --stage dev Creating deployment package. Creating IAM role: \u0026lt;任意のプロジェクト名\u0026gt;-dev Creating lambda function: \u0026lt;任意のプロジェクト名\u0026gt;-dev Creating Rest API Resources deployed:  - Lambda ARN: arn:aws:lambda:ap-northeast-1:\u0026lt;AWSアカウント名\u0026gt;:function:\u0026lt;任意のプロジェクト名\u0026gt;-dev  - Rest API URL: https://\u0026lt;文字列\u0026gt;.execute-api.ap-northeast-1.amazonaws.com/api/ デプロイ完了後は期待するレスポンスが返ってくるか確認しましょう。\n$ curl https://\u0026lt;文字列\u0026gt;.execute-api.ap-northeast-1.amazonaws.com/api/hello {\u0026#34;hello\u0026#34;:\u0026#34;world\u0026#34;} $ curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;payload\u0026#34;:\u0026#34;hello, world\u0026#34;}\u0026#39; https://\u0026lt;文字列\u0026gt;.execute-api.ap-northeast-1.amazonaws.com/api/hello {\u0026#34;result\u0026#34;:\u0026#34;hello, world\u0026#34;} 以上、AWS Chalice を使ったアプリケーションの作成からデプロイまでの一連の流れでした。\nStepEX. デプロイしたアプリケーションの削除 不要になったアプリケーションは chalice delete コマンドを使って削除しておきましょう。\n$ chalice delete Deleting Rest API: \u0026lt;文字列\u0026gt; Deleting function: arn:aws:lambda:ap-northeast-1:\u0026lt;AWSアカウント名\u0026gt;:function:\u0026lt;任意のプロジェクト名\u0026gt;-dev Deleting IAM role: \u0026lt;任意のプロジェクト名\u0026gt;-dev より実践的な開発を行うにあたって ユニットテストを自動化しましょう 近頃はユニットテストの自動化は一般的に行われているかと思います。AWS Chalice でも Python の一般的なテストツール Pytest を使ってユニットテストを実装することができます。例として、今回は次のサンプルに対するテストコードを実装してみます。\nfrom chalice import Chalice app = Chalice(app_name=\u0026#39;\u0026lt;任意のプロジェクト名\u0026gt;\u0026#39;) app.log.setLevel(logging.INFO) @app.route(\u0026#39;/hello\u0026#39;) def hello(): app.log.debug(\u0026#34;Invoking from function hello\u0026#34;) return {\u0026#39;hello\u0026#39;: \u0026#39;world\u0026#39;} @app.route(\u0026#39;/hello\u0026#39;, methods=[\u0026#39;POST\u0026#39;], content_types=[\u0026#39;application/json\u0026#39;], cors=True) def hello_post(): app.log.debug(\u0026#34;Invoking from function hello_post\u0026#34;) request = app.current_request return {\u0026#39;result\u0026#39;: request.json_body[\u0026#39;payload\u0026#39;]} Step1. テストコードの実装 では、まず必要なファイルを用意していきます。中身は後で入れるとしてここでは空ファイルだけ作ります。\n$ touch test_requirements.txt # テストプログラムの依存ライブラリ定義 $ mkdir tests $ touch tests/__init__.py $ touch tests/conftest.py # テストプログラム間で共通の処理を実装 $ touch tests/test_app.py # テストプログラムを実装 次に各ファイルを編集していきます。今回は HTTP レスポンスのステータスコードとボディの中身を確認するだけの簡単なテストプログラムを用意しました。\npytest-chalice pytest-cov (EOF) import pytest from app import app as chalice_app @pytest.fixture def app(): return chalice_app from http import HTTPStatus import json def test_hello_get(client): response = client.get(\u0026#39;/hello\u0026#39;) assert response.status_code == HTTPStatus.OK assert response.json == {\u0026#39;hello\u0026#39;: \u0026#39;world\u0026#39;} def test_hello_post(client): headers = {\u0026#39;Content-type\u0026#39;:\u0026#39;application/json\u0026#39;} payload = {\u0026#39;payload\u0026#39;:\u0026#39;hello, world\u0026#39;} response = client.post(\u0026#39;/hello\u0026#39;, headers=headers, body=json.dumps(payload)) assert response.status_code == HTTPStatus.OK assert response.json == {\u0026#39;result\u0026#39;: \u0026#39;hello, world\u0026#39;} def test_hello_put(client): response = client.put(\u0026#39;/hello\u0026#39;) assert response.status_code == HTTPStatus.METHOD_NOT_ALLOWED assert response.json == {\u0026#34;Code\u0026#34;:\u0026#34;MethodNotAllowedError\u0026#34;,\u0026#34;Message\u0026#34;:\u0026#34;Unsupported method: PUT\u0026#34;} Step2. ユニットテストの実行 ではテストプログラムが用意できたので pytest コマンドを使ってユニットテストを実行しましょう。実行例では、出力メッセージからユニットテスト 3 件が実行され、3 件とも成功 (PASSED) して、C1 カバレッジが 100% 網羅されていることがわかります。\n$ python3 -m pip install -r test_requirements.txt $ pytest -v --cov --cov-branch =========================== test session starts ============================ platform linux -- Python 3.7.10, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /home/cloudshell-user/venv37/bin/python3 cachedir: .pytest_cache rootdir: /home/cloudshell-user/\u0026lt;任意のプロジェクト名\u0026gt; plugins: cov-3.0.0, chalice-0.0.5 collected 3 items tests/test_app.py::test_hello_get PASSED [ 33%] tests/test_app.py::test_hello_post PASSED [ 66%] tests/test_app.py::test_hello_put PASSED [100%] ---------- coverage: platform linux, python 3.7.10-final-0 ----------- Name Stmts Miss Branch BrPart Cover ----------------------------------------------------- app.py 11 0 0 0 100% tests/__init__.py 0 0 0 0 100% tests/conftest.py 4 0 0 0 100% tests/test_app.py 16 0 0 0 100% ----------------------------------------------------- TOTAL 31 0 0 0 100% ============================ 3 passed in 0.05s ============================= 以上、ちょっとしたユニットテストの自動化のご紹介でした。\nCI/CD パイプラインを構築しよう 次はもう一歩進んで、ソースコードの更新をトリガーに自動でテストを実行してデプロイまで実施する CI/CD パイプラインを構築し、実際に動かすところまで行ってみたいと思います。\nStep1. CI/CD パイプラインの作成 AWS Chalice には CI/CD パイプライン用の CloudFormation(CFn) テンプレートを自動生成してくれるとても便利な chalice generate-pipeline コマンドがあります。今回の例では、先ほど作ったユニットテストもパイプラインの中で実行するようにしますので、buildspec.yml を分離する -b オプションも付与して実行します。\n$ chalice generate-pipeline -b buildspec.yml \u0026lt;テンプレートファイル名\u0026gt; 作成された CFn テンプレートは中身が大きいのでここには貼り付けませんが、おおよそ次のような AWS リソース作成が定義されています。\n AWS CodeCommit リポジトリ AWS CodeBuild プロジェクト AWS CodePipeline パイプライン Amazon Simple Storage Service(S3) バケット AWS IAM ロールおよびポリシー  なお、今回はユニットテストの結果をレポート出力できるように作成されたテンプレートファイルを編集して CodeBuild に割り当てる権限を追加しました。\n\u0026#34;CodeBuildPolicy\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;AWS::IAM::Policy\u0026#34;, \u0026#34;Properties\u0026#34;: { \u0026#34;PolicyName\u0026#34;: \u0026#34;CodeBuildPolicy\u0026#34;, \u0026#34;PolicyDocument\u0026#34;: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { + \u0026#34;Action\u0026#34;: [ + \u0026#34;codebuild:CreateReportGroup\u0026#34;, + \u0026#34;codebuild:CreateReport\u0026#34;, + \u0026#34;codebuild:UpdateReport\u0026#34;, + \u0026#34;codebuild:BatchPutTestCases\u0026#34;, + \u0026#34;codebuild:BatchPutCodeCoverages\u0026#34; + ], + \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, + \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; + }, { \u0026#34;Action\u0026#34;: [ では CFn テンプレートからリソースをデプロイしましょう。実行例のようにスタック作成の成功と出力されたら完了です。\n$ aws cloudformation deploy --stack-name \u0026lt;スタック名\u0026gt; --template-file \u0026lt;テンプレートファイル名\u0026gt; --capabilities CAPABILITY_IAM : Successfully created/updated stack - \u0026lt;スタック名\u0026gt; Step2. パイプラインジョブ定義の編集 次に chalice generate-pipeline で生成されたパイプラインジョブの定義を編集して、ユニットテストも自動で実行するようにしていきましょう。編集前のファイル内容はこちらです。\nartifacts: files: - transformed.yaml type: zip phases: install: commands: - sudo pip install --upgrade awscli - aws --version - sudo pip install \u0026#39;chalice\u0026gt;=1.26.0,\u0026lt;1.27.0\u0026#39; - sudo pip install -r requirements.txt - chalice package /tmp/packaged - aws cloudformation package --template-file /tmp/packaged/sam.json --s3-bucket ${APP_S3_BUCKET} --output-template-file transformed.yaml version: \u0026#39;0.1\u0026#39; 編集後のファイルは次のようにしてみました。記載順序の入れ替えやビルドフェーズの分割も併せて実施していてわかりにくくなっていますが、要約すると test_requirements.txt の読み込みと pytest コマンドの実行をステップとして追加しています。\nversion: 0.2 phases: install: commands: - python --version pre_build: commands: - sudo pip install --upgrade awscli pip - aws --version - sudo pip install -r requirements.txt -r test_requirements.txt build: commands: - pytest -v -junit-xml=test-result.xml --cov --cov-branch --cov-report=xml --cov-report=term post_build: commands: - chalice package /tmp/packaged - aws cloudformation package --template-file /tmp/packaged/sam.json --s3-bucket ${APP_S3_BUCKET} --output-template-file transformed.yaml reports: pytest_reports: files: - test-result.xml file-format: JUNITXML cobertura_reports: files: - coverage.xml file-format: COBERTURAXML artifacts: files: - transformed.yaml Step3. CI/CD パイプラインの動作確認 環境もパイプラインジョブの定義も整いましたので、あとは作成された CodeCommit リポジトリへソースコードを登録して、それをトリガーに自動でパイプラインが実行されるところまで見ていきましょう。\nまずはローカルに git リポジトリを作成して、ソースコードのコミットまでしていきます。今回の .gitignore ファイルは GitHub さんが公開する Python 向けテンプレートを使ってます。\n$ git init . $ curl https://raw.githubusercontent.com/github/gitignore/master/Python.gitignore \u0026gt; .gitignore $ git add . $ git commit -m \u0026#34;Initial commit\u0026#34; 次に、ソースコードを格納する AWS CodeCommit リポジトリの URL を確認します。\n$ aws cloudformation describe-stacks --stack-name \u0026lt;スタック名\u0026gt; --query \u0026#39;Stacks[0].Outputs\u0026#39; : - OutputKey: SourceRepoURL  OutputValue: https://git-codecommit.ap-northeast-1.amazonaws.com/v1/repos/\u0026lt;プロジェクト名\u0026gt; : 先ほど調べたリポジトリ情報を元に、リモートリポジトリを追加します。\n$ git remote add codecommit https://git-codecommit.ap-northeast-1.amazonaws.com/v1/repos/\u0026lt;プロジェクト名\u0026gt; CodeCommit へ Push できるように認証情報ヘルパーを設定します。\n$ git config --global credential.helper \u0026#39;!aws codecommit credential-helper $@\u0026#39; $ git config --global credential.UseHttpPath true ではソースコードをリモートレポジトリへ push してみましょう。\n$ git push codecommit master さてここからは AWS マネジメントコンソールに移ります。先ほどの git push をトリガーにパイプラインが起動していることを確認するためまずは CodePipeline 画面へ。次のように CI/CD パイプラインが動作していることが確認できるかと思います。\nでは次に、ビルド処理に追加したユニットテストが動いているかを確認しましょう。Build ステージの CodeBuild アクションボックスの「成功しました」メッセージのすぐ下にある「詳細」ボタンをクリックして CodeBuild 画面へ進みましょう。次のようにビルドログからユニットテストが実行されていることが確認できるかと思います。\nまた、テストとカバレッジのレポートについてはレポートグループの方にも登録されていることが確認できるかと思います。\nでは CodePipeline 画面へ戻りましょう。Beta ステージで実施しているアプリケーションのデプロイにも成功していることがわかりますね。ExcecuteChangeSet アクションボックスの「詳細」ボタンをクリックし、CloudFormation 画面へ進みましょう。\nCloudFormation 画面では出力の一覧から EndpointURL 値を確認しましょう。\n最後に、期待するレスポンスが返ってくるか生成された EndpointURL に向けて HTTP リクエストを発行して確認しましょう。実行例では期待するレスポンスが返ってきてますね。\n$ curl https://\u0026lt;文字列\u0026gt;.execute-api.ap-northeast-1.amazonaws.com/api/hello {\u0026#34;hello\u0026#34;:\u0026#34;world\u0026#34;} $ curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;payload\u0026#34;:\u0026#34;hello, world\u0026#34;}\u0026#39; https://\u0026lt;文字列\u0026gt;.execute-api.ap-northeast-1.amazonaws.com/api/hello {\u0026#34;result\u0026#34;:\u0026#34;hello, world\u0026#34;} 本来はもっとパイプラインに条件分岐を持たせたりすると思いますが、ひとまずこれでソースコードのプッシュを契機にテストを走らせてからデプロイを行う、という最低限の流れを自動化することができました。\n以上、ちょっとした CI/CD パイプラインの作成でした。\nちゃんとしたアプリケーションを開発するには 公式ドキュメントを活用しよう ちゃんとしたアプリケーションにするにはもちろん色々と機能を実装しないといけませんよね。こんなときにいつも私がお世話になっているのが公式ドキュメントです。トピックごとに簡単なサンプルを交えた実装方法について記載がありますのでとても参考になるかと思います。\nhttps://aws.github.io/chalice/main.html\nサービス別資料を参考にしよう もちろん AWS さんのサービス別資料もとても参考になります。こちらも目を通していただけると良いかと思います。\n余談ですが 本記事執筆のきっかけとなった AWS Summit Online Japan 2021 で行った日立の講演についてもご紹介させてください。\nこの講演では、IoT やサーバレス技術を活用した異なるタイプ(データ分析系とアプリケーション開発系)の事例を 1 件ずつお話させていただきました。AWS Chalice については、2 つ目のアプリケーション開発系の事例にて本当に軽くですが触れておりますのでよろしければご参照いただけると幸いです。\nこの講演を通じて「ほほう、日立ってこんなこともしてたんだ」と多少なりとも良いイメージを抱いてもらえるきっかけになってくれると嬉しいなと思ってます σ(,,´∀ ｀,,)\n終わりに AWS Chalice はいかがだったでしょうか？\nAWS には、AWS SAM (Serverless Application Model)といった別のサーバレスアプリケーション開発用フレームワークがありますが、こちらと比較すると機能がかなり限定されることもあって、今回ご紹介した AWS Chalice は多くの方にとって馴染みもなければ、なかなかに触れる機会も少ないのかもしれません。\nただ、実際に触ってみていただけばわかる通り、非常に簡単・手軽にサーバレスアプリケーションを作ることができますので、迅速性を求められる PoC(概念実証)や PoV(価値実証)といった場面で特に有用なのでは、と考えています。気になった方はぜひ触ってみて、そのお手軽さを実際に体験していただければと思います。\n以上、AWS 上でサーバレスアプリケーションを手軽に開発できるようにする「AWS Chalice」のご紹介でした。\n  AWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。  ","permalink":"https://chacco38.github.io/posts/2021/10/aws-chalice/","summary":"はじめに みなさん、こんにちは。今回は「AWS Chalice」を活用したサーバレスアプリケーション開発についてのお話です。AWS Summit Online Japan 2021 の日立製作所の講演で軽く触れたこともあり、どこかで紹介したいと思っておりました。\nさて、AWS Chalice は多くの方にとってあまり馴染みのないツールだと思いますが、実際に使ってみるととても手軽に Amazon API Gateway と AWS Lambda を使用するサーバレスアプリケーションを作成してデプロイすることができます。もちろん、どんなツールにも得手不得手はあるので「すべてのプロジェクトで AWS Chalice の活用が最適か？」と言われればもちろん「No！」なのですが、ちょっと試しに動く Web API をサッと手軽に作りたい、といったケースにはとても良いソリューションだと思います。\n今回は AWS CloudShell 上に開発環境を作るところからはじめて、簡素なサンプルを用いた一連の開発の流れ、テスト自動化を組み込んだシンプルな CI/CD パイプラインの構築まで紹介していきたいと思います。これから実際に動くサーバレスアプリケーションを手軽に作りたいと思われている方は参考にしてみてはいかがでしょうか。\n改めて AWS Chalice とは AWS Chalice とは、Amazon AWS Gateway や AWS Lambda を用いたサーバレスアプリケーションを、お手軽に開発できるようにする Python 製のサーバレスアプリケーションフレームワークです。具体的には、Web API をシンプルで直感的なコードで実装できるようにする機能や、作成したコードからアプリケーションの作成やデプロイを実行するコマンドラインインタフェース(CLI)といった開発者にやさしい機能を提供してくれます。\n普段から Python に触れている方であれば似たような機能として Flask や Bottle をイメージされるかと思いますが、これらにサーバレス環境へデプロイする機能が追加で付与されたものが AWS Chalice、とイメージしていただくと良いのかなと思います。\nhttps://github.com/aws/chalice\nではさっそく AWS Chalice を使ってみよう 「はじめに」で既に述べたとおり、今回は AWS CloudShell 上に開発環境を作るところからはじめて、簡素なサンプルを用いた一連の開発の流れ、テスト自動化を組み込んだシンプルな CI/CD パイプラインの構築まで紹介していきたいと思います。\nまずは開発環境の構築から 今回は AWS CloudShell 上に開発環境を作っていきたいと思います。 それではまず AWS Chalice をインストールする事前準備として Python の仮想環境を作成していきましょう。","title":"AWS Chaliceを使ってサーバレスアプリケーションを開発しよう"}]