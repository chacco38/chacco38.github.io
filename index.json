[{"content":"","date":"March 29, 2023","permalink":"/tags/certificate-manager/","section":"タグ一覧","summary":"","title":"Certificate Manager"},{"content":" Google Cloud に関する技術記事やコラムです。 ","date":"March 29, 2023","permalink":"/tags/google-cloud/","section":"タグ一覧","summary":" Google Cloud に関する技術記事やコラムです。 ","title":"Google Cloud"},{"content":"みなさん、こんにちは。今回はHTTPSロードバランサなどに割り当てたGoogleマネージドSSL証明書のルート証明書を入手する方法のご紹介をしていきたいと思います。\nルート証明書の入手方法 # GoogleマネージドSSL証明書を含めGoogle Cloudサービスで利用するルート証明書は、「Google Trust Services」というルート認証局で管理されており、こちらのリポジトリサイトからルート証明書および中間証明書をダウンロードすることが可能です。\n終わりに # ということで、今回はHTTPSロードバランサなどに割り当てたGoogleマネージドSSL証明書のルート証明書を入手する方法のご紹介でした。何かしらの理由でクライアント端末へ個別に適用したいといったケースで参考にしていただければ幸いです。\nGoogle Cloud は、Google LLC の商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"March 29, 2023","permalink":"/posts/2023/03/gcp-certificate-manager-root-ca/","section":"記事一覧","summary":"みなさん、こんにちは。今回はHTTPSロードバランサなどに割り当てたGoogleマネージドSSL証明書のルート証明書を入手する方法のご紹介をしていきたいと思います。","title":"GoogleマネージドSSL証明書のルート証明書を入手する方法"},{"content":"","date":"March 29, 2023","permalink":"/","section":"クラウドCoEの何でも屋","summary":"","title":"クラウドCoEの何でも屋"},{"content":" ","date":"March 29, 2023","permalink":"/tags/","section":"タグ一覧","summary":" ","title":"タグ一覧"},{"content":"","date":"March 29, 2023","permalink":"/posts/","section":"記事一覧","summary":"","title":"記事一覧"},{"content":"","date":"January 4, 2023","permalink":"/tags/amazon-ec2/","section":"タグ一覧","summary":"","title":"Amazon EC2"},{"content":"","date":"January 4, 2023","permalink":"/tags/amazon-rds/","section":"タグ一覧","summary":"","title":"Amazon RDS"},{"content":"","date":"January 4, 2023","permalink":"/tags/amazon-rds-for-oracle/","section":"タグ一覧","summary":"","title":"Amazon RDS for Oracle"},{"content":" Amazon Web Services (AWS) に関する技術記事やコラムです。 ","date":"January 4, 2023","permalink":"/tags/aws/","section":"タグ一覧","summary":" Amazon Web Services (AWS) に関する技術記事やコラムです。 ","title":"AWS"},{"content":"","date":"January 4, 2023","permalink":"/tags/aws-cloud9/","section":"タグ一覧","summary":"","title":"AWS Cloud9"},{"content":"みなさん、こんにちは。今回はCloud9入門として、誰もが一度は直面するであろうCloud9環境でaws configure実行した際に次のようなクレデンシャルのアップデートができなかった旨のメッセージが出力され、そこでForce updateを選択したにもかかわらず設定が保存されないぞ…^^; といった事象の原因と対処方法について紹介したいと思います。\nCloud9環境でAWS CLI設定を保存する方法 # そもそもこの事象、Cloud9のデフォルト設定ではAWS managed temporary credentialsという機能が有効になっていて、この機能が~/.aws/credentialsファイルの中身を上書きするのが原因です。そのため、aws configureで設定した内容を保持するにはこちらの機能を無効にする必要があります。\n具体的には、次の画像のようにCloud9の環境設定(Preferences)を開き、AWS Settingsの項目から設定できます。ということで、これにてAWS CLIの設定を保存することが可能になりました。めでたしめでたし。\n終わりに # いまさらの情報でしたがいかがだったでしょうか。もちろん AWS managed temporary credentials の権限で十分な方はとくに気にする必要はありませんが、もし困った際は参考にしていただければと思います。\n以上、Cloud9でaws configureコマンドの設定が保存されない事象の解決方法でした。\nAWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"January 4, 2023","permalink":"/posts/2023/01/aws-cloud9-disable-managed-temporary-credentials/","section":"記事一覧","summary":"みなさん、こんにちは。今回はCloud9入門として、誰もが一度は直面するであろうCloud9環境でaws configure実行した際に次のようなクレデンシャルのアップデートができなかった旨のメッセージが出力され、そこでForce updateを選択したにもかかわらず設定が保存されないぞ…^^; といった事象の原因と対処方法について紹介したいと思います。","title":"AWS Cloud9でaws configureコマンドの設定が保存されない事象の解決方法"},{"content":"","date":"January 4, 2023","permalink":"/tags/aws-iam/","section":"タグ一覧","summary":"","title":"AWS IAM"},{"content":"","date":"January 4, 2023","permalink":"/tags/security-group/","section":"タグ一覧","summary":"","title":"Security Group"},{"content":"","date":"January 4, 2023","permalink":"/tags/terraform/","section":"タグ一覧","summary":"","title":"Terraform"},{"content":"みなさん、こんにちは。今回はTerraformの入門ということでAmazon EC2のサンプルコードを書いてみましたのでこちらを紹介していきたいと思います。\nなお、サンプルコードを書いた際のTerraformおよびAWSプロバイダーのバージョンは次のとおりです。最新バージョンでは定義方法が異なっている可能性があるため、実際にコードを書く際は最新の「Terraformドキュメント」と「AWSプロバイダードキュメント」を確認しながら開発を進めていただければと思います。\n例）versions.tf\n# Requirements terraform { required_version = \u0026#34;~\u0026gt; 1.3.6\u0026#34; required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.46.0\u0026#34; } } } サンプルコードを書いてみた # 今回は次のような構成のサンプルコードを書いてみました。なお、変数定義部分などの一部省略している点、ならびにステップごとの細かい説明などは省いていますのでご承知おきください。詳細については「AWSプロバイダードキュメント」をご参照ください。\n例1. シングルEC2インスタンスの作成 ちなみに、すべてのサンプルコードに共通してプロバイダー定義は次のようにしています。\n例）providers.tf\n# デフォルトのプロバイダー設定 provider \u0026#34;aws\u0026#34; { region = var.aws_region default_tags { tags = { Owner = \u0026#34;matt\u0026#34; Terraform = \u0026#34;true\u0026#34; } } } 例1. シングルEC2インスタンスの作成 # KMSで管理する暗号鍵にてディスク暗号化を施したEC2インスタンスを実装する際の例です。追加ディスクの有無や個数は、サーバ要件によって大きく異なると思うのでdynamicブロックを使って動的定義にしています。\n例）main.tf\n# Amazon EC2インスタンスの定義 resource \u0026#34;aws_instance\u0026#34; \u0026#34;this\u0026#34; { ami = var.ami instance_type = var.instance_type availability_zone = var.availability_zone subnet_id = data.aws_subnet.this.id vpc_security_group_ids = [aws_security_group.this.id] monitoring = true root_block_device { volume_size = lookup(var.root_block_device, \u0026#34;volume_size\u0026#34;, null) volume_type = lookup(var.root_block_device, \u0026#34;volume_type\u0026#34;, null) iops = lookup(var.root_block_device, \u0026#34;iops\u0026#34;, null) throughput = lookup(var.root_block_device, \u0026#34;throughput\u0026#34;, null) encrypted = true kms_key_id = data.aws_kms_key.this.arn } dynamic \u0026#34;ebs_block_device\u0026#34; { for_each = var.ebs_block_devices content { device_name = lookup(ebs_block_device.value, \u0026#34;device_name\u0026#34;, null) volume_size = lookup(ebs_block_device.value, \u0026#34;volume_size\u0026#34;, null) volume_type = lookup(ebs_block_device.value, \u0026#34;volume_type\u0026#34;, null) iops = lookup(ebs_block_device.value, \u0026#34;iops\u0026#34;, null) throughput = lookup(ebs_block_device.value, \u0026#34;throughput\u0026#34;, null) snapshot_id = lookup(ebs_block_device.value, \u0026#34;snapshot_id\u0026#34;, null) encrypted = true kms_key_id = data.aws_kms_key.this.arn } } tags = { Name = var.instance_name } } # 空のセキュリティグループの作成 resource \u0026#34;aws_security_group\u0026#34; \u0026#34;this\u0026#34; { name = var.security_group_name vpc_id = data.aws_vpc.this.id tags = { Name = var.security_group_name } lifecycle { create_before_destroy = true } } 例）data.tf\n# VPC情報の取得 data \u0026#34;aws_vpc\u0026#34; \u0026#34;this\u0026#34; { cidr_block = var.vpc_cidr_block filter { name = \u0026#34;tag:Name\u0026#34; values = [var.vpc_name] } } # サブネット情報の取得 data \u0026#34;aws_subnet\u0026#34; \u0026#34;private\u0026#34; { vpc_id = data.aws_vpc.default.id availability_zone = var.availability_zone cidr_block = var.private_subnet_cidr_block filter { name = \u0026#34;tag:Name\u0026#34; values = [var.private_subnet_name] } } # 暗号鍵情報の取得 data \u0026#34;aws_kms_key\u0026#34; \u0026#34;this\u0026#34; { key_id = var.kms_key_alias_name } 例）outputs.tf\noutput \u0026#34;instance_id\u0026#34; { description = \u0026#34;The ID of the instance\u0026#34; value = try(aws_instance.this.id, \u0026#34;\u0026#34;) } output \u0026#34;instance_arn\u0026#34; { description = \u0026#34;The ARN of the instance\u0026#34; value = try(aws_instance.this.arn, \u0026#34;\u0026#34;) } output \u0026#34;instance_private_ip\u0026#34; { description = \u0026#34;The private IP address of the instance\u0026#34; value = try(aws_instance.this.private_ip, \u0026#34;\u0026#34;) } output \u0026#34;security_group_id\u0026#34; { description = \u0026#34;The ID of the security group\u0026#34; value = try(aws_security_group.this.id, \u0026#34;\u0026#34;) } 終わりに # 今回はTerraformの入門ということで、Amazon EC2のサンプルコードをいくつかご紹介してきましたがいかがだったでしょうか。こんな記事でも誰かの役に立っていただけるのであれば幸いです。\nなお、今回ご紹介したコードはあくまでサンプルであり、動作を保証するものではございません。そのまま使用したことによって発生したトラブルなどについては一切責任を負うことはできませんのでご注意ください。\nAWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 Terraform は、HashiCorp, Inc. の米国およびその他の国における商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"January 4, 2023","permalink":"/posts/2023/01/aws-ec2-sample-terraform-code/","section":"記事一覧","summary":"みなさん、こんにちは。今回はTerraformの入門ということでAmazon EC2のサンプルコードを書いてみましたのでこちらを紹介していきたいと思います。","title":"TerraformでAmazon EC2を構築するサンプルコードを書いてみた"},{"content":"みなさん、こんにちは。今回はTerraformの入門ということでAmazon RDS for Oracleのサンプルコードを書いてみましたのでこちらを紹介していきたいと思います。\nなお、サンプルコードを書いた際のTerraformおよびAWSプロバイダーのバージョンは次のとおりです。最新バージョンでは定義方法が異なっている可能性があるため、実際にコードを書く際は最新の「Terraformドキュメント」と「AWSプロバイダードキュメント」を確認しながら開発を進めていただければと思います。\n例）versions.tf\n# Requirements terraform { required_version = \u0026#34;~\u0026gt; 1.3.6\u0026#34; required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.55.0\u0026#34; } } } サンプルコードを書いてみた # 今回は次のような構成のサンプルコードを書いてみました。なお、変数定義部分などの一部省略している点、ならびにステップごとの細かい説明などは省いていますのでご承知おきください。詳細については「AWSプロバイダードキュメント」をご参照ください。\n例1. RDSサブネットグループの作成 例2. RDSパラメータグループの作成 例3. RDSオプショングループの作成 例4. RDSインスタンスの作成 ちなみに、すべてのサンプルコードに共通してプロバイダー定義は次のようにしています。\n例）providers.tf\n# デフォルトのプロバイダー設定 provider \u0026#34;aws\u0026#34; { region = var.aws_region default_tags { tags = { Owner = \u0026#34;matt\u0026#34; Terraform = \u0026#34;true\u0026#34; } } } 例1. RDSサブネットグループの作成 # RDSサブネットグループの実装例で、こちらについては特筆すべき点はとくにないと思います。\n例）main.tf\n# RDSサブネットグループの定義 resource \u0026#34;aws_db_subnet_group\u0026#34; \u0026#34;this\u0026#34; { name = var.rds_subnet_group_name subnet_ids = [for x in data.aws_subnet.private : x.id] tags = { Name = var.rds_subnet_group_name } } 例）data.tf\n# VPC情報の取得 data \u0026#34;aws_vpc\u0026#34; \u0026#34;this\u0026#34; { cidr_block = var.vpc_cidr_block } # サブネット情報の取得 data \u0026#34;aws_subnet\u0026#34; \u0026#34;private\u0026#34; { for_each = var.private_subnets vpc_id = data.aws_vpc.default.id availability_zone = each.value.availability_zone cidr_block = each.value.cidr_block } 例2. RDSパラメータグループの作成 # RDSパラメータグループの実装例で、定義するパラメータの個数などは環境によってマチマチだと思いますので、変数側で調整できるようにdynamicブロックを使って動的に定義しています。\n例）main.tf\n# RDSパラメータグループの定義 resource \u0026#34;aws_db_parameter_group\u0026#34; \u0026#34;this\u0026#34; { name = var.rds_parameter_group_name family = var.rds_engine_family dynamic \u0026#34;parameter\u0026#34; { for_each = var.rds_parameters content { name = parameter.value.name value = parameter.value.value apply_method = parameter.value.apply_method } } tags = { Name = var.rds_parameter_group_name } } 例）variables.tf\n# RDSパラメータ一覧 variable \u0026#34;rds_parameters\u0026#34; { type = map(any) default = { audit_trail = { name = \u0026#34;audit_trail\u0026#34; value = \u0026#34;DB,EXTENDED\u0026#34; apply_method = \u0026#34;pending-reboot\u0026#34; } } } 例3. RDSオプショングループの作成 # RDSオプショングループの実装例で、タイムゾーンの設定とS3統合の設定だけしています。S3統合を有効にしているのは、なんとなくDATA PUMPを使って手軽にエクスポート/インポートをしたいケースもあるかな、と思って有効にしています。\n例）main.tf\n# RDSオプショングループの定義 resource \u0026#34;aws_db_option_group\u0026#34; \u0026#34;this\u0026#34; { name = var.rds_option_group_name engine_name = var.rds_engine major_engine_version = var.rds_engine_major_version option { option_name = \u0026#34;Timezone\u0026#34; option_settings { name = \u0026#34;TIME_ZONE\u0026#34; value = \u0026#34;Asia/Tokyo\u0026#34; } } option { option_name = \u0026#34;S3_INTEGRATION\u0026#34; version = \u0026#34;1.0\u0026#34; } tags = { Name = var.rds_option_group_name } } 例4. RDSインスタンスの作成 # RDSインスタンスの実装例で、モニタリング用やS3統合用のIAMロールの定義なども併せて記載しています。\nなお、マスターユーザ/パスワード情報はSecrets Managerで管理というケースが最近は多くなってきていると思いますが、今回の例ではパスワード情報はプロジェクト管理とし、ここではあくまで構築用の初期パスワードという扱いでSSMパラメータストアにSecureStingとして事前に格納しておいた情報を扱うようにしています。\n例）main.tf\n# RDSインスタンスの定義 resource \u0026#34;aws_db_instance\u0026#34; \u0026#34;this\u0026#34; { identifier = var.rds_instance_name instance_class = var.rds_instance_class engine = var.rds_engine engine_version = var.rds_engine_version license_model = \u0026#34;license-included\u0026#34; multi_az = var.rds_malti_az username = data.aws_ssm_parameter.rds_master_username.value password = data.aws_ssm_parameter.rds_master_password.value parameter_group_name = aws_db_parameter_group.this.name option_group_name = aws_db_option_group.this.name # storage storage_type = var.rds_storage_type allocated_storage = var.rds_allocated_storage max_allocated_storage = var.rds_max_allocated_storage storage_encrypted = true kms_key_id = data.aws_kms_key.this.arn # network db_subnet_group_name = aws_db_subnet_group.this.name vpc_security_group_ids = [aws_security_group.this.id] port = var.rds_port # monitoring monitoring_interval = var.rds_monitoring_interval monitoring_role_arn = aws_iam_role.monitoring.arn performance_insights_enabled = true performance_insights_kms_key_id = data.aws_kms_key.this.arn performance_insights_retention_period = var.rds_performance_insights_retention_period # backup backup_retention_period = var.rds_backup_retention_period backup_window = var.rds_backup_window copy_tags_to_snapshot = true delete_automated_backups = true deletion_protection = false skip_final_snapshot = true # maintenance maintenance_window = var.rds_maintenance_windows auto_minor_version_upgrade = false publicly_accessible = false apply_immediately = true lifecycle { ignore_changes = [password] } tags = { Name = var.rds_instance_name } } # 空のセキュリティグループの定義 resource \u0026#34;aws_security_group\u0026#34; \u0026#34;this\u0026#34; { name = var.security_group_name vpc_id = data.aws_vpc.this.id tags = { Name = var.security_group_name } lifecycle { create_before_destroy = true } } # モニタリング用のIAMロール定義 resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;monitoring\u0026#34; { name = var.monitoring_role_name assume_role_policy = jsonencode({ \u0026#34;Version\u0026#34; : \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34; : [ { \u0026#34;Sid\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;Effect\u0026#34; : \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34; : { \u0026#34;Service\u0026#34; : \u0026#34;monitoring.rds.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34; : \u0026#34;sts:AssumeRole\u0026#34; } ] }) tags = { Name = monitoring_role_name } } # モニタリング用のIAMロールへのポリシー追加 resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;this\u0026#34; { policy_arn = \u0026#34;arn:aws:iam::aws:policy/service-role/AmazonRDSEnhancedMonitoringRole\u0026#34; role = aws_iam_role.monitoring.name } # S3統合用のIAMロール定義 resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;s3_integration\u0026#34; { name = var.iam_s3integration_role_name assume_role_policy = jsonencode({ \u0026#34;Version\u0026#34; : \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34; : [ { \u0026#34;Sid\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;Effect\u0026#34; : \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34; : { \u0026#34;Service\u0026#34; : \u0026#34;monitoring.rds.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34; : \u0026#34;sts:AssumeRole\u0026#34; } ] }) tags = { Name = var.iam_s3integration_role_name } } # S3統合用のIAMポリシー定義 resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;s3integration\u0026#34; { name = var.iam_s3integration_policy_name path = \u0026#34;/\u0026#34; policy = jsonencode({ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:PutObjectAcl\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;${data.aws_s3_bucket.s3_integration.arn}\u0026#34;, \u0026#34;${data.aws_s3_bucket.s3_integration.arn}/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:Encrypt\u0026#34;, \u0026#34;kms:ReEncrypt*\u0026#34;, \u0026#34;kms:Decrypt\u0026#34;, \u0026#34;kms:DescribeKey\u0026#34;, \u0026#34;kms:GenerateDataKey\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;${data.aws_kms_key.this.arn}\u0026#34; ] } ] }) tags = { Name = var.iam_s3integration_policy_name } } # S3統合用IAMロールにIAMポリシーを割り当て resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;s3_integration\u0026#34; { policy_arn = aws_iam_policy.s3_integration.arn role = aws_iam_role.s3_integration.name } # RDBインスタンスにS3統合用のIAMロールの割り当て resource \u0026#34;aws_db_instance_role_association\u0026#34; \u0026#34;s3_integration\u0026#34; { db_instance_identifier = aws_db_instance.this.id feature_name = \u0026#34;S3_INTEGRATION\u0026#34; role_arn = aws_iam_role.s3_integration.arn } 例）data.tf\n# VPC情報の取得 data \u0026#34;aws_vpc\u0026#34; \u0026#34;this\u0026#34; { cidr_block = var.vpc_cidr_block } # SSMパラメータ情報の取得（管理ユーザ名） data \u0026#34;aws_ssm_parameter\u0026#34; \u0026#34;rds_master_username\u0026#34; { name = var.ssm_parameters.rds_master_username.name } # SSMパラメータ情報の取得（管理ユーザの初期パスワード） data \u0026#34;aws_ssm_parameter\u0026#34; \u0026#34;rds_master_password\u0026#34; { name = var.ssm_parameters.rds_master_password.name } # 暗号鍵情報の取得 data \u0026#34;aws_kms_key\u0026#34; \u0026#34;this\u0026#34; { key_id = var.kms_key_alias_name } # S3統合用S3バケット情報の取得 data \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;s3_integration\u0026#34; { bucket = var.s3_bucket_name } 例）outputs.tf\noutput \u0026#34;security_group_id\u0026#34; { description = \u0026#34;The ID of the security group\u0026#34; value = try(aws_security_group.this.id, \u0026#34;\u0026#34;) } 終わりに # 今回はTerraformの入門ということで、Amazon RDS for Oracleのサンプルコードをいくつかご紹介してきましたがいかがだったでしょうか。こんな記事でも誰かの役に立っていただけるのであれば幸いです。\nなお、今回ご紹介したコードはあくまでサンプルであり、動作を保証するものではございません。そのまま使用したことによって発生したトラブルなどについては一切責任を負うことはできませんのでご注意ください。\nAWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 Terraform は、HashiCorp, Inc. の米国およびその他の国における商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"January 4, 2023","permalink":"/posts/2023/01/aws-rds-oracle-sample-terraform-code/","section":"記事一覧","summary":"みなさん、こんにちは。今回はTerraformの入門ということでAmazon RDS for Oracleのサンプルコードを書いてみましたのでこちらを紹介していきたいと思います。","title":"TerraformでAmazon RDS for Oracleを構築するサンプルコードを書いてみた"},{"content":"","date":"December 29, 2022","permalink":"/tags/amazon-api-gateway/","section":"タグ一覧","summary":"","title":"Amazon API Gateway"},{"content":"","date":"December 29, 2022","permalink":"/tags/amazon-ecs/","section":"タグ一覧","summary":"","title":"Amazon ECS"},{"content":"","date":"December 29, 2022","permalink":"/tags/amazon-route-53/","section":"タグ一覧","summary":"","title":"Amazon Route 53"},{"content":"","date":"December 29, 2022","permalink":"/tags/aws-fargate/","section":"タグ一覧","summary":"","title":"AWS Fargate"},{"content":"みなさん、こんにちは。今回はTerraformの入門ということでAmazon API Gatewayのサンプルコードを書いてみましたのでこちらを紹介していきたいと思います。\nなお、サンプルコードを書いた際のTerraformおよびAWSプロバイダーのバージョンは次のとおりです。最新バージョンでは定義方法が異なっている可能性があるため、実際にコードを書く際は最新の「Terraformドキュメント」と「AWSプロバイダードキュメント」を確認しながら開発を進めていただければと思います。\n例）versions.tf\n# Requirements terraform { required_version = \u0026#34;~\u0026gt; 1.3.6\u0026#34; required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.46.0\u0026#34; } } } サンプルコードを書いてみた # 今回は次のような構成のサンプルコードを書いてみました。なお、変数定義部分などの一部省略している点、ならびにステップごとの細かい説明などは省いていますのでご承知おきください。詳細については「AWSプロバイダードキュメント」をご参照ください。\n例1. リージョナルREST APIゲートウェイの作成 例2. プライベートREST APIゲートウェイの作成 ちなみに、すべてのサンプルコードに共通してプロバイダー定義は次のようにしています。\n例）providers.tf\n# デフォルトのプロバイダー設定 provider \u0026#34;aws\u0026#34; { region = var.aws_region default_tags { tags = { Owner = \u0026#34;matt\u0026#34; Terraform = \u0026#34;true\u0026#34; } } } 例1. リージョナルREST APIゲートウェイの作成 # Cognitoオーソライザを使って認証を行うリージョナルREST APIゲートウェイの例で、バックエンドのサービスとの接続にはVPCリンクを経由するようにしています。今回はOpenAPI定義もTerraformでデプロイさせていますが、ここはTerraform管理下から切り離しても良い部分かもしれません。\n例）main.tf\n# リージョナルREST APIゲートウェイの定義 resource \u0026#34;aws_api_gateway_rest_api\u0026#34; \u0026#34;regional\u0026#34; { name = var.regional_api_gateway_name body = templatefile(\u0026#34;${path.module}/regional_openapi.yaml\u0026#34;, { vpc_link = aws_api_gateway_vpc_link.reginal.id, cognito_user_pool = aws_cognito_user_pool.this.arn, api_gateway_role = aws_iam_role.regional_api_gateway.arn, backend_dns_name = aws_lb.backend.dns_name, }) endpoint_configuration { types = [\u0026#34;REGIONAL\u0026#34;] } tags = { Name = var.regional_api_gateway_name } } # リージョナルREST APIのデプロイ定義 resource \u0026#34;aws_api_gateway_deployment\u0026#34; \u0026#34;regional\u0026#34; { rest_api_id = aws_api_gateway_rest_api.regional.id triggers = { redeployment = sha1(templatefile(\u0026#34;${path.module}/regional_openapi.yaml\u0026#34;, { vpc_link = aws_api_gateway_vpc_link.reginal.id, cognito_user_pool = aws_cognito_user_pool.this.arn, api_gateway_role = aws_iam_role.regional_api_gateway.arn, backend_dns_name = aws_lb.backend.dns_name, })) } lifecycle { create_before_destroy = true } } # リージョナルREST APIのステージ定義 resource \u0026#34;aws_api_gateway_stage\u0026#34; \u0026#34;regional\u0026#34; { deployment_id = aws_api_gateway_deployment.regional.id rest_api_id = aws_api_gateway_rest_api.regional.id stage_name = var.regional_api_gateway_stage_name access_log_settings { destination_arn = aws_cloudwatch_log_group.regional_apigateway_log.arn format = \u0026#34;{\\\u0026#34;requestId\\\u0026#34;:\\\u0026#34;$context.requestId\\\u0026#34;, \\\u0026#34;extendedRequestId\\\u0026#34;:\\\u0026#34;$context.extendedRequestId\\\u0026#34;, \\\u0026#34;ip\\\u0026#34;: \\\u0026#34;$context.identity.sourceIp\\\u0026#34;, \\\u0026#34;caller\\\u0026#34;:\\\u0026#34;$context.identity.caller\\\u0026#34;, \\\u0026#34;user\\\u0026#34;:\\\u0026#34;$context.identity.user\\\u0026#34;, \\\u0026#34;requestTime\\\u0026#34;:\\\u0026#34;$context.requestTime\\\u0026#34;, \\\u0026#34;httpMethod\\\u0026#34;:\\\u0026#34;$context.httpMethod\\\u0026#34;, \\\u0026#34;resourcePath\\\u0026#34;:\\\u0026#34;$context.resourcePath\\\u0026#34;, \\\u0026#34;status\\\u0026#34;:\\\u0026#34;$context.status\\\u0026#34;, \\\u0026#34;protocol\\\u0026#34;:\\\u0026#34;$context.protocol\\\u0026#34;, \\\u0026#34;responseLength\\\u0026#34;:\\\u0026#34;$context.responseLength\\\u0026#34;}\u0026#34; } } # リージョナルREST APIのメソッド設定定義 resource \u0026#34;aws_api_gateway_method_settings\u0026#34; \u0026#34;regional\u0026#34; { rest_api_id = aws_api_gateway_rest_api.regional.id stage_name = aws_api_gateway_stage.regional.stage_name method_path = \u0026#34;*/*\u0026#34; settings { metrics_enabled = true logging_level = \u0026#34;INFO\u0026#34; data_trace_enabled = true } } # リージョナルREST APIのバックエンドサービス接続用VPCリンク定義 resource \u0026#34;aws_api_gateway_vpc_link\u0026#34; \u0026#34;regional\u0026#34; { name = var.vpc_link_name target_arns = [aws_lb.backend.arn] tags = { Name = var.vpc_link_name } } # モニタリング用IAMロールの接続 resource \u0026#34;aws_api_gateway_account\u0026#34; \u0026#34;this\u0026#34; { cloudwatch_role_arn = aws_iam_role.cloudwatch.arn } # モニタリング用IAMロールの定義 resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;cloudwatch\u0026#34; { name = var.iam_cloudwatch_role_name assume_role_policy = jsonencode({ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;apigateway.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] }) tags = { Name = var.iam_cloudwatch_role_name } lifecycle { create_before_destroy = true } } # モニタリング用IAMロールへのポリシー割り当て resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;cloudwatch\u0026#34; { role = aws_iam_role.cloudwatch.name policy_arn = \u0026#34;arn:aws:iam::aws:policy/service-role/AmazonAPIGatewayPushToCloudWatchLogs\u0026#34; } # リージョナルREST API用IAMロールの定義 resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;regional_api_gateway\u0026#34; { name = var.iam_regional_api_gateway_role_name assume_role_policy = jsonencode({ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;apigateway.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] }) tags = { Name = var.iam_regional_api_gateway_role_name } lifecycle { create_before_destroy = true } } # リージョナルREST API用IAMポリシーの定義 resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;regional_api_gateway\u0026#34; { name = var.iam_regional_api_gateway_policy_name path = \u0026#34;/\u0026#34; policy = jsonencode({ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: [ \u0026#34;apigateway:*\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:apigateway:*::/restapis/*/authorizers\u0026#34; ], \u0026#34;Condition\u0026#34;: { \u0026#34;ArnLike\u0026#34;: { \u0026#34;apigateway:CognitoUserPoolProviderArn\u0026#34;: [ \u0026#34;${aws_cognito_user_pool.this.arn}\u0026#34; ] } } } ] }) tags = { Name = var.iam_regional_api_gateway_policy_name } lifecycle { create_before_destroy = true } } # リージョナルREST API用IAMロールへのポリシー割り当て resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;regional_api_gateway\u0026#34; { policy_arn = aws_iam_policy.regional_api_gateway.arn role = aws_iam_role.regional_api_gateway.name } 例2. プライベートREST APIゲートウェイの作成 # 特定のVPCエンドポイント経由のアクセスのみを許可するプライベートREST APIゲートウェイの例です。今回はOpenAPI定義もTerraformでデプロイさせていますが、ここはTerraform管理下から切り離しても良い部分かもしれません。\n例）main.tf\n# プライベートREST APIゲートウェイの定義 resource \u0026#34;aws_api_gateway_rest_api\u0026#34; \u0026#34;private\u0026#34; { name = var.private_api_gateway_name body = templatefile(\u0026#34;${path.module}/private_openapi.yaml\u0026#34;, { api_gateway_role = aws_iam_role.private_api_gateway.arn }) endpoint_configuration { types = [\u0026#34;PRIVATE\u0026#34;] vpc_endpoint_ids = [aws_vpc_endpoint.execute_api.id] } tags = { Name = var.private_api_gateway_name } } # プライベートREST APIゲートウェイのポリシー定義 (特定のVPCエンドポイントのみ許可) resource \u0026#34;aws_api_gateway_rest_api_policy\u0026#34; \u0026#34;private\u0026#34; { rest_api_id = aws_api_gateway_rest_api.private.id policy = jsonencode({ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;execute-api:Invoke\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;${aws_api_gateway_rest_api.private.execution_arn}/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;execute-api:Invoke\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;${aws_api_gateway_rest_api.private.execution_arn}/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringNotEquals\u0026#34;: { \u0026#34;aws:SourceVpce\u0026#34;: \u0026#34;${aws_vpc_endpoint.execute_api.id}\u0026#34; } } } ] }) } # プライベートREST APIのデプロイ定義 resource \u0026#34;aws_api_gateway_deployment\u0026#34; \u0026#34;private\u0026#34; { rest_api_id = aws_api_gateway_rest_api.private.id triggers = { redeployment = sha1(templatefile(\u0026#34;${path.module}/private_openapi.yaml\u0026#34;, { api_gateway_role = aws_iam_role.private_api_gateway.arn })) } lifecycle { create_before_destroy = true } } # プライベートREST APIのステージ定義 resource \u0026#34;aws_api_gateway_stage\u0026#34; \u0026#34;private\u0026#34; { deployment_id = aws_api_gateway_deployment.private.id rest_api_id = aws_api_gateway_rest_api.private.id stage_name = var.private_api_gateway_stage_name access_log_settings { destination_arn = aws_cloudwatch_log_group.private_apigateway_log.arn format = \u0026#34;{\\\u0026#34;requestId\\\u0026#34;:\\\u0026#34;$context.requestId\\\u0026#34;, \\\u0026#34;extendedRequestId\\\u0026#34;:\\\u0026#34;$context.extendedRequestId\\\u0026#34;, \\\u0026#34;ip\\\u0026#34;: \\\u0026#34;$context.identity.sourceIp\\\u0026#34;, \\\u0026#34;caller\\\u0026#34;:\\\u0026#34;$context.identity.caller\\\u0026#34;, \\\u0026#34;user\\\u0026#34;:\\\u0026#34;$context.identity.user\\\u0026#34;, \\\u0026#34;requestTime\\\u0026#34;:\\\u0026#34;$context.requestTime\\\u0026#34;, \\\u0026#34;httpMethod\\\u0026#34;:\\\u0026#34;$context.httpMethod\\\u0026#34;, \\\u0026#34;resourcePath\\\u0026#34;:\\\u0026#34;$context.resourcePath\\\u0026#34;, \\\u0026#34;status\\\u0026#34;:\\\u0026#34;$context.status\\\u0026#34;, \\\u0026#34;protocol\\\u0026#34;:\\\u0026#34;$context.protocol\\\u0026#34;, \\\u0026#34;responseLength\\\u0026#34;:\\\u0026#34;$context.responseLength\\\u0026#34;}\u0026#34; } } # プライベートREST APIのメソッド設定定義 resource \u0026#34;aws_api_gateway_method_settings\u0026#34; \u0026#34;private\u0026#34; { rest_api_id = aws_api_gateway_rest_api.private.id stage_name = aws_api_gateway_stage.private.stage_name method_path = \u0026#34;*/*\u0026#34; settings { metrics_enabled = true logging_level = \u0026#34;INFO\u0026#34; data_trace_enabled = true } } # モニタリング用IAMロールの接続 resource \u0026#34;aws_api_gateway_account\u0026#34; \u0026#34;this\u0026#34; { cloudwatch_role_arn = aws_iam_role.cloudwatch.arn } # モニタリング用IAMロールの定義 resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;cloudwatch\u0026#34; { name = var.iam_cloudwatch_role_name assume_role_policy = jsonencode({ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;apigateway.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] }) tags = { Name = var.iam_cloudwatch_role_name } lifecycle { create_before_destroy = true } } # モニタリング用IAMロールへのポリシー割り当て resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;cloudwatch\u0026#34; { role = aws_iam_role.cloudwatch.name policy_arn = \u0026#34;arn:aws:iam::aws:policy/service-role/AmazonAPIGatewayPushToCloudWatchLogs\u0026#34; } # プライベートREST API用IAMロールの定義 resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;private_api_gateway\u0026#34; { name = var.iam_private_api_gateway_role_name assume_role_policy = jsonencode({ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;apigateway.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] }) tags = { Name = var.iam_private_api_gateway_role_name } lifecycle { create_before_destroy = true } } # プライベートREST API用IAMポリシーの定義 resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;private_api_gateway\u0026#34; { name = var.iam_private_api_gateway_policy_name path = \u0026#34;/\u0026#34; policy = jsonencode({ # 割愛 }) tags = { Name = var.iam_private_api_gateway_policy_name } lifecycle { create_before_destroy = true } } # プライベートREST API用IAMロールへのポリシー割り当て resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;private_api_gateway\u0026#34; { policy_arn = aws_iam_policy.private_api_gateway.arn role = aws_iam_role.private_api_gateway.name } 終わりに # 今回はTerraformの入門ということで、Amazon API Gatewayのサンプルコードをいくつかご紹介してきましたがいかがだったでしょうか。こんな記事でも誰かの役に立っていただけるのであれば幸いです。\nなお、今回ご紹介したコードはあくまでサンプルであり、動作を保証するものではございません。そのまま使用したことによって発生したトラブルなどについては一切責任を負うことはできませんのでご注意ください。\nAWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 Terraform は、HashiCorp, Inc. の米国およびその他の国における商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"December 29, 2022","permalink":"/posts/2022/12/aws-apigateway-sample-terraform-code/","section":"記事一覧","summary":"みなさん、こんにちは。今回はTerraformの入門ということでAmazon API Gatewayのサンプルコードを書いてみましたのでこちらを紹介していきたいと思います。","title":"TerraformでAmazon API Gatewayを構築するサンプルコードを書いてみた"},{"content":"みなさん、こんにちは。今回はTerraformの入門ということでAmazon ECSのサンプルコードを書いてみましたのでこちらを紹介していきたいと思います。\nなお、サンプルコードを書いた際のTerraformおよびAWSプロバイダーのバージョンは次のとおりです。最新バージョンでは定義方法が異なっている可能性があるため、実際にコードを書く際は最新の「Terraformドキュメント」と「AWSプロバイダードキュメント」を確認しながら開発を進めていただければと思います。\n例）versions.tf\n# Requirements terraform { required_version = \u0026#34;~\u0026gt; 1.3.6\u0026#34; required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.46.0\u0026#34; } } } サンプルコードを書いてみた # 今回は次のような構成のサンプルコードを書いてみました。なお、変数定義部分などの一部省略している点、ならびにステップごとの細かい説明などは省いていますのでご承知おきください。詳細については「AWSプロバイダードキュメント」をご参照ください。\n例1. ECSクラスタの作成 例2. タスク定義の作成 例3. ECSサービスの作成 ちなみに、すべてのサンプルコードに共通してプロバイダー定義は次のようにしています。\n例）providers.tf\n# デフォルトのプロバイダー設定 provider \u0026#34;aws\u0026#34; { region = var.aws_region default_tags { tags = { Owner = \u0026#34;matt\u0026#34; Terraform = \u0026#34;true\u0026#34; } } } 例1. ECSクラスタの作成 # ECSクラスタの例でContainer Insightsを有効化している以外には特筆するところはないかと思います。\n例）main.tf\n# ECSクラスタの作成 resource \u0026#34;aws_ecs_cluster\u0026#34; \u0026#34;this\u0026#34; { name = var.ecs_cluster_name setting { name = \u0026#34;containerInsights\u0026#34; value = \u0026#34;enabled\u0026#34; } tags = { Name = var.ecs_cluster_name } } 例2. タスク定義の作成 # AWS Fargate向けのタスク定義の例で、タスク起動用IAMロールやコンテナ用IAMロールなども併せて作成しています。\n例）main.tf\n# タスク定義の作成 resource \u0026#34;aws_ecs_task_definition\u0026#34; \u0026#34;this\u0026#34; { family = var.ecs_task_name network_mode = \u0026#34;awsvpc\u0026#34; requires_compatibilities = [\u0026#34;FARGATE\u0026#34;] cpu = var.ecs_task_cpu memory = var.ecs_task_memory execution_role_arn = aws_iam_role.ecs_execution.arn task_role_arn = aws_iam_role.ecs_task.arn container_definitions = jsonencode([ # 割愛 ]) tags = { Name = var.ecs_task_name } } # タスク起動用IAMロールの定義 resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;ecs_execution\u0026#34; { name = var.iam_ecs_execution_role_name assume_role_policy = jsonencode({ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;ecs-tasks.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] }) tags = { Name = var.iam_ecs_execution_role_name } } # タスク起動用IAMロールへのポリシー割り当て resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;ecs_execution\u0026#34; { policy_arn = \u0026#34;arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\u0026#34; role = aws_iam_role.ecs_execution.name } # コンテナ用IAMロールの定義 resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;ecs_task\u0026#34; { name = var.iam_ecs_task_role_name assume_role_policy = jsonencode({ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;ecs-tasks.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] }) tags = { Name = var.iam_ecs_task_role_name } } # コンテナ用IAMポリシーの定義（例ではSSMパラメータストアのアクセス権限を付与） resource \u0026#34;aws_iam_policy\u0026#34; \u0026#34;ecs_task\u0026#34; { name = var.iam_ecs_task_policy_name path = \u0026#34;/service-role/\u0026#34; policy = jsonencode({ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: [ \u0026#34;ssm:GetParametersByPath\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:GetParameter\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] } ] }) tags = { Name = var.iam_ecs_task_policy_name } } # コンテナ用IAMロールへのポリシー割り当て resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;ecs_task\u0026#34; { policy_arn = aws_iam_policy.ecs_task.arn role = aws_iam_role.ecs_task.name } 例3. ECSサービスの作成 # AWS Fargate向けのECSサービスの例で、タスク起動用IAMロールやコンテナ用IAMロールなども併せて作成しています。\n例）main.tf\n# ECSサービスの作成 resource \u0026#34;aws_ecs_service\u0026#34; \u0026#34;this\u0026#34; { name = var.ecs_service_name cluster = aws_ecs_cluster.this.id task_definition = aws_ecs_task_definition.this.arn desired_count = var.ecs_desired_count health_check_grace_period_seconds = var.ecs_health_check_grace_period_seconds launch_type = \u0026#34;FARGATE\u0026#34; force_new_deployment = var.ecs_force_new_deployment network_configuration { security_groups = [aws_security_group.this.id] subnets = [for x in data.aws_subnet.private : x.id] } load_balancer { target_group_arn = aws_lb_target_group.backend.arn container_name = var.ecs_container_name container_port = var.ecs_container_port } tags = { Name = var.ecs_service_name } depends_on = [aws_lb.cc_external_nlb] } # セキュリティグループの定義（通信制御の定義は割愛） resource \u0026#34;aws_security_group\u0026#34; \u0026#34;this\u0026#34; { name = var.security_group_name vpc_id = data.aws_vpc.this.id tags = { Name = var.security_group_name } lifecycle { create_before_destroy = true } } # NLB向けのECSサービス用ターゲットグループの定義 resource \u0026#34;aws_lb_target_group\u0026#34; \u0026#34;this\u0026#34; { name = var.nlb_target_group_name port = var.ecs_container_port protocol = \u0026#34;TCP\u0026#34; target_type = \u0026#34;ip\u0026#34; vpc_id = data.aws_vpc.this.id tags = { Name = var.elb_target_group_name } } 例）data.tf\n# VPC情報の取得 data \u0026#34;aws_vpc\u0026#34; \u0026#34;this\u0026#34; { cidr_block = var.vpc_cidr_block } # サブネット情報の取得 data \u0026#34;aws_subnet\u0026#34; \u0026#34;private\u0026#34; { for_each = var.private_subnets vpc_id = data.aws_vpc.default.id availability_zone = each.value.availability_zone cidr_block = each.value.cidr_block } 例）outputs.tf\noutput \u0026#34;security_group_id\u0026#34; { description = \u0026#34;The ID of the security group\u0026#34; value = try(aws_security_group.this.id, \u0026#34;\u0026#34;) } 終わりに # 今回はTerraformの入門ということで、Amazon ECSのサンプルコードをいくつかご紹介してきましたがいかがだったでしょうか。こんな記事でも誰かの役に立っていただけるのであれば幸いです。\nなお、今回ご紹介したコードはあくまでサンプルであり、動作を保証するものではございません。そのまま使用したことによって発生したトラブルなどについては一切責任を負うことはできませんのでご注意ください。\nAWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 Terraform は、HashiCorp, Inc. の米国およびその他の国における商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"December 29, 2022","permalink":"/posts/2022/12/aws-ecs-sample-terraform-code/","section":"記事一覧","summary":"みなさん、こんにちは。今回はTerraformの入門ということでAmazon ECSのサンプルコードを書いてみましたのでこちらを紹介していきたいと思います。","title":"TerraformでAmazon ECSを構築するサンプルコードを書いてみた"},{"content":"みなさん、こんにちは。今回はTerraformの入門ということでAmazon Route 53のサンプルコードを書いてみましたのでこちらを紹介していきたいと思います。\nなお、サンプルコードを書いた際のTerraformおよびAWSプロバイダーのバージョンは次のとおりです。最新バージョンでは定義方法が異なっている可能性があるため、実際にコードを書く際は最新の「Terraformドキュメント」と「AWSプロバイダードキュメント」を確認しながら開発を進めていただければと思います。\n例）versions.tf\n# Requirements terraform { required_version = \u0026#34;~\u0026gt; 1.3.6\u0026#34; required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.46.0\u0026#34; } } } サンプルコードを書いてみた # 今回は次のような構成のサンプルコードを書いてみました。なお、変数定義部分などの一部省略している点、ならびにステップごとの細かい説明などは省いていますのでご承知おきください。詳細については「AWSプロバイダードキュメント」をご参照ください。\n例1. プライベートDNSゾーンの作成 例2. 同一アカウント内でのプライベートDNSゾーンの共有 例3. 異なるアカウント間でのプライベートDNSゾーンの共有 例4. Aレコードの登録 例5. ALIASレコードの登録 例6. CNAMEレコードの登録 ちなみに、すべてのサンプルコードに共通してプロバイダー定義は次のようにしています。\n例）providers.tf\n# デフォルトのプロバイダー設定 provider \u0026#34;aws\u0026#34; { region = var.aws_region default_tags { tags = { Owner = \u0026#34;matt\u0026#34; Terraform = \u0026#34;true\u0026#34; } } } # プライベートDNSゾーンを共有する先のAWSアカウント情報 provider \u0026#34;aws\u0026#34; { alias = \u0026#34;guest\u0026#34; region = var.aws_region assume_role { role_arn = var.iam_assume_role } default_tags { tags = { Owner = \u0026#34;matt\u0026#34; Terraform = \u0026#34;true\u0026#34; } } } 例1. プライベートDNSゾーンの作成 # プライベートDNSゾーンの例です。クロスアカウントで共有するといったケースだとvpcブロックにすべて列挙することができないため、後からaws_route53_zone_associationリソースを作成したことによって生じた差分を無視するようにignore_changesを定義しています。\n例）main.tf\n# プライベートDNSゾーンの定義 resource \u0026#34;aws_route53_zone\u0026#34; \u0026#34;this\u0026#34; { name = \u0026#34;example.com\u0026#34; # NOTE: The aws_route53_zone vpc argument accepts multiple configuration # blocks. The below usage of the single vpc configuration, the # lifecycle configuration, and the aws_route53_zone_association # resource is for illustrative purposes (e.g. for a separate # cross-account authorization process, which is not shown here). vpc { vpc_id = data.aws_vpc.primary.id } lifecycle { ignore_changes = [vpc] } } 例）data.tf\n# VPC情報の取得 data \u0026#34;aws_vpc\u0026#34; \u0026#34;primary\u0026#34; { cidr_block = var.primary_vpc_cidr_block filter { name = \u0026#34;tag:Name\u0026#34; values = [var.primary_vpc_name] } } 例2. 同一アカウント内でのプライベートDNSゾーンの共有 # 同一アカウント内の別VPCからプライベートDNSゾーンに登録された名前を引けるようにする場合の例です。\n例）main.tf\n# 別VPCへのプライベートDNSゾーンの関連付け resource \u0026#34;aws_route53_zone_association\u0026#34; \u0026#34;this\u0026#34; { vpc_id = data.aws_vpc.secondary.id zone_id = aws_route53_zone.this.id } 例）data.tf\n# VPC情報の取得 data \u0026#34;aws_vpc\u0026#34; \u0026#34;secondary\u0026#34; { cidr_block = var.secondary_vpc_cidr_block filter { name = \u0026#34;tag:Name\u0026#34; values = [var.secondary_vpc_name] } } 例3. 異なるアカウント間でのプライベートDNSゾーンの共有 # 異なるアカウントのVPCからプライベートDNSゾーンに登録された名前を引けるようにする場合の例です。\n例）main.tf\n# 別アカウント上のVPCへの割り当て承認 resource \u0026#34;aws_route53_vpc_association_authorization\u0026#34; \u0026#34;this\u0026#34; { vpc_id = data.aws_vpc.guest.id zone_id = aws_route53_zone.this.id } # 別アカウント上のVPCへのプライベートDNSゾーンの関連付け resource \u0026#34;aws_route53_zone_association\u0026#34; \u0026#34;this\u0026#34; { provider = aws.guest vpc_id = data.aws_vpc.guest.id zone_id = aws_route53_zone.this.id } 例）data.tf\n# VPC情報の取得 data \u0026#34;aws_vpc\u0026#34; \u0026#34;guest\u0026#34; { provider = aws.guest cidr_block = var.guest_vpc_cidr_block filter { name = \u0026#34;tag:Name\u0026#34; values = [var.guest_vpc_name] } } 例4. Aレコードの登録 # ドメイン名とIPv4アドレスの対応付けを行うAレコードを登録する場合の例です。\n例）main.tf\n# DNSレコードの定義 resource \u0026#34;aws_route53_record\u0026#34; \u0026#34;www\u0026#34; { zone_id = aws_route53_zone.this.zone_id name = \u0026#34;www.example.com\u0026#34; type = \u0026#34;A\u0026#34; ttl = 300 records = [aws_eip.alb.public_ip] } 例5. ALIASレコードの登録 # AWSリソースのオリジナルドメイン名とは異なる名前でアクセスさせたい場合に活用するALIASレコードを登録する場合の例です。なお、ALIASレコードはttlが60秒で固定のためご留意ください。\n例）main.tf\n# DNSレコードの定義 resource \u0026#34;aws_route53_record\u0026#34; \u0026#34;www\u0026#34; { zone_id = aws_route53_zone.this.zone_id name = \u0026#34;www.example.com\u0026#34; type = \u0026#34;A\u0026#34; alias { name = aws_vpc_endpoint.this.dns_entry[0].dns_name zone_id = aws_vpc_endpoint.this.dns_entry[0].hosted_zone_id evaluate_target_health = true } } 例6. CNAMEレコードの登録 # オリジナルドメイン名とは異なる名前でアクセスさせたい場合に活用するCNAMEレコードを登録する場合の例です。\n例）main.tf\n# DNSレコードの定義 resource \u0026#34;aws_route53_record\u0026#34; \u0026#34;www\u0026#34; { zone_id = aws_route53_zone.this.zone_id name = \u0026#34;www\u0026#34; type = \u0026#34;CNAME\u0026#34; ttl = 300 records = [\u0026#34;www.example.co.jp\u0026#34;] } 終わりに # 今回はTerraformの入門ということで、Amazon Route 53のサンプルコードをいくつかご紹介してきましたがいかがだったでしょうか。こんな記事でも誰かの役に立っていただけるのであれば幸いです。\nなお、今回ご紹介したコードはあくまでサンプルであり、動作を保証するものではございません。そのまま使用したことによって発生したトラブルなどについては一切責任を負うことはできませんのでご注意ください。\nAWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 Terraform は、HashiCorp, Inc. の米国およびその他の国における商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"December 29, 2022","permalink":"/posts/2022/12/aws-route53-sample-terraform-code/","section":"記事一覧","summary":"みなさん、こんにちは。今回はTerraformの入門ということでAmazon Route 53のサンプルコードを書いてみましたのでこちらを紹介していきたいと思います。","title":"TerraformでAmazon Route 53を構築するサンプルコードを書いてみた"},{"content":"","date":"December 26, 2022","permalink":"/tags/cloud-audit-logs/","section":"タグ一覧","summary":"","title":"Cloud Audit Logs"},{"content":"みなさん、こんにちは。今回はTerraformの入門ということでCloud Audit Logsのサンプルコードを書いてみましたのでこちらを紹介していきたいと思います。\nなお、サンプルコードを書いた際のTerraformおよびGoogleプロバイダーのバージョンは次のとおりです。最新バージョンでは定義方法が異なっている可能性があるため、実際にコードを書く際は最新の「Terraformドキュメント」と「Googleプロバイダードキュメント」を確認しながら開発を進めていただければと思います。\n# Requirements terraform { required_version = \u0026#34;1.3.4\u0026#34; required_providers { google = { source = \u0026#34;hashicorp/google\u0026#34; version = \u0026#34;4.46.0\u0026#34; } } } サンプルコードを書いてみた # 今回は次のような構成のサンプルコードを書いてみました。なお、変数定義部分などの一部省略している点、ならびにステップごとの細かい説明などは省いていますのでご承知おきください。詳細については「Googleプロバイダードキュメント」をご参照ください。\n例1. データアクセス監査ログのデフォルト構成設定 例2. サービスごとのデータアクセス監査ログ構成設定 ちなみに、すべてのサンプルコードに共通してプロバイダー定義は次のようにしています。\n# デフォルトのプロバイダー設定 provider \u0026#34;google\u0026#34; { project = var.project_id region = \u0026#34;asia-northeast1\u0026#34; zone = \u0026#34;asia-northeast1-b\u0026#34; } 例1. データアクセス監査ログのデフォルト構成設定 # データアクセス監査ログのデフォルト構成にて、すべてのログタイプ(=ADMIN_READ、DATA_READ、DATA_WRITEの3種)を有効化する場合の例です。コード量を減らすためにdynamicブロックを使ってaudit_log_configブロックを定義してみました。\n# デフォルトで有効化するログタイプ一覧 variable \u0026#34;default_enabled_log_types\u0026#34; { default = [\u0026#34;ADMIN_READ\u0026#34;, \u0026#34;DATA_READ\u0026#34;, \u0026#34;DATA_WRITE\u0026#34;] } # 設定対象プロジェクトの情報取得 data \u0026#34;google_project\u0026#34; \u0026#34;this\u0026#34; {} # Cloud Audit Logsリソース定義 resource \u0026#34;google_project_iam_audit_config\u0026#34; \u0026#34;default\u0026#34; { count = length(var.default_enabled_log_types) \u0026gt; 0 ? 1 : 0 project = data.google_project.this.id service = \u0026#34;allServices\u0026#34; dynamic \u0026#34;audit_log_config\u0026#34; { for_each = var.default_enabled_log_types content { log_type = audit_log_config.value } } } 例2. サービスごとのデータアクセス監査ログ構成設定 # 特定のサービスのデータアクセス監査ログを有効化する場合の例です。今回はCloud SQLとCloud Spannerのデータ書き込みDATA_WRITE監査ログのみを有効化しています。なお、Cloud Audit Logsの設定で指定するサービス名については公式ドキュメントの「サービスとリソースのマッピング」を参照してください。\n# サービスごとに有効化するログタイプ一覧 variable \u0026#34;enabled_log_types\u0026#34; { default = { \u0026#34;cloudsql.googleapis.com\u0026#34; = [\u0026#34;DATA_WRITE\u0026#34;] \u0026#34;spanner.googleapis.com\u0026#34; = [\u0026#34;DATA_WRITE\u0026#34;] } } # 設定対象プロジェクトの情報取得 data \u0026#34;google_project\u0026#34; \u0026#34;this\u0026#34; {} # Cloud Audit Logsリソース定義 resource \u0026#34;google_project_iam_audit_config\u0026#34; \u0026#34;service\u0026#34; { for_each = var.enabled_log_types project = data.google_project.this.id service = each.key dynamic \u0026#34;audit_log_config\u0026#34; { for_each = each.value content { log_type = audit_log_config.value } } } 終わりに # 今回はTerraformの入門ということで、Cloud Audit Logsのサンプルコードをいくつかご紹介してきましたがいかがだったでしょうか。\n記事執筆時点ではgoogle_project_iam_audit_configリソースはaudit_log_configブロックが1つ以上定義されていないとエラーとなる仕様のため、今回は有効化するログがなければそもそもリソースを作らないようにしています。ただ、これだとリソースを作ってないことになるので、あとから手で有効化された際にTerraformで差分を検知できません。そこの点が正直イケてないなぁと思ってますが、こんな記事でも誰かの役に立っていただけるのであれば幸いです^^;\nなお、今回ご紹介したコードはあくまでサンプルであり、動作を保証するものではございません。そのまま使用したことによって発生したトラブルなどについては一切責任を負うことはできませんのでご注意ください。\nGoogle Cloud は、Google LLC の商標または登録商標です。 Terraform は、HashiCorp, Inc. の米国およびその他の国における商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"December 26, 2022","permalink":"/posts/2022/12/gcp-cloud-audit-logs-sample-terraform-code/","section":"記事一覧","summary":"みなさん、こんにちは。今回はTerraformの入門ということでCloud Audit Logsのサンプルコードを書いてみましたのでこちらを紹介していきたいと思います。","title":"TerraformでCloud Audit Logsを構築するサンプルコードを書いてみた"},{"content":"","date":"December 23, 2022","permalink":"/tags/amazon-aurora/","section":"タグ一覧","summary":"","title":"Amazon Aurora"},{"content":"","date":"December 23, 2022","permalink":"/tags/cloud-kms/","section":"タグ一覧","summary":"","title":"Cloud KMS"},{"content":"みなさん、こんにちは。今回はTerraformの入門ということでAmazon Auroraのサンプルコードを書いてみましたのでこちらを紹介していきたいと思います。\nなお、サンプルコードを書いた際のTerraformおよびAWSプロバイダーのバージョンは次のとおりです。最新バージョンでは定義方法が異なっている可能性があるため、実際にコードを書く際は最新の「Terraformドキュメント」と「AWSプロバイダードキュメント」を確認しながら開発を進めていただければと思います。\n# Requirements terraform { required_version = \u0026#34;~\u0026gt; 1.3.6\u0026#34; required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.46.0\u0026#34; } } } サンプルコードを書いてみた # 今回は次のような構成のサンプルコードを書いてみました。なお、変数定義部分などの一部省略している点、ならびにステップごとの細かい説明などは省いていますのでご承知おきください。詳細については「AWSプロバイダードキュメント」をご参照ください。\n例1. RDSサブネットグループの作成 例2. RDSパラメータグループの作成 例3. RDSクラスタの作成 例4. RDSインスタンスの作成 ちなみに、すべてのサンプルコードに共通してプロバイダー定義は次のようにしています。\n例）providers.tf\n# デフォルトのプロバイダー設定 provider \u0026#34;aws\u0026#34; { region = var.aws_region default_tags { tags = { Owner = \u0026#34;matt\u0026#34; Terraform = \u0026#34;true\u0026#34; } } } 例1. RDSサブネットグループの作成 # RDSサブネットグループの実装例で、こちらについては特筆すべき点はとくにないと思います。\n例）main.tf\n# RDSサブネットグループの定義 resource \u0026#34;aws_db_subnet_group\u0026#34; \u0026#34;this\u0026#34; { name = var.rds_subnet_group_name subnet_ids = [for x in data.aws_subnet.private : x.id] tags = { Name = var.rds_subnet_group_name } } 例）data.tf\n# VPC情報の取得 data \u0026#34;aws_vpc\u0026#34; \u0026#34;this\u0026#34; { cidr_block = var.vpc_cidr_block } # サブネット情報の取得 data \u0026#34;aws_subnet\u0026#34; \u0026#34;private\u0026#34; { for_each = var.private_subnets vpc_id = data.aws_vpc.default.id availability_zone = each.value.availability_zone cidr_block = each.value.cidr_block } 例2. RDSパラメータグループの作成 # RDSパラメータグループの実装例で、定義するパラメータの個数などは環境によってマチマチだと思いますので、変数側で調整できるようにdynamicブロックを使って動的に定義しています。\n例）main.tf\n# RDSパラメータグループの定義 resource \u0026#34;aws_rds_cluster_parameter_group\u0026#34; \u0026#34;this\u0026#34; { name_prefix = var.rds_parameter_group_name family = var.rds_engine_family dynamic \u0026#34;parameter\u0026#34; { for_each = var.rds_parameters content { name = parameter.value.name value = parameter.value.value apply_method = parameter.value.apply_method } } tags = { Name = var.rds_parameter_group_name } lifecycle { create_before_destroy = true } } 例）variables.tf\n# RDSパラメータ一覧 variable \u0026#34;rds_parameters\u0026#34; { type = map(any) default = { server_audit_events = { name = \u0026#34;server_audit_events\u0026#34; value = \u0026#34;CONNECT,QUERY,TABLE\u0026#34; apply_method = \u0026#34;immediate\u0026#34; } server_audit_logging = { name = \u0026#34;server_audit_logging\u0026#34; value = \u0026#34;1\u0026#34; apply_method = \u0026#34;immediate\u0026#34; } time_zone = { name = \u0026#34;time_zone\u0026#34; value = \u0026#34;Asia/Tokyo\u0026#34; apply_method = \u0026#34;pending-reboot\u0026#34; } } } 例3. RDSクラスタの作成 # RDSクラスタの実装例で、定義するパラメータの個数などは環境によってマチマチだと思いますので、変数側で調整できるようにdynamicブロックを使って動的に定義しています。\n例）main.tf\n# RDSクラスタの定義 resource \u0026#34;aws_rds_cluster\u0026#34; \u0026#34;this\u0026#34; { cluster_identifier = var.rds_cluster_name engine = var.rds_engine engine_version = var.rds_engine_version database_name = var.rds_database_name master_username = data.aws_ssm_parameter.rds_master_username.value master_password = data.aws_ssm_parameter.rds_master_password.value db_cluster_parameter_group_name = aws_rds_cluster_parameter_group.this.name # storage storage_encrypted = true kms_key_id = data.aws_kms_key.this.arn # network db_subnet_group_name = aws_db_subnet_group.this.name vpc_security_group_ids = [aws_security_group.cc_secret_db.id] port = var.rds_port # monitoring enabled_cloudwatch_logs_exports = [\u0026#34;error\u0026#34;, \u0026#34;audit\u0026#34;, \u0026#34;slowquery\u0026#34;] # backup backtrack_window = var.rds_backtrack_window backup_retention_period = var.rds_backup_retention_period preferred_backup_window = var.rds_preferred_backup_window copy_tags_to_snapshot = true deletion_protection = var.rds_deletion_protection skip_final_snapshot = var.rds_skip_final_snapshot final_snapshot_identifier = var.rds_skip_final_snapshot ? null : \u0026#34;${var.rds_cluster_name}-snapshot-final\u0026#34; # maintenance preferred_maintenance_window = var.rds_preferred_maintenance_window apply_immediately = true tags = { Name = var.rds_cluster_name } lifecycle { ignore_changes = [master_password, availability_zones] } } 例）data.tf\n# VPC情報の取得 data \u0026#34;aws_vpc\u0026#34; \u0026#34;this\u0026#34; { cidr_block = var.vpc_cidr_block } # SSMパラメータ情報の取得（管理ユーザ名） data \u0026#34;aws_ssm_parameter\u0026#34; \u0026#34;rds_master_username\u0026#34; { name = var.ssm_parameters.rds_master_username.name } # SSMパラメータ情報の取得（管理ユーザの初期パスワード） data \u0026#34;aws_ssm_parameter\u0026#34; \u0026#34;rds_master_password\u0026#34; { name = var.ssm_parameters.rds_master_password.name } # 暗号鍵情報の取得 data \u0026#34;aws_kms_key\u0026#34; \u0026#34;this\u0026#34; { key_id = var.kms_key_alias_name } 例4. RDSインスタンスの作成 # RDSインスタンスの実装例で、モニタリング用のIAMロールの定義なども併せて記載しています。\nなお、マスターユーザ/パスワード情報はSecrets Managerで管理というケースが最近は多くなってきていると思いますが、今回の例ではパスワード情報はプロジェクト管理とし、ここではあくまで構築用の初期パスワードという扱いでSSMパラメータストアにSecureStingとして事前に格納しておいた情報を扱うようにしています。\n例）main.tf\n# RDSインスタンスの定義 resource \u0026#34;aws_rds_cluster_instance\u0026#34; \u0026#34;this\u0026#34; { count = var.rds_instance_count cluster_identifier = aws_rds_cluster.this.id identifier = \u0026#34;${var.rds_instance_name}-${count.index + 1}\u0026#34; instance_class = var.rds_instance_class engine = aws_rds_cluster.this.engine engine_version = aws_rds_cluster.this.engine_version # network db_subnet_group_name = aws_db_subnet_group.this.name # monitoring monitoring_interval = var.rds_monitoring_interval monitoring_role_arn = aws_iam_role.monitoring.arn performance_insights_enabled = true performance_insights_kms_key_id = data.aws_kms_key.this.arn performance_insights_retention_period = var.rds_performance_insights_retention_period # backup copy_tags_to_snapshot = true # maintenance preferred_maintenance_window = var.rds_preferred_maintenance_window auto_minor_version_upgrade = false publicly_accessible = false apply_immediately = true tags = { Name = var.rds_instance_name } } # モニタリング用のIAMロール定義 resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;monitoring\u0026#34; { name = var.monitoring_role_name assume_role_policy = jsonencode({ \u0026#34;Version\u0026#34; : \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34; : [ { \u0026#34;Sid\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;Effect\u0026#34; : \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34; : { \u0026#34;Service\u0026#34; : \u0026#34;monitoring.rds.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34; : \u0026#34;sts:AssumeRole\u0026#34; } ] }) tags = { Name = monitoring_role_name } } # モニタリング用のIAMロールへのポリシー追加 resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;this\u0026#34; { policy_arn = \u0026#34;arn:aws:iam::aws:policy/service-role/AmazonRDSEnhancedMonitoringRole\u0026#34; role = aws_iam_role.monitoring.name } 終わりに # 今回はTerraformの入門ということで、Amazon Auroraのサンプルコードをいくつかご紹介してきましたがいかがだったでしょうか。こんな記事でも誰かの役に立っていただけるのであれば幸いです。\nなお、今回ご紹介したコードはあくまでサンプルであり、動作を保証するものではございません。そのまま使用したことによって発生したトラブルなどについては一切責任を負うことはできませんのでご注意ください。\nAWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 Terraform は、HashiCorp, Inc. の米国およびその他の国における商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"December 23, 2022","permalink":"/posts/2022/12/aws-rds-aurora-sample-terraform-code/","section":"記事一覧","summary":"みなさん、こんにちは。今回はTerraformの入門ということでAmazon Auroraのサンプルコードを書いてみましたのでこちらを紹介していきたいと思います。","title":"TerraformでAmazon Auroraを構築するサンプルコードを書いてみた"},{"content":"みなさん、こんにちは。今回はTerraformの入門ということでAWS Cloud9のサンプルコードを書いてみましたのでこちらを紹介していきたいと思います。\nなお、サンプルコードを書いた際のTerraformおよびAWSプロバイダーのバージョンは次のとおりです。最新バージョンでは定義方法が異なっている可能性があるため、実際にコードを書く際は最新の「Terraformドキュメント」と「AWSプロバイダードキュメント」を確認しながら開発を進めていただければと思います。\n# Requirements terraform { required_version = \u0026#34;~\u0026gt; 1.3.6\u0026#34; required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.46.0\u0026#34; } } } サンプルコードを書いてみた # 今回は次のような構成のサンプルコードを書いてみました。なお、変数定義部分などの一部省略している点、ならびにステップごとの細かい説明などは省いていますのでご承知おきください。詳細については「AWSプロバイダードキュメント」をご参照ください。\n例1. Cloud9環境の作成 例2. Cloud9環境へのメンバー追加 ちなみに、すべてのサンプルコードに共通してプロバイダー定義は次のようにしています。\n# デフォルトのプロバイダー設定 provider \u0026#34;aws\u0026#34; { region = var.aws_region default_tags { tags = { Owner = \u0026#34;matt\u0026#34; Terraform = \u0026#34;true\u0026#34; } } } 例1. Cloud9環境の作成 # 接続方式としてSSMを活用するCloud9環境の例です。automatic_stop_time_minutesパラメータをウッカリ指定し忘れるとEC2インスタンスがずっと起動しっぱなしとなり、ムダに課金をしてしまうことになるためご注意ください。\n# Cloud9環境を展開する先のVPC情報の取得 data \u0026#34;aws_vpc\u0026#34; \u0026#34;this\u0026#34; { cidr_block = var.vpc_cidr_block filter { name = \u0026#34;tag:Name\u0026#34; values = [var.vpc_name] } } # Cloud9環境を展開する先のサブネット情報の取得 data \u0026#34;aws_subnet\u0026#34; \u0026#34;private\u0026#34; { vpc_id = data.aws_vpc.default.id availability_zone = var.aws_availability_zone cidr_block = var.private_subnet_cidr_block filter { name = \u0026#34;tag:Name\u0026#34; values = [var.private_subnet_name] } } # Cloud9環境の定義 resource \u0026#34;aws_cloud9_environment_ec2\u0026#34; \u0026#34;this\u0026#34; { name = var.cloud9_name instance_type = \u0026#34;t3.small\u0026#34; connection_type = \u0026#34;CONNECT_SSM\u0026#34; subnet_id = data.aws_subnet.private.id automatic_stop_time_minutes = 30 } 例2. Cloud9環境へのメンバー追加 # 本設定は個人でCloud9を立てて使う分には必要はないのですが、だれかと共有して利用するといった場合は次のような形で利用するメンバーを追加で登録することが可能です。\nvariable \u0026#34;cloud9_members\u0026#34; { descriptions = \u0026#34;IAM user name list of cloud9 environment members\u0026#34; default = [\u0026#34;sample_iam_user1\u0026#34;, \u0026#34;sample_iam_user2\u0026#34;] } # Cloud9環境の利用メンバーのIAMユーザの情報取得 data \u0026#34;aws_iam_user\u0026#34; \u0026#34;cloud9_members\u0026#34; { for_each = toset(var.cloud9_members) user_name = each.value } # Cloud9環境を利用するメンバー追加 resource \u0026#34;aws_cloud9_environment_membership\u0026#34; \u0026#34;this\u0026#34; { for_each = toset(var.cloud9_members) environment_id = aws_cloud9_environment_ec2.this.id permissions = \u0026#34;read-write\u0026#34; user_arn = data.aws_iam_user.cloud9_members[each.value].arn } 終わりに # 今回はTerraformの入門ということで、Cloud9のサンプルコードをいくつかご紹介してきましたがいかがだったでしょうか。こんな記事でも誰かの役に立っていただけるのであれば幸いです。\nなお、今回ご紹介したコードはあくまでサンプルであり、動作を保証するものではございません。そのまま使用したことによって発生したトラブルなどについては一切責任を負うことはできませんのでご注意ください。\nAWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 Terraform は、HashiCorp, Inc. の米国およびその他の国における商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"December 23, 2022","permalink":"/posts/2022/12/aws-cloud9-sample-terraform-code/","section":"記事一覧","summary":"みなさん、こんにちは。今回はTerraformの入門ということでAWS Cloud9のサンプルコードを書いてみましたのでこちらを紹介していきたいと思います。","title":"TerraformでAWS Cloud9を構築するサンプルコードを書いてみた"},{"content":"みなさん、こんにちは。今回はTerraformの入門ということでCloud Key Management Service(KMS)のサンプルコードを書いてみましたのでこちらを紹介していきたいと思います。\nなお、サンプルコードを書いた際のTerraformおよびGoogleプロバイダーのバージョンは次のとおりです。最新バージョンでは定義方法が異なっている可能性があるため、実際にコードを書く際は最新の「Terraformドキュメント」と「Googleプロバイダードキュメント」を確認しながら開発を進めていただければと思います。\n# Requirements terraform { required_version = \u0026#34;1.3.4\u0026#34; required_providers { google = { source = \u0026#34;hashicorp/google\u0026#34; version = \u0026#34;4.46.0\u0026#34; } } } サンプルコードを書いてみた # 今回は次のような構成のサンプルコードを書いてみました。なお、変数定義部分などの一部省略している点、ならびにステップごとの細かい説明などは省いていますのでご承知おきください。詳細については「Googleプロバイダードキュメント」をご参照ください。\n例1. キーリングの作成 例2. 標準的な顧客管理暗号鍵(CMEK)の作成 例3. 暗号鍵に対するアクセス権限の付与 ちなみに、すべてのサンプルコードに共通してプロバイダー定義は次のようにしています。\n# デフォルトのプロバイダー設定 provider \u0026#34;google\u0026#34; { project = var.project_id region = \u0026#34;asia-northeast1\u0026#34; zone = \u0026#34;asia-northeast1-b\u0026#34; } 例1. キーリングの作成 # Cloud Spannerなどのマルチリージョンのリソース向けの鍵を管理するキーリングの例で、今回はasia1(東京/大阪/ソウル)をターゲットにしています。キーリングのオプションはlocationくらいしかないのでとくに困ることはないかと思います。\nただ、キーリングの扱いについては一点注意が必要で、Google Cloudの仕様でGoogle Cloud上からキーリングを削除することはできません。その一方、Terraform管理上はdestroyなどで簡単に削除することができてしまうため、その対処としてTerraform管理上からも削除できないように例ではprevent_destroyを指定します。\n# キーリング定義 resource \u0026#34;google_kms_key_ring\u0026#34; \u0026#34;this\u0026#34; { name = \u0026#34;my-kms-keyring-asia1\u0026#34; location = \u0026#34;asia1\u0026#34; lifecycle { prevent_destroy = true } } 例2. 標準的な顧客管理暗号鍵(CMEK)の作成 # Cloud KMS上で顧客管理の暗号鍵を生成する例で、ここでは自動ローテーションを90日(=7,776,000秒)で設定しています。今回のケースではversion_templateの部分はすべてデフォルト値なので、あえて書く必要はなかったのですが明記しておいた方が良いと感じて書きました。\nhttps://cloud.google.com/kms/docs/algorithms\n# 暗号鍵定義 resource \u0026#34;google_kms_crypto_key\u0026#34; \u0026#34;this\u0026#34; { name = \u0026#34;my-kms-crypto-key\u0026#34; key_ring = google_kms_key_ring.this.id purpose = \u0026#34;ENCRYPT_DECRYPT\u0026#34; rotation_period = \u0026#34;7776000s\u0026#34; # min 86400s(=1d) version_template { algorithm = \u0026#34;GOOGLE_SYMMETRIC_ENCRYPTION\u0026#34; protection_level = \u0026#34;SOFTWARE\u0026#34; } } 例3. 暗号鍵に対するアクセス権限の付与 # キーポリシーの設定にて対象となる暗号鍵個別に権限を付与する場合の例で、ここでは対象の暗号鍵を使って暗号および複合をできるようにするための権限を特定のサービスアカウントへ付与しています。\n余談ですが、権限設定は暗号鍵個別のキーポリシー設定ではなく、IAM設定にて対象アカウントへまるっとCloud KMS権限を付与することでも可能なため、適宜使い分けていただければと思います。\n# 権限付与対象のアカウント情報取得（例ではサービスアカウントを指定） data \u0026#34;google_service_account\u0026#34; \u0026#34;sample\u0026#34; { account_id = var.service_account_id } # アクセス権の付与 resource \u0026#34;google_kms_crypto_key_iam_binding\u0026#34; \u0026#34;apigee_db\u0026#34; { crypto_key_id = google_kms_crypto_key.this.id role = \u0026#34;roles/cloudkms.cryptoKeyEncrypterDecrypter\u0026#34; members = [data.google_service_account.this.member] } 終わりに # 今回はTerraformの入門ということで、Cloud KMSのサンプルコードをいくつかご紹介してきましたがいかがだったでしょうか。こんな記事でも誰かの役に立っていただけるのであれば幸いです。\nなお、今回ご紹介したコードはあくまでサンプルであり、動作を保証するものではございません。そのまま使用したことによって発生したトラブルなどについては一切責任を負うことはできませんのでご注意ください。\nGoogle Cloud は、Google LLC の商標または登録商標です。 Terraform は、HashiCorp, Inc. の米国およびその他の国における商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"December 23, 2022","permalink":"/posts/2022/12/gcp-cloud-kms-sample-terraform-code/","section":"記事一覧","summary":"みなさん、こんにちは。今回はTerraformの入門ということでCloud Key Management Service(KMS)のサンプルコードを書いてみましたのでこちらを紹介していきたいと思います。","title":"TerraformでCloud KMSを構築するサンプルコードを書いてみた"},{"content":"","date":"December 13, 2022","permalink":"/tags/amazon-cloudfront/","section":"タグ一覧","summary":"","title":"Amazon CloudFront"},{"content":"","date":"December 13, 2022","permalink":"/tags/aws-waf/","section":"タグ一覧","summary":"","title":"AWS WAF"},{"content":"みなさん、こんにちは。今回はTerraformの入門ということでCloudFrontおよびAPI Gatewayに割り当てるAWS WAFのサンプルコードを書いてみましたのでこちらを紹介していきたいと思います。\nなお、サンプルコードを書いた際のTerraformおよびAWSプロバイダーのバージョンは次のとおりです。最新バージョンでは定義方法が異なっている可能性があるため、実際にコードを書く際は最新の「Terraformドキュメント」と「AWSプロバイダードキュメント」を確認しながら開発を進めていただければと思います。\n# Requirements terraform { required_version = \u0026#34;1.3.6\u0026#34; required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;4.46.0\u0026#34; } } } サンプルコードを書いてみた # 今回は次のようなサンプルコードを書いてみました。なお、変数定義部分などの一部省略している点、ならびにステップごとの細かい説明などは省いていますのでご承知おきください。詳細については「AWSプロバイダードキュメント」をご参照ください。\n例1. 汎用的なCloudFront向け定義 例2. 汎用的なAPI Gateway向け定義 例3. IPホワイトリストを用いたアクセス制御 例4. カスタムヘッダーを用いたアクセス制御 例5. CloudWatch Logsを活用したロギング設定 例6. S3バケットを活用したロギング設定 ちなみに、すべてのサンプルコードに共通してプロバイダー定義は次のようにしています。\n# デフォルトのプロバイダー設定 provider \u0026#34;aws\u0026#34; { region = var.aws_region default_tags { tags = { terraform = \u0026#34;true\u0026#34; } } } # CloudFront向けのプロバイダー設定 provider \u0026#34;aws\u0026#34; { alias = \u0026#34;us_east_1\u0026#34; region = \u0026#34;us-east-1\u0026#34; default_tags { tags = { terraform = \u0026#34;true\u0026#34; } } } 例1. 汎用的なCloudFront向け定義 # 次のAWSマネージドルールを組み合わせてそれっぽい汎用的なCloudFront向けのWAF設定を記載してみました。Linux OS以降はアプリケーションの実装により適宜変えるイメージです。\nAmazon IP 評価リストマネージドルールグループ 匿名 IP リストマネージドルールグループ コアルールセット (CRS) マネージドルールグループ 既知の不正な入力マネージドルールグループ Linux オペレーティングシステムマネージドルールグループ POSIX オペレーティングシステムマネージドルールグループ SQL データベースマネージドルールグループ CloudFrontリソースはus-east-1リージョンに配置されるため、WAFリソースも同様にus-east-1へ作成する必要があるのでご留意ください。 優先度についてはAWSマネジメントコンソールでポチポチした際に付与される優先度と同じ順序にしています。 CloudFrontへのWAF割り当てはaws_wafv2_web_acl_associationではなく、aws_cloudfront_distributionにて実施するため今回は省略しています。 8KB以上のリクエストボディを扱えるようにSizeRestrictions_BODYルールを、他のクラウドプロバイダーからのアクセスを誤ってブロックしないようにHostingProviderIPListルールを除外しています。 # WAFのWeb ACL設定 resource \u0026#34;aws_wafv2_web_acl\u0026#34; \u0026#34;cloudfront\u0026#34; { provider = aws.us_east_1 name = var.aws_gloal_wafv2_name scope = \u0026#34;CLOUDFRONT\u0026#34; default_action { allow {} } rule { name = \u0026#34;AWS-AWSManagedRulesAmazonIpReputationList\u0026#34; priority = 0 override_action { none {} } statement { managed_rule_group_statement { name = \u0026#34;AWSManagedRulesAmazonIpReputationList\u0026#34; vendor_name = \u0026#34;AWS\u0026#34; } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \u0026#34;AWS-AWSManagedRulesAmazonIpReputationList\u0026#34; sampled_requests_enabled = true } } rule { name = \u0026#34;AWS-AWSManagedRulesAnonymousIpList\u0026#34; priority = 1 override_action { none {} } statement { managed_rule_group_statement { name = \u0026#34;AWSManagedRulesAnonymousIpList\u0026#34; vendor_name = \u0026#34;AWS\u0026#34; excluded_rule { name = \u0026#34;HostingProviderIPList\u0026#34; } } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \u0026#34;AWS-AWSManagedRulesAnonymousIpList\u0026#34; sampled_requests_enabled = true } } rule { name = \u0026#34;AWS-AWSManagedRulesCommonRuleSet\u0026#34; priority = 2 override_action { none {} } statement { managed_rule_group_statement { name = \u0026#34;AWSManagedRulesCommonRuleSet\u0026#34; vendor_name = \u0026#34;AWS\u0026#34; excluded_rule { name = \u0026#34;SizeRestrictions_BODY\u0026#34; } } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \u0026#34;AWS-AWSManagedRulesCommonRuleSet\u0026#34; sampled_requests_enabled = true } } rule { name = \u0026#34;AWS-AWSManagedRulesKnownBadInputsRuleSet\u0026#34; priority = 3 override_action { none {} } statement { managed_rule_group_statement { name = \u0026#34;AWSManagedRulesKnownBadInputsRuleSet\u0026#34; vendor_name = \u0026#34;AWS\u0026#34; } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \u0026#34;AWS-AWSManagedRulesKnownBadInputsRuleSet\u0026#34; sampled_requests_enabled = true } } rule { name = \u0026#34;AWS-AWSManagedRulesLinuxRuleSet\u0026#34; priority = 4 override_action { none {} } statement { managed_rule_group_statement { name = \u0026#34;AWSManagedRulesLinuxRuleSet\u0026#34; vendor_name = \u0026#34;AWS\u0026#34; } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \u0026#34;AWS-AWSManagedRulesLinuxRuleSet\u0026#34; sampled_requests_enabled = true } } rule { name = \u0026#34;AWS-AWSManagedRulesUnixRuleSet\u0026#34; priority = 5 override_action { none {} } statement { managed_rule_group_statement { name = \u0026#34;AWSManagedRulesUnixRuleSet\u0026#34; vendor_name = \u0026#34;AWS\u0026#34; } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \u0026#34;AWS-AWSManagedRulesUnixRuleSet\u0026#34; sampled_requests_enabled = true } } rule { name = \u0026#34;AWS-AWSManagedRulesSQLiRuleSet\u0026#34; priority = 6 override_action { none {} } statement { managed_rule_group_statement { name = \u0026#34;AWSManagedRulesSQLiRuleSet\u0026#34; vendor_name = \u0026#34;AWS\u0026#34; } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \u0026#34;AWS-AWSManagedRulesSQLiRuleSet\u0026#34; sampled_requests_enabled = true } } visibility_config { cloudwatch_metrics_enabled = true metric_name = var.aws_gloal_wafv2_name sampled_requests_enabled = true } } 例2. 汎用的なAPI Gateway向け定義 # 「例1. 汎用的なCloudFront向け定義」と同等内容のそれっぽい汎用的なAPI Gateway向けのWAF定義です。CloudFront向けとの差異はスコープがREGIONALになっている点とデフォルトのプロバイダー設定を利用している点の2点です。\n# WAFのWeb ACL設定 resource \u0026#34;aws_wafv2_web_acl\u0026#34; \u0026#34;apigw\u0026#34; { name = var.aws_apigw_wafv2_name scope = \u0026#34;REGIONAL\u0026#34; default_action { allow {} } rule { name = \u0026#34;AWS-AWSManagedRulesAmazonIpReputationList\u0026#34; priority = 0 override_action { none {} } statement { managed_rule_group_statement { name = \u0026#34;AWSManagedRulesAmazonIpReputationList\u0026#34; vendor_name = \u0026#34;AWS\u0026#34; } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \u0026#34;AWS-AWSManagedRulesAmazonIpReputationList\u0026#34; sampled_requests_enabled = true } } rule { name = \u0026#34;AWS-AWSManagedRulesAnonymousIpList\u0026#34; priority = 1 override_action { none {} } statement { managed_rule_group_statement { name = \u0026#34;AWSManagedRulesAnonymousIpList\u0026#34; vendor_name = \u0026#34;AWS\u0026#34; excluded_rule { name = \u0026#34;HostingProviderIPList\u0026#34; } } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \u0026#34;AWS-AWSManagedRulesAnonymousIpList\u0026#34; sampled_requests_enabled = true } } rule { name = \u0026#34;AWS-AWSManagedRulesCommonRuleSet\u0026#34; priority = 2 override_action { none {} } statement { managed_rule_group_statement { name = \u0026#34;AWSManagedRulesCommonRuleSet\u0026#34; vendor_name = \u0026#34;AWS\u0026#34; excluded_rule { name = \u0026#34;SizeRestrictions_BODY\u0026#34; } } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \u0026#34;AWS-AWSManagedRulesCommonRuleSet\u0026#34; sampled_requests_enabled = true } } rule { name = \u0026#34;AWS-AWSManagedRulesKnownBadInputsRuleSet\u0026#34; priority = 3 override_action { none {} } statement { managed_rule_group_statement { name = \u0026#34;AWSManagedRulesKnownBadInputsRuleSet\u0026#34; vendor_name = \u0026#34;AWS\u0026#34; } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \u0026#34;AWS-AWSManagedRulesKnownBadInputsRuleSet\u0026#34; sampled_requests_enabled = true } } rule { name = \u0026#34;AWS-AWSManagedRulesLinuxRuleSet\u0026#34; priority = 4 override_action { none {} } statement { managed_rule_group_statement { name = \u0026#34;AWSManagedRulesLinuxRuleSet\u0026#34; vendor_name = \u0026#34;AWS\u0026#34; } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \u0026#34;AWS-AWSManagedRulesLinuxRuleSet\u0026#34; sampled_requests_enabled = true } } rule { name = \u0026#34;AWS-AWSManagedRulesUnixRuleSet\u0026#34; priority = 5 override_action { none {} } statement { managed_rule_group_statement { name = \u0026#34;AWSManagedRulesUnixRuleSet\u0026#34; vendor_name = \u0026#34;AWS\u0026#34; } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \u0026#34;AWS-AWSManagedRulesUnixRuleSet\u0026#34; sampled_requests_enabled = true } } rule { name = \u0026#34;AWS-AWSManagedRulesSQLiRuleSet\u0026#34; priority = 6 override_action { none {} } statement { managed_rule_group_statement { name = \u0026#34;AWSManagedRulesSQLiRuleSet\u0026#34; vendor_name = \u0026#34;AWS\u0026#34; } } visibility_config { cloudwatch_metrics_enabled = true metric_name = \u0026#34;AWS-AWSManagedRulesSQLiRuleSet\u0026#34; sampled_requests_enabled = true } } visibility_config { cloudwatch_metrics_enabled = true metric_name = var.aws_apigw_wafv2_name sampled_requests_enabled = true } } # API GatewayへのWAFの割り当て resource \u0026#34;aws_wafv2_web_acl_association\u0026#34; \u0026#34;apigw\u0026#34; { resource_arn = var.aws_apigw_arn # or aws_api_gateway_stage.*.arn web_acl_arn = aws_wafv2_web_acl.wafv2_web_acl_regional.arn } 例3. IPホワイトリストを用いたアクセス制御 # 許可したIPアドレスからのみアクセスを許可するホワイトリストベースのアクセス制御を用いた例です。デフォルトアクションはブロックにして、ホワイトリストに合致した場合のみ許可するようにしています。\n# WAFのWeb ACL設定 resource \u0026#34;aws_wafv2_web_acl\u0026#34; \u0026#34;cloudfront\u0026#34; { provider = aws.us_east_1 name = var.aws_cloudfront_wafv2_name scope = \u0026#34;CLOUDFRONT\u0026#34; default_action { block {} } rule { name = \u0026#34;AllowedIPAddressList\u0026#34; priority = 0 action { allow {} } statement { ip_set_reference_statement { arn = aws_wafv2_ip_set.cloudfront.arn } } visibility_config { cloudwatch_metrics_enabled = false metric_name = \u0026#34;AllowedIPAddressList\u0026#34; sampled_requests_enabled = true } } visibility_config { cloudwatch_metrics_enabled = true metric_name = var.aws_cloudfront_wafv2_name sampled_requests_enabled = true } } # WAFのIP sets設定 resource \u0026#34;aws_wafv2_ip_set\u0026#34; \u0026#34;cloudfront\u0026#34; { provider = aws.us_east_1 name = var.aws_wafv2_ipset_name description = var.aws_wafv2_ipset_description addresses = var.aws_wafv2_ipset_addresses scope = \u0026#34;CLOUDFRONT\u0026#34; ip_address_version = \u0026#34;IPV4\u0026#34; } 例4. カスタムヘッダーを用いたアクセス制御 # CloudFrontなどで付与した特定のカスタムヘッダーが含まれていた場合にのみアクセスを許可するといったケースの例です。なお、カスタムヘッダー名と設定値はAWS Systems Manager Parameter Storeへ事前に格納しておき、Terraform実行時にdataブロックを用いて値を取得してくるようにしています。\n# WAFのWeb ACL設定 resource \u0026#34;aws_wafv2_web_acl\u0026#34; \u0026#34;apigw\u0026#34; { name = var.aws_apigw_wafv2_name scope = \u0026#34;REGIONAL\u0026#34; default_action { block {} } rule { name = \u0026#34;AllowedCustomHeader\u0026#34; priority = 0 action { allow {} } statement { byte_match_statement { field_to_match { single_header { name = data.aws_ssm_parameter.custom_header_name.value } } positional_constraint = \u0026#34;CONTAINS\u0026#34; search_string = data.aws_ssm_parameter.custom_header_value.value text_transformation { priority = 0 type = \u0026#34;NONE\u0026#34; } } } visibility_config { cloudwatch_metrics_enabled = true metric_name = var.aws_apigw_wafv2_name sampled_requests_enabled = true } } # SSMパラメータストアからのデータ取得 data \u0026#34;aws_ssm_parameter\u0026#34; \u0026#34;custom_header_name\u0026#34; { name = var.aws_ssmparam_custom_header_name } data \u0026#34;aws_ssm_parameter\u0026#34; \u0026#34;custom_header_value\u0026#34; { name = var.aws_ssmparam_custom_header_value } 例5. CloudWatch Logsを活用したロギング設定 # WAFログをCloudWatch Logsへ出力する際の例です。WAFログのロググループ名は\u0026quot;aws-waf-logs-*\u0026ldquo;とする必要があること、CloudFront向けWAFについてはus-east-1にすることを忘れがちなのでご注意ください。\nhttps://docs.aws.amazon.com/ja_jp/waf/latest/developerguide/logging-cw-logs.html#logging-cw-logs-naming\n# CloudFront向けWAFログ設定 resource \u0026#34;aws_wafv2_web_acl_logging_configuration\u0026#34; \u0026#34;cloudfront\u0026#34; { provider = aws.us_east_1 log_destination_configs = [aws_cloudwatch_log_group.cloudfront_waf.arn] resource_arn = aws_wafv2_web_acl.cloudfront.arn } # CloudFront向けWAFログ用のロググループ設定 resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;cloudfront_waf\u0026#34; { provider = aws.us_east_1 name = \u0026#34;aws-waf-logs-${var.aws_cloudfront_wafv2_name}\u0026#34; } # API Gateway向けWAFログ設定 resource \u0026#34;aws_wafv2_web_acl_logging_configuration\u0026#34; \u0026#34;apigw\u0026#34; { log_destination_configs = [aws_cloudwatch_log_group.apigw_waf.arn] resource_arn = aws_wafv2_web_acl.apigw.arn } # API Gateway向けWAFログ用のロググループ設定 resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;apigw_waf\u0026#34; { name = \u0026#34;aws-waf-logs-${var.aws_apigw_wafv2_name}\u0026#34; } 例6. S3バケットを活用したロギング設定 # WAFログをS3バケットへ出力する際の例です。WAFログを格納するS3バケット名は\u0026quot;aws-waf-logs-*\u0026ldquo;とする必要があることを忘れがちなのでご注意ください。また、CloudFront向けWAFのログ用S3バケットは、他のリソースとは異なりデフォルト側のプロバイダー設定を利用するのでこちらもご注意ください。\nhttps://docs.aws.amazon.com/ja_jp/waf/latest/developerguide/logging-s3.html#logging-s3-naming\n# CloudFront向けWAFログ設定 resource \u0026#34;aws_wafv2_web_acl_logging_configuration\u0026#34; \u0026#34;cloudfront\u0026#34; { provider = aws.us_east_1 log_destination_configs = [aws_s3_bucket.cloudfront_waf_log.arn] resource_arn = aws_wafv2_web_acl.cloudfront.arn } # CloudFront向けWAFログ用のログバケット設定（詳細は割愛） resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;cloudfront_waf_log\u0026#34; { bucket = \u0026#34;aws-waf-logs-${var.aws_cloudfront_wafv2_name}\u0026#34; # 以下省略 } # API Gateway向けWAFログ設定 resource \u0026#34;aws_wafv2_web_acl_logging_configuration\u0026#34; \u0026#34;apigw\u0026#34; { log_destination_configs = [aws_s3_bucket.apigw_waf_log.arn] resource_arn = aws_wafv2_web_acl.apigw.arn } # API Gateway向けWAFログ用のログバケット設定（詳細は割愛） resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;apigw_waf_log\u0026#34; { bucket = \u0026#34;aws-waf-logs-${var.aws_apigw_wafv2_name}\u0026#34; # 以下省略 } 終わりに # 今回はTerraformの入門ということで、CloudFrontおよびAPI Gatewayに割り当てるAWS WAFのサンプルコードをいくつかご紹介してきましたがいかがだったでしょうか。こんな記事でも誰かの役に立っていただけるのであれば幸いです。\nなお、今回ご紹介したコードはあくまでサンプルであり、動作を保証するものではございません。そのまま使用したことによって発生したトラブルなどについては一切責任を負うことはできませんのでご注意ください。\nAWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 Terraform は、HashiCorp, Inc. の米国およびその他の国における商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"December 13, 2022","permalink":"/posts/2022/12/aws-waf-sample-terraform-code/","section":"記事一覧","summary":"みなさん、こんにちは。今回はTerraformの入門ということでCloudFrontおよびAPI Gatewayに割り当てるAWS WAFのサンプルコードを書いてみましたのでこちらを紹介していきたいと思います。","title":"TerraformでAWS WAFを構築するサンプルコードを書いてみた"},{"content":"みなさん、こんにちは。AWSを利用するにあたってタグの付与は、運用管理を容易にするなどの観点から一般的に行われている設定かと思います。今回はTerraform入門としてそんなAWSタグをTerraformで付与する際の小ネタのご紹介です。\n基本的なタグの設定方法 # 基本的な設定方法としてはAWSプロバイダーの定義にて、デフォルトで付与するタグを設定することです。これにより、すべてのAWSリソースへ漏れることなく付与することが可能なのでこちらを活用するのがオススメです。\nprovider \u0026#34;aws\u0026#34; { region = var.aws_region default_tags { tags = { owner = \u0026#34;matt\u0026#34; terraform = \u0026#34;true\u0026#34; } } } 複数パターンのタグ設定方法 # 「基本的なタグの設定方法」では1つのパターンのみでしたが、たとえば環境を表すタグ情報などをちょっと変えたものをいくつかグルーピングしたい、といったケースもあるかと思います。そのような場合はグループごとにプロバイダー定義を用意して、各リソース定義にてプロバイダーを指定することによって期待通りの動作をさせることができます。\nprovider \u0026#34;aws\u0026#34; { alias = \u0026#34;dev\u0026#34; region = var.aws_region default_tags { tags = { owner = \u0026#34;matt\u0026#34; environment = \u0026#34;dev\u0026#34; terraform = \u0026#34;true\u0026#34; } } } provider \u0026#34;aws\u0026#34; { alias = \u0026#34;prod\u0026#34; region = var.aws_region default_tags { tags = { owner = \u0026#34;matt\u0026#34; environmemt = \u0026#34;prod\u0026#34; terraform = \u0026#34;true\u0026#34; } } } resource \u0026#34;aws_xxxx_xxxx\u0026#34; \u0026#34;dev\u0026#34; { provider = aws.dev # 以下、省略 } resource \u0026#34;aws_xxxx_xxxx\u0026#34; \u0026#34;prod\u0026#34; { provider = aws.prod # 以下、省略 } リソース個別タグの設定方法 # リソースの名前タグなどのリソースごとに異なる値となるタグについては、各リソース定義のtagsブロック内で定義します。なお、プロバイダー設定で行ったタグ設定と同じキーのタグを、各リソース定義のtagsブロックにて指定することによって値を上書きすることも可能です。\nresource \u0026#34;aws_xxxx_xxxx\u0026#34; \u0026#34;sample\u0026#34; { name = var.resource_name # 途中、省略 tags = { name = var.resource_name } } モジュールへのタグ設定の継承方法 # モジュールへタグ設定を継承する場合は、providersブロックを用いて継承したいタグ設定を行ったプロバイダーを指定することで可能です。なお、インターネット上にはモジュール側に空のプロバイダーブロックを継承したプロバイダーの分定義するといったサンプルもありますが、そちらは古いフォーマットなのでご注意ください。\nprovider \u0026#34;aws\u0026#34; { region = var.aws_region default_tags { tags = { owner = \u0026#34;matt\u0026#34; workload = \u0026#34;common\u0026#34; terraform = \u0026#34;true\u0026#34; } } } provider \u0026#34;aws\u0026#34; { alias = \u0026#34;web\u0026#34; region = var.aws_region default_tags { tags = { owner = \u0026#34;matt\u0026#34; workload = \u0026#34;web\u0026#34; terraform = \u0026#34;true\u0026#34; } } } module \u0026#34;sample\u0026#34; { source \u0026#34;../../modules/sample\u0026#34; providers = { aws = aws aws.web = aws.web } # 以下、省略 } terraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; configuration_aliases = [ aws, aws.web, ] } } } resource \u0026#34;aws_xxxx_xxxx\u0026#34; \u0026#34;default\u0026#34; { # 以下、省略 } resource \u0026#34;aws_xxxx_xxxx\u0026#34; \u0026#34;web\u0026#34; { provider = aws.web # 以下、省略 } 終わりに # 普段からTerraformを触っている人からすると「なにをいまさら…」といった情報かもしれませんが、こんな記事でもだれかの助けになれれば幸いです。以上、Terraformで管理するAWSリソースへタグ情報を付与する際の小ネタの紹介でした。\nAWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 Terraform は、HashiCorp, Inc. の米国およびその他の国における商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"December 13, 2022","permalink":"/posts/2022/12/aws-tagging-using-terraform/","section":"記事一覧","summary":"みなさん、こんにちは。AWSを利用するにあたってタグの付与は、運用管理を容易にするなどの観点から一般的に行われている設定かと思います。今回はTerraform入門としてそんなAWSタグをTerraformで付与する際の小ネタのご紹介です。","title":"Terraformで管理するAWSリソースへタグ情報を付与する際の小ネタ紹介"},{"content":"みなさん、こんにちは。AWS WAFを有効化したCloudFrontやAPI Gatewayに対する8KB以上のリクエストがブロックされてしまうといった事象に遭遇したので、今回はこちらの事象の原因と解決方法について紹介していきたいと思います。\nリクエストがブロックされた原因 # WAFのログを確認すると、そのままズバリAWS WAFのコアルールセット(CRS)マネージドルールグループ AWSManagedRulesCommonRuleSetのSizeRestrictions_BODYルールに合致したため8KB(8,192バイト)を超えるBodyを持つリクエストがブロックされていることがわかりました。\n余談ですが、なぜ8KBという制限が存在するかというとそもそもAWS WAFで検査できるBodyは最初の8KBという制限があります(おそらく性能の観点で)。そのため、8KBを超えるBodyを持つ(=全文検査ができない)リクエストをデフォルトでブロックしているのではないか、と個人的には考えております。\n解決方法 # 次の例のようにSizeRestrictions_BODYルールのデフォルトアクションをBlockからCountへ変更することによって、8KBを超えるBodyを持つリクエストがブロックされず、ちゃんと扱えるようになりました。めでたしめでたし。\n余談、Terraformで設定する場合は # managed_rule_group_statementブロック内のexcluded_ruleブロックへ除外したいルールの名前を定義することで実装することが可能です。本定義を利用したサンプルコードを書いてみましたので興味のある方は次の記事をご参照いただければと思います。\n終わりに # API Gatewayを扱ってきた方からするとなにを今更いってんだか…といった内容かもしれませんが、こんな情報でも誰かの役に経っていただければ幸いです。なお、今回ご紹介したマネージドルールを抜きにしても、AWS WAFにはBody以外にもHeaderやCookieにもサイズや数の制限がありますので、それを超えるリクエストを扱う場合は次のサイトを参考にしていただければと思います。\n以上、AWS WAFを有効化したCloudFrontやAPI Gatewayに対する8KBを超えるリクエストがブロックされた事象の原因と解決方法のご紹介でした。\nAWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"December 12, 2022","permalink":"/posts/2022/12/aws-waf-blocked-over-8kb-requests/","section":"記事一覧","summary":"みなさん、こんにちは。AWS WAFを有効化したCloudFrontやAPI Gatewayに対する8KB以上のリクエストがブロックされてしまうといった事象に遭遇したので、今回はこちらの事象の原因と解決方法について紹介していきたいと思います。","title":"AWS WAFを有効化したCloudFrontやAPI Gatewayに対する8KBを超えるリクエストに失敗した件について"},{"content":"","date":"December 12, 2022","permalink":"/tags/azure-virtual-desktopavd/","section":"タグ一覧","summary":"","title":"Azure Virtual Desktop(AVD)"},{"content":"みなさん、こんにちは。AWS WAFを有効化したCloudFrontやAPI Gatewayに対するアクセスがAzure Virtual Desktop(AVD)からだとブロックされてしまうといった事象に遭遇したので、今回はこちらの事象の原因と解決方法について紹介していきたいと思います。\nアクセスがブロックされた原因 # WAFのログを確認すると、AWS WAFの匿名IPリストマネージドルールグループ AWSManagedRulesAnonymousIpListのHostingProviderIPListルールに合致したためAVDからのアクセスがブロックされていることがわかりました。うーむ、現状だとAzure Virtual Desktopはエンドユーザトラフィックのソースになる可能性が低いと判断されてしまっているということですね…^^;\n解決方法 # ホワイトリストを作る(特定のIPアドレスだけ許可する)などのいくつか解決方法はありますが、ホスティングサービスやクラウドサービスを活用して仮想デスクトップを構築・利用しているケースなどもそれなりにいることを考慮すると、このルールで無条件ブロックしていたホスティングサービスやクラウドサービスからの攻撃については別の異なるルールで保護することを前提に、デフォルトアクションをBlockからCountへ変更してしまうという判断もありかと思います。\nということで、次の例のようにデフォルトアクションを変更することで、Azure Virtual Desktopからのアクセスがブロックされないようになりました。めでたしめでたし。\n余談、Terraformで設定する場合は # managed_rule_group_statementブロック内のexcluded_ruleブロックへ除外したいルールの名前を定義することで実装することが可能です。本定義を利用したサンプルコードを書いてみましたので興味のある方は次の記事をご参照いただければと思います。\n終わりに # 今回はたまたまクライアントとしてAzure Virtual Desktopを利用していましたが、マネージドルールの内容を見るかぎり、AzureやGoogle Cloudなどのクラウドサービス全般からのアクセスはことごとくブロックされてしまう可能性があるようにも読めます。もしAWS上のWEBサービスに対して特定ユーザからのアクセスができないようで困っています、といった方は可能性の1つとして同件か疑ってみても良いのかなと思います。\n以上、Azure Virtual DesktopからAWSのCloudFrontやAPI Gatewayへのアクセスがブロックされた事象の原因と解決方法のご紹介でした。\nAWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 Microsoft Azure は，Microsoft Corporation の商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"December 12, 2022","permalink":"/posts/2022/12/aws-waf-blocked-requests-from-other-cloud/","section":"記事一覧","summary":"みなさん、こんにちは。AWS WAFを有効化したCloudFrontやAPI Gatewayに対するアクセスがAzure Virtual Desktop(AVD)からだとブロックされてしまうといった事象に遭遇したので、今回はこちらの事象の原因と解決方法について紹介していきたいと思います。","title":"Azure Virtual DesktopからAWSのCloudFrontやAPI Gatewayへのアクセスがブロックされた件について"},{"content":"","date":"December 8, 2022","permalink":"/tags/amazon-s3/","section":"タグ一覧","summary":"","title":"Amazon S3"},{"content":"","date":"December 8, 2022","permalink":"/tags/elastic-load-balancingelb/","section":"タグ一覧","summary":"","title":"Elastic Load Balancing(ELB)"},{"content":"","date":"December 8, 2022","permalink":"/tags/nginx/","section":"タグ一覧","summary":"","title":"Nginx"},{"content":"みなさん、こんにちは。Nginxでリバースプロキシを立ててパスベースルーティングなどでALBやS3などのURLへ振り分け設定をしていたところ、Nginxが499を返してくるという事象が発生したので、今回はこちらの事象の原因と解決方法について紹介していきたいと思います。\n499が発生した原因 # 事象発生時のリバースプロキシ設定は次のような定義となっていたのですが、この定義だとNginxのproxy_passに指定しているURLの名前解決はNginx起動時のみに行われ、それ以降はそのIPアドレスを使い続けるという挙動となってしまうことがわかりました。\nserver { : (省略) : location /api/ { proxy_pass http://XXXXXXXX.ap-northeast-1.elb.amazonaws.com/; } } そのため、何らかの要因でALBやS3などのIPアドレスに変更が生じたものの、それにNginxが追従できずに古いIPアドレスへの転送を続けてしまったため499を返してくるという事象へと至っておりました。\n解決方法 # 名前解決をNginx起動時のみにしていることが原因だったため、次の例のようにresolverディレクティブを定義し、validオプションを使って適宜キャッシュの時間を調整することによって解決することができました。めでたしめでたし。\nserver { : (省略) : location ~ /api/(.*) { resolver ${NAMESERVER} valid=30s; set $endpoint_url http://XXXXXXXX.ap-northeast-1.elb.amazonaws.com/; proxy_pass $endpoint_url$1$is_args$args; } } setで変数化している理由ですが、proxy_passへは変数で指定しないと名前解決をしに行かない、といった事例があったためsetにて変数化を行っています。 locationおよびproxy_passの指定値が変わった理由ですが、setで変数化したことによって/api/以降の文字列がすべてなかったことにされてしまう挙動となったため、それを解決するためにlocationおよびproxy_passの指定値を変更しています。 終わりに # Nginxでリバースプロキシを立てていて原因不明の499エラーがでて困ってます、といった方はぜひ参考にしてみてはいかがでしょうか。以上、NginxリバースプロキシでALBやS3などへのルーティング設定をしていたときに発生した499エラーの原因と解決方法のご紹介でした。\nAWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 NGINX は NGINX, Inc. の登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"December 8, 2022","permalink":"/posts/2022/12/aws-nginx-proxy-returns-499-response/","section":"記事一覧","summary":"みなさん、こんにちは。Nginxでリバースプロキシを立ててパスベースルーティングなどでALBやS3などのURLへ振り分け設定をしていたところ、Nginxが499を返してくるという事象が発生したので、今回はこちらの事象の原因と解決方法について紹介していきたいと思います。","title":"NginxリバースプロキシでALBやS3などへのルーティング設定をした際に499エラーが発生した件について"},{"content":"みなさん、こんにちは。今回は既存のリソースをterraform importコマンドを使ってTerraform管理下へインポートする際の小ネタの紹介です。\nTerraformで実装する際、たとえば複数のリージョンやアベイラビリティゾーンに同様のリソースを配置する際などのユースケースにおいてcountやfor_eachを活用されている方も多いかと思います。このcountやfor_eachを活用して定義したリソースに対して、既存環境を素直にTerraform管理下へインポートしようとすると次のようにコマンドがエラーになってしまいます。今回はそんなときの解決方法を紹介したいと思います。\n$ terraform import google_compute_subnetwork.private[\u0026#34;tokyo\u0026#34;] \\ ${GOOGLE_PROJECT}/asia-northeast1/matt-private-snet-tokyo ╷ │ Error: Index value required │ │ on \u0026lt;import-address\u0026gt; line 1: │ 1: google_compute_subnetwork.private[tokyo] │ │ Index brackets must contain either a literal number or a literal string. ╵ For information on valid syntax, see: https://www.terraform.io/docs/cli/state/resource-addressing.html countやfor_eachで定義したリソースをimportする方法 # 結論から言いますと 「\u0026rsquo;(シングルクォーテーション)で定義したリソース名を囲む」 が1つの解となります。\n出力例のように、Terraformリソース名を\u0026rsquo;(シングルクォーテーション)で囲んであげると期待通りに既存のリソースがインポートされる動きとなります。めでたしめでたし。\n$ terraform import \u0026#39;google_compute_subnetwork.private[\u0026#34;tokyo\u0026#34;]\u0026#39; \\ ${GOOGLE_PROJECT}/asia-northeast1/matt-private-snet-tokyo Acquiring state lock. This may take a few moments... Import successful! The resources that were imported are shown above. These resources are now in your Terraform state and will henceforth be managed by Terraform. Releasing state lock. This may take a few moments... 余談、ウッカリ違うリソースへimportしてしまった時は # そんなときは一旦ウッカリimportしてしまったリソースをTerraform管理下から外し、正しいリソースへimportしなおしてください。Terraform管理下から外す際の手順は、次の記事を参照していただければと思います。\n終わりに # いまさらの情報でしたがいかがだったでしょうか。グーグル先生に聞けば一発かも知れませんが、わが身にも起きた事象だったので一応記事にしてみた次第でした。こんな記事でも誰かの手助けになれば幸いです。\n以上、既存環境をterraform importコマンドを使ってTerraform管理下へインポートする際の小ネタの紹介でした。\nTerraform は、HashiCorp, Inc. の米国およびその他の国における商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"December 6, 2022","permalink":"/posts/2022/12/terraform-import-resources-using-for-each/","section":"記事一覧","summary":"みなさん、こんにちは。今回は既存のリソースをterraform importコマンドを使ってTerraform管理下へインポートする際の小ネタの紹介です。","title":"Terraformでcountやfor_eachを使って定義したリソースをimportする方法"},{"content":"みなさん、こんにちは。今回は既存のリソースをterraform state rmコマンドを使ってTerraform管理下から外す際の小ネタの紹介です。\nTerraformで実装する際、たとえば複数複数のリージョンやアベイラビリティゾーンに同様のリソースを配置する際などのユースケースにおいてcountやfor_eachを活用されている方も多いかと思います。このcountやfor_eachを活用して定義したリソースに対して素直にterraform state rmコマンドを実行すると次のようにコマンドがエラーになってしまいます。今回はそんなときの解決方法を紹介したいと思います。\n$ terraform state rm google_compute_subnetwork.private[\u0026#34;tokyo\u0026#34;] Acquiring state lock. This may take a few moments... ╷ │ Error: Index value required │ │ on line 1: │ (source code not available) │ │ Index brackets must contain either a literal number or a literal string. ╵ Releasing state lock. This may take a few moments... countやfor_eachで定義したリソースを管理外にする方法 # 結論から言いますと 「\u0026rsquo;(シングルクォーテーション)で定義したリソース名を囲む」 が1つの解となります。\n出力例のように、Terraformリソース名を\u0026rsquo;(シングルクォーテーション)で囲んであげると期待通りに既存のリソースがTerraform管理外になる動きとなります。めでたしめでたし。\n$ terraform state rm \u0026#39;google_compute_subnetwork.private[\u0026#34;tokyo\u0026#34;]\u0026#39; Acquiring state lock. This may take a few moments... Removed google_compute_subnetwork.private[\u0026#34;tokyo\u0026#34;] Successfully removed 1 resource instance(s). Releasing state lock. This may take a few moments... 終わりに # いまさらの情報でしたがいかがだったでしょうか。「え、こんなユースケースなくない？」と思われるかもしれませんが、「ついつい、うっかり既存環境を間違って違うリソースにインポートしちゃいました」といったケースもあるかと思います。そんなときの手助けになれば幸いです。\n以上、terraform state rmコマンドを使ってTerraform管理下から外す際の小ネタの紹介でした。\nTerraform は、HashiCorp, Inc. の米国およびその他の国における商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"December 6, 2022","permalink":"/posts/2022/12/terraform-state-rm-resources-using-for-each/","section":"記事一覧","summary":"みなさん、こんにちは。今回は既存のリソースをterraform state rmコマンドを使ってTerraform管理下から外す際の小ネタの紹介です。","title":"Terraformでcountやfor_eachを使って定義したリソースをTerraform管理下から外す方法"},{"content":"","date":"April 28, 2022","permalink":"/tags/cloud-spanner/","section":"タグ一覧","summary":"","title":"Cloud Spanner"},{"content":"みなさん、こんにちは。昨今、インフラのコード化(IaC)を導入する事例はとても多くなってきていると思いますが、その一方で開発の初期段階からしっかりコード化をしようと思うとメンバーのスキルセットによってはスピード感が…と悩まれている方もいらっしゃるのではないでしょうか。\n今回はそんな悩みに対する1つの解として、開発の初期段階にサクッと手で作った既存のGoogle Cloud環境からTerraformコードを自動生成する方法について紹介していきたいと思います。\nTerraformコードを自動生成してみた # Terraformコードの自動生成方法としてはいくつか方法はあるのですが、今回はGoogle Cloud CLIのgcloud beta resource-config bulk-exportコマンドを使ってみました。本コマンドを利用することで既存環境まるっと一括でコード化することも、特定サービスのリソースのみをコード化することも可能です。\n例1. 既存環境のすべてをコード化する場合 # 既存環境のすべてをTerraformコード化する場合は、次のコマンドで自動生成することが可能です。自動生成してみた率直な意見としては、出力されたコード群のディレクトリ構成がサービス毎にバラバラで統一感がないなーでした^^;\ngcloud beta resource-config bulk-export \\ --resource-format=terraform --path=\u0026lt;出力ディレクトリ\u0026gt; 例2. 特定サービスのリソースのみをコード化する場合 # 特定のサービスのみをTerraformコード化する場合は、次のコマンドで自動生成することが可能です。なお、複数のリソース種別を「,(カンマ)」でつなげて列挙することでまとめて出力することも可能です。\ngcloud beta resource-config bulk-export \\ --resource-format=terraform --path=\u0026lt;出力ディレクトリ\u0026gt; \\ --project=\u0026lt;プロジェクト名\u0026gt; --resource-types=\u0026lt;リソース種別名\u0026gt; なお、--resource-typesオプションに指定することができるbulk-exportサポート対象のリソース種別名については、次の例のようにgcloud beta resource-config list-resource-typesコマンドで取得することが可能です。\ngcloud beta resource-config list-resource-types --format=yaml 次の出力例を見ていただくといろいろと出力があるのですが、--resource-typesオプションに指定する値はKindの部分(=例ではArtifactRegistryRepository)となるのでご留意いただければと思います。\n$ gcloud beta resource-config list-resource-types --format=yaml : --- GVK: Group: artifactregistry.cnrm.cloud.google.com Kind: ArtifactRegistryRepository Version: v1beta1 ResourceNameFormat: //artifactregistry.googleapis.com/projects/{{project}}/locations/{{location}}/repositories/{{repository_id}} SupportsBulkExport: true SupportsExport: true SupportsIAM: true --- : 終わりに # 今回は手動で構築した既存のGoogle Cloud環境からサクッとTerraformコードを自動生成する方法を紹介してきましたがいかがだったでしょうか。こんな記事でも誰かの役に立っていただけるのであれば幸いです。\nなお、自動生成されたコードは設定値がハードコーディングされているため、そのままでは他の環境で使いまわせるような代物ではありません。他の環境でも使いまわせるコードを目指すのであれば、べた書きされた設定値を変数化したりなどのリファクタリングが必要となりますのでご注意ください。\nとはいえ、リソース定義を書く際の参考としては十分に利用価値はあるかと思いますので、Terraformのコード開発を少しでも楽にする手段の1つとして十分ありなのかなと個人的には思っています^^\nGoogle Cloud は、Google LLC の商標または登録商標です。 Terraform は、HashiCorp, Inc. の米国およびその他の国における商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"April 28, 2022","permalink":"/posts/2022/04/gcp-auto-generate-terraform-code-from-current-resources/","section":"記事一覧","summary":"みなさん、こんにちは。昨今、インフラのコード化(IaC)を導入する事例はとても多くなってきていると思いますが、その一方で開発の初期段階からしっかりコード化をしようと思うとメンバーのスキルセットによってはスピード感が…と悩まれている方もいらっしゃるのではないでしょうか。","title":"Google Cloudに手動で構築した環境からTerraformコードを自動生成してみた"},{"content":"みなさん、こんにちは。今回はTerraformの入門ということでCloud Spannerのサンプルコードを書いてみましたのでこちらを紹介していきたいと思います。\nなお、サンプルコードを書いた際のTerraformおよびGoogleプロバイダーのバージョンは次のとおりです。最新バージョンでは定義方法が異なっている可能性があるため、実際にコードを書く際は最新の「Terraformドキュメント」と「Googleプロバイダードキュメント」を確認しながら開発を進めていただければと思います。\n# Requirements terraform { required_version = \u0026#34;1.3.4\u0026#34; required_providers { google = { source = \u0026#34;hashicorp/google\u0026#34; version = \u0026#34;4.46.0\u0026#34; } } } サンプルコードを書いてみた # 今回は次のような構成のサンプルコードを書いてみました。なお、変数定義部分などの一部省略している点、ならびにステップごとの細かい説明などは省いていますのでご承知おきください。詳細については「Googleプロバイダードキュメント」をご参照ください。\n例1. 東京/大阪マルチリージョンインスタンスの例 例2. Google Standard SQL言語データベースの例 例3. PostgreSQL言語データベースの例 例4. 顧客管理暗号化鍵(CMEK)を利用したデータベースの例 ちなみに、すべてのサンプルコードに共通してプロバイダー定義は次のようにしています。\n# デフォルトのプロバイダー設定 provider \u0026#34;google\u0026#34; { project = var.project_id region = \u0026#34;asia-northeast1\u0026#34; zone = \u0026#34;asia-northeast1-b\u0026#34; } 例1. 東京/大阪マルチリージョンインスタンスの例 # Cloud Spannerのマルチリージョンインスタンスの例です。なお、コンピューティング容量の割り当て単位としては、今回の例で採用している処理ユニットベース以外にノード単位でも指定できますが、どちらで指定しても結果に大差はないのでノード単位の例は割愛します。\n# asia1マルチリージョンのSpannerインスタンス定義 resource \u0026#34;google_spanner_instance\u0026#34; \u0026#34;sample\u0026#34; { display_name = \u0026#34;my-sample-spanner\u0026#34; # 4-30 characters config = \u0026#34;asia1\u0026#34; processing_units = 100 # 1000 == 1 node force_destroy = var.force_destry_enabled ? true : false } 例2. Google Standard SQL言語データベースの例 # Google Standard SQL言語ベースのデータベースを作成する例です。database_dialectオプションはデフォルト値がGOOGLE_STANDARD_SQLなので省略可能なのですが、個人的にはここの部分は明示したい気持ちに駆られるのであえて書いてます^^; version_retention_periodオプションについても同様で、要件にあわせて変える部分だと思うのでここもあえて書いてみました。\n# Google Standard SQL言語ベースのデータベース定義 resource \u0026#34;google_spanner_database\u0026#34; \u0026#34;google\u0026#34; { instance = google_spanner_instance.sample.id name = \u0026#34;my-googlesql-db\u0026#34; # 2-30 characters database_dialect = \u0026#34;GOOGLE_STANDARD_SQL\u0026#34; version_retention_period = \u0026#34;3d\u0026#34; # Default 1h, Max 7d deletion_protection = var.deletion_protection_enabled ? true : false } 例3. PostgreSQL言語データベースの例 # PostgreSQL言語ベースのデータベースを作成する例です。「例2」とほぼ同じなので特筆すべき点はないかと思います。\n# PostgreSQL言語ベースのデータベース定義 resource \u0026#34;google_spanner_database\u0026#34; \u0026#34;postgre\u0026#34; { instance = google_spanner_instance.sample.id name = \u0026#34;my-postgresql-db\u0026#34; # 2-30 characters database_dialect = \u0026#34;POSTGRESQL\u0026#34; version_retention_period = \u0026#34;24h\u0026#34; # Default 1h, Max 7d deletion_protection = var.deletion_protection_enabled ? true : false } 例4. 顧客管理暗号化鍵(CMEK)を利用したデータベースの例 # 法令順守などのために顧客が管理する暗号鍵を利用したデータベースを作成する際の例です。dataブロック定義を使って作成済みのCloud KMSリソースのIDを取得し、それを指定するようにしています。なお、今回はCloud Spanner用サービスアカウントに特定の暗号化鍵に対して個別にアクセス権を付与するようにしています。\n# Cloud KMSで管理する顧客管理暗号鍵(CMEK)で暗号化したデータベース定義 resource \u0026#34;google_spanner_database\u0026#34; \u0026#34;google\u0026#34; { instance = google_spanner_instance.sample[0].name name = \u0026#34;my-cmek-encrypted-db\u0026#34; # 2-30 characters database_dialect = \u0026#34;GOOGLE_STANDARD_SQL\u0026#34; version_retention_period = \u0026#34;3d\u0026#34; encryption_config { kms_key_name = data.google_kms_crypto_key.spanner.id } deletion_protection = var.deletion_protection_enabled ? true : false } # Cloud KMSで管理する顧客管理暗号鍵(CMEK)を利用するための権限付与 resource \u0026#34;google_kms_crypto_key_iam_member\u0026#34; \u0026#34;crypto_key\u0026#34; { crypto_key_id = data.google_kms_crypto_key.spanner.id role = \u0026#34;roles/cloudkms.cryptoKeyEncrypterDecrypter\u0026#34; member = \u0026#34;serviceAccount:service-${data.google_project.this.number}@gcp-sa-spanner.iam.gserviceaccount.com\u0026#34; } # プロジェクトリソース取得 data \u0026#34;google_project\u0026#34; \u0026#34;this\u0026#34; {} # キーリングリソース取得 data \u0026#34;google_kms_key_ring\u0026#34; \u0026#34;asia1\u0026#34; { name = var.gcp_kms_keyring_name location = \u0026#34;asia1\u0026#34; } # 暗号化キーリソース取得 data \u0026#34;google_kms_crypto_key\u0026#34; \u0026#34;spanner\u0026#34; { name = var.gcp_kms_key_nameGoogle key_ring = data.google_kms_key_ring.asia1.id } 終わりに # 今回はTerraformの入門ということで、Cloud Spannerのサンプルコードをいくつかご紹介してきましたがいかがだったでしょうか。こんな記事でも誰かの役に立っていただけるのであれば幸いです。\nなお、今回ご紹介したコードはあくまでサンプルであり、動作を保証するものではございません。そのまま使用したことによって発生したトラブルなどについては一切責任を負うことはできませんのでご注意ください。\nGoogle Cloud は、Google LLC の商標または登録商標です。 Terraform は、HashiCorp, Inc. の米国およびその他の国における商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"April 28, 2022","permalink":"/posts/2022/04/gcp-cloud-spanner-sample-terraform-code/","section":"記事一覧","summary":"みなさん、こんにちは。今回はTerraformの入門ということでCloud Spannerのサンプルコードを書いてみましたのでこちらを紹介していきたいと思います。","title":"TerraformでCloud Spannerを構築するサンプルコードを書いてみた"},{"content":"みなさん、こんにちは。今回は2022年3月14日に発表されたGoogle Cloudの価格改定についてのお話です。一部のニュースメディアでは「大幅値上げ」と取り上げられていますが、実際のところ「日本での利用にあたって」どの程度影響がありそうかを見やすくしてみましたのでご紹介していきたいと思います。\nCloud Storage(GCS)の価格改定 # No. 項目 値上げ/値下げ 1 下り（外向き）の通信に対する無料枠の拡大(1GB ⇒ 100GB) 無料化⬇️\n(MAX$0.23/GB\t⇒ $0) 2 マルチリージョンのNearline Storage料金の改定 50%UP⬆️\n($0.01/GB ⇒ $0.015) 3 マルチリージョン(asia)のColdline Storage料金の改定 25%UP⬆️\n($0.007/GB ⇒ $0.00875) 5 マルチリージョン(asia)のArchive Storage料金の改定 25%DOWN⬇️\n($0.004/GB ⇒ $0.003) 7 デュアルリージョン(asia1)のStandard Storage料金の改定 10%UP⬆️\n($0.046/GB ⇒ $0.0506) 9 デュアルリージョン(asia1)のNearline Storage料金の改定 10%UP⬆️\n($0.032/GB ⇒ $0.0352) 11 デュアルリージョン(asia1)のColdline Storage料金の改定 10%UP⬆️\n($0.012/GB ⇒ $0.0132) 13 デュアルリージョン(asia1)のArchive Storage料金の改定 13%DOWN⬇️\n($0.0065/GB ⇒ $0.0056) 14 Coldline StorageクラスBオペレーション料金の改定 100%UP⬆️\n($0.05 ⇒ $0.1) 15 Coldline StorageクラスAオペレーション料金の改定\n(シングルリージョンの場合) 100%UP⬆️\n($0.1 ⇒ $0.2) 16 Coldline StorageクラスAオペレーション料金の改定\n(マルチリージョン/デュアルリージョンの場合) 300%UP⬆️\n($0.1 ⇒ $0.4) 17 上記以外のクラスAオペレーション料金の改定\n(マルチリージョン/デュアルリージョンの場合) 100%UP⬆️\n($0.05 ⇒ $0.1) 17 asia/asia1ロケーションのデータレプリケーション料金の有料化 有料化⬆️\n($0 ⇒ $0.08/GB) Cloud Load Balancingの価格改定 # No. 項目 値上げ/値下げ 1 ロードバランサで処理された下り（外向き）データの有料化 有料化⬆️\n($0 ⇒ MAX$0.012/GB) Network Intelligence Centerの価格改定 # No. 項目 値上げ/値下げ 1 ネットワークトポロジおよびパフォーマンスダッシュボード機能の有料化 有料化⬆️\n($0 ⇒ $0.0011/h) Compute Engine(GCE)の価格改定 # No. 項目 値上げ/値下げ 1 永続ディスクのリージョンスナップショット料金の改定 92%UP⬆️\n($0.026/GB ⇒ $0.05/GB) 2 〃　のマルチリージョンスナップショット料金の改定 150%UP⬆️\n($0.026/GB ⇒ $0.065/GB) 3 〃　のリージョンアーカイブスナップショット機能の新設 26%DOWN⬇️\n($0.026/GB ⇒ $0.019/GB) 4 〃　のマルチリージョンアーカイブスナップショット機能の新設 7%DOWN⬇️\n($0.026/GB ⇒ $0.024/GB) 終わりに # 今回は他のメガクラウドとの料金比較まではしていないため、市場に対する割高感があるかまでは何とも言えないものの、既存のGoogle Cloudユーザから見るとたしかにメディアで語られているとおり大幅な値上げと言われても仕方ないように感じますね^^;\nとはいえ、ほんの一部ではありますがArchive Storageやアーカイブスナップショットといった既存よりも安くなってくる部分もあるため、前向きにこれを良い機会だととらえてデータのライフサイクルを見直し、これらの長期保管用サービスの活用をもっと推進していけば今よりコストをおさえることも可能なのではないでしょうか。\n以上、2022年10月1日に施行されるGoogle Cloudの価格改定についてのお話でした。\nGoogle Cloud は、Google LLC の商標または登録商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。 ","date":"April 13, 2022","permalink":"/posts/2022/04/gcp-pricing-change-in-oct-2022/","section":"記事一覧","summary":"みなさん、こんにちは。今回は2022年3月14日に発表されたGoogle Cloudの価格改定についてのお話です。一部のニュースメディアでは「大幅値上げ」と取り上げられていますが、実際のところ「日本での利用にあたって」どの程度影響がありそうかを見やすくしてみましたのでご紹介していきたいと思います。","title":"2022年10月1日に施行されるGoogle Cloudの価格改定について"},{"content":"","date":"April 13, 2022","permalink":"/tags/cloud-load-balancing/","section":"タグ一覧","summary":"","title":"Cloud Load Balancing"},{"content":"","date":"April 13, 2022","permalink":"/tags/cloud-storage/","section":"タグ一覧","summary":"","title":"Cloud Storage"},{"content":"","date":"April 13, 2022","permalink":"/tags/google-compute-enginegce/","section":"タグ一覧","summary":"","title":"Google Compute Engine(GCE)"},{"content":"","date":"April 13, 2022","permalink":"/tags/network-intelligence-center/","section":"タグ一覧","summary":"","title":"Network Intelligence Center"},{"content":"みなさん、こんにちは。Google Cloudのサブネットにはソフトウェア的に割り当てを制限されてはいないものの、実際に割り当ててしまうと通信障害などを起こしてしまう割り当てはいけないIPv4アドレス範囲があるのをご存じでしょうか。\n今回は意外と見落としがちなGoogle Cloudのサブネットで有効なIPv4アドレス範囲、および範囲外のアドレスを割り当ててしまった際にはこんな事象が起きましたよ、という実体験について紹介していきたいと思います。\n有効なIPv4アドレス範囲とは # それでは早速ですが、次の一覧（公式ドキュメントより抜粋）が有効なIPv4アドレス範囲の答えとなっています。なお、特段の要件がない限りは、基本的に★マークをつけた「RFC1918」に準拠する形でサブネットの設計をしていただくのが無難かと思います。\n有効なIPv4アドレス範囲 ソース 説明 ★ 10.0.0.0/8\n172.16.0.0/12\n192.168.0.0/16 RFC1918 プライベートIPアドレス 100.64.0.0/10 RFC6598 共有アドレス空間 192.0.0.0/24 RFC6890 IETF プロトコルの割り当て 192.0.2.0/24 (TEST-NET-1)\n198.51.100.0/24 (TEST-NET-2)\n203.0.113.0/24 (TEST-NET-3) RFC5737 ドキュメント 192.88.99.0/24 RFC7526 IPv6 から IPv4 へのリレー（サポート終了） 198.18.0.0/15 RFC2544 ベンチマーク テスト 240.0.0.0/4 RFC5735\n/ RFC1112 将来の使用（クラス E） 範囲外を設定してしまうと、、、 # 事象1. VPCピアリングによるVPC間通信ができないことがある # 2022/04/12時点で、VPCピアリングを双方向で設定するとステータス上は「有効」と表示されるものの、内部で自動的に作成されるはずのルートリソースができておらず、通信が各々のVPCへルーティングできないといった事象が確認されています。\nなお、本事象発生時にCloud Consoleから手動でルートの作成を試みたものの、自動作成されるルートと同じ内容のモノをあとからユーザが手動で作成する、というのは叶わずの状況でした。（ネクストホップとしてVPCピアリングリソースを指定することができなかったため）\nそのため、この事象に対する有効な対処方法としては、結局のところ「有効なIPアドレス範囲に変更したサブネットへ作り直してください」となってしまうのかなと思います。\n終わりに # 普段から「(RFCは良く知らないけど)プライベートIPアドレスは \u0026ldquo;おまじない\u0026rdquo; の意味も込めて10.0.0/8、172.16.0.0/12、192.168.0.0/16の範囲から指定するようにしています」という慎重派の方には無縁の問題ではございますが、中には「プライベートネットワークなんだからIPアドレスは何を指定しても良いでしょ」という方もいるかと思います。\n今回ご紹介したとおりGoogle Cloudでは、いくつかのRFCを元に有効なIPアドレス範囲が定義されており、それを逸脱すると諸問題が起きてしまう可能性がありますのでぜひご留意いただければと思います。\nGoogle Cloud は、Google LLC の商標または登録商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。 ","date":"April 12, 2022","permalink":"/posts/2022/04/gcp-subnet-valid-ip-ranges/","section":"記事一覧","summary":"みなさん、こんにちは。Google Cloudのサブネットにはソフトウェア的に割り当てを制限されてはいないものの、実際に割り当ててしまうと通信障害などを起こしてしまう割り当てはいけないIPv4アドレス範囲があるのをご存じでしょうか。","title":"Google Cloudのサブネットには有効なIPアドレス範囲に制限があるのでご注意を"},{"content":"","date":"April 12, 2022","permalink":"/tags/vpc/","section":"タグ一覧","summary":"","title":"VPC"},{"content":"","date":"April 12, 2022","permalink":"/tags/vpc-peering/","section":"タグ一覧","summary":"","title":"VPC Peering"},{"content":" Microsoft Azure に関する技術記事やコラムです。 ","date":"April 6, 2022","permalink":"/tags/azure/","section":"タグ一覧","summary":" Microsoft Azure に関する技術記事やコラムです。 ","title":"Azure"},{"content":"","date":"April 6, 2022","permalink":"/tags/azure-cloud-shell/","section":"タグ一覧","summary":"","title":"Azure Cloud Shell"},{"content":"みなさん、こんにちは。Azure Cloud ShellのBashモードでは、lessコマンドでファイル参照中に内容を編集しようと v キーを押してエディタを起動するとemacsが起動してきます。もちろんemacsも良いエディタですが、Red Hat系のLinuxに慣れている方からすると「lessで v を押したらviが起動してきてほしい！てか、moreはviが起動するのになんでlessはviじゃないんだよ！」と思う方もきっといるはずです。今回はそんなときの解決方法を紹介していきたいと思います。\nエディタを切り替える方法 # それでは解決方法ですが、環境変数 VISUAL に起動したいコマンド(今回はvi)を指定してください。これでlessからviエディタを起動させることができるようになります。\n実行例）\nexport VISUAL=\u0026#34;vi\u0026#34; 起動時に自動で設定する方法 # とはいえ、Cloud Shellを起動するたびに毎回環境変数の設定をするのは面倒です。そんなときは設定ファイル ~/.bashrc の末尾に次の1行を追記しましょう。これでCloud Shellが起動した際に自動で適用されるようになります。めでたしめでたし。\n作成例）~/.bashrc\nexport VISUAL=\u0026#34;vi\u0026#34; 終わりに # いまさらの情報でしたがいかがだったでしょうか。lessがなんか使いにくいんだよなとお困りの方は参考にしていただければと思います。以上、Azure Cloud ShellのBashモードにてlessコマンドからviエディタを起動する方法でした。\nMicrosoft Azure は，Microsoft Corporation の商標または登録商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。 ","date":"April 6, 2022","permalink":"/posts/2022/04/azure-cloudshell-less-editor/","section":"記事一覧","summary":"みなさん、こんにちは。Azure Cloud ShellのBashモードでは、lessコマンドでファイル参照中に内容を編集しようと v キーを押してエディタを起動するとemacsが起動してきます。もちろんemacsも良いエディタですが、Red Hat系のLinuxに慣れている方からすると「lessで v を押したらviが起動してきてほしい！てか、moreはviが起動するのになんでlessはviじゃないんだよ！」と思う方もきっといるはずです。今回はそんなときの解決方法を紹介していきたいと思います。","title":"Azure Cloud ShellのBashモードにてlessコマンドからviエディタを起動する方法"},{"content":"","date":"April 6, 2022","permalink":"/tags/cloud-shell/","section":"タグ一覧","summary":"","title":"Cloud Shell"},{"content":"みなさん、こんにちは。Google CloudのCloud Shellでは、lessコマンドでファイル参照中に内容を編集しようと v キーを押してエディタを起動するとemacsが起動してきます。もちろんemacsも良いエディタですが、Red Hat系のLinuxに慣れている方からすると「lessで v を押したらviが起動してきてほしい！てか、moreはviが起動するのになんでlessはviじゃないんだよ！」と思う方もきっといるはずです。今回はそんなときの解決方法を紹介していきたいと思います。\nエディタを切り替える方法 # それでは解決方法ですが、環境変数 VISUAL に起動したいコマンド(今回はvi)を指定してください。これでlessからviエディタを起動させることができるようになります。\n実行例）\nexport VISUAL=\u0026#34;vi\u0026#34; 起動時に自動で設定する方法 # とはいえ、Cloud Shellを起動するたびに毎回環境変数の設定をするのは面倒です。そんなときは設定ファイル ~/.bashrc の末尾に次の1行を追記しましょう。これでCloud Shellが起動した際に自動で適用されるようになります。めでたしめでたし。\n作成例）~/.bashrc\nexport VISUAL=\u0026#34;vi\u0026#34; 終わりに # いまさらの情報でしたがいかがだったでしょうか。lessがなんか使いにくいんだよなとお困りの方は参考にしていただければと思います。以上、Google Cloud Shellにてlessコマンドからviエディタを起動する方法でした。\nGoogle Cloud は、Google LLC の商標または登録商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。 ","date":"April 6, 2022","permalink":"/posts/2022/04/gcp-cloudshell-less-editor/","section":"記事一覧","summary":"みなさん、こんにちは。Google CloudのCloud Shellでは、lessコマンドでファイル参照中に内容を編集しようと v キーを押してエディタを起動するとemacsが起動してきます。もちろんemacsも良いエディタですが、Red Hat系のLinuxに慣れている方からすると「lessで v を押したらviが起動してきてほしい！てか、moreはviが起動するのになんでlessはviじゃないんだよ！」と思う方もきっといるはずです。今回はそんなときの解決方法を紹介していきたいと思います。","title":"Google Cloud Shellにてlessコマンドからviエディタを起動する方法"},{"content":"","date":"March 31, 2022","permalink":"/tags/cloud-logging/","section":"タグ一覧","summary":"","title":"Cloud Logging"},{"content":"みなさん、こんにちは。今日はGoogle Cloudの監査ログ(Cloud Audit Logs)についてのお話です。公式ドキュメントでは少しわかりにくかった部分もあったため、個人的に悩んだポイントを含めて紹介していきたいと思います。\nCloud Audit logsとは # Cloud Audit Logsとは、Google Cloud上のリソースに対するAPI操作や各種イベントなどの情報を、監査ログとしてCloud Loggingへ記録してくれるサービス(厳密に言えばCloud Loggingの一機能)です。\n人によってはAWSでいうところのCloudTrailに相当する機能ですよ、と言えばイメージしやすいかもしれませんね。\n生成する監査ログの種類 # Cloud Audit Logsで生成する監査ログは次のとおりに細分化されおり、他のクラウドサービス(AWSやAzure)のようにユーザが明示的に有効化せずとも最低限おさえておくべき部分はデフォルトで生成してくれている印象です。\nNo. 種類 (ログ名) 概要 1 管理アクティビティ監査ログ (activity) VM作成やIAM権限変更などのリソースの構成やメタデータの変更(管理書き込み)をするAPI呼び出しに関するイベントを記録します。デフォルト有効で無効化はできません。 2 データアクセス監査ログ (data_access) リソースの構成やメタデータの読み込み(管理読み込み)をするAPI呼び出しに関するイベント、およびリソース上に保持するユーザ管理データに対するReadオペレーション(データ読み込み)/Writeオペレーション(データ書き込み)について記録します。BigQueryのみデフォルト有効、それ以外のサービスはデフォルト無効です。 3 システムイベント監査ログ (system_event) オートスケールなどの直接的なユーザ操作を伴わないリソース構成変更に関するイベントを記録します。デフォルト有効で無効化はできません。 4 ポリシー拒否監査ログ (policy) VPC Service Controlsで設定したセキュリティポリシーに違反し、拒否されたイベントを記録します。デフォルト有効で無効化はできませんが、除外フィルタを利用することでCloud Loggingに保存しないように設定できます。また、組織に属していないプロジェクトでは、VPC Service Controls機能自体が利用できないため本ログは生成されません。 5 アクセスの透過性ログ (access_transparency) サポート問合せなどを受けてGoogleスタッフがGoogle Cloudリソースに行った操作が記録されます。デフォルト無効で、組織に属していないプロジェクトでは有効化できません。 アクセスの透過性ログは、公式ドキュメント上では監査ログの扱いではありませんが、Cloud Audit Logs APIで取得するログのため、本記事では監査ログの1種類として扱っています。 生成した監査ログの保存先 # Cloud Audit Logsのデータは次に示すCloud Loggingの所定のログバケットへ保存するようになっています。\nNo. バケット名 概要 1 _Required Cloud Audit Logs専用のログバケットで「管理アクティビティ監査ログ」、「システムイベント監査ログ」、「アクセスの透過性ログ」の保存先として利用されます。本ログバケットに格納されるログは課金対象外(無料)で、保存期間は400日間固定で変更はできません。 2 _Default Cloud Loggingのデフォルトログバケットで「データアクセス監査ログ」、「「ポリシー拒否監査ログ」に加え、さまざまなGoogle Cloudリソースログの保存先として利用されます。本ログバケットに格納されるログは課金対象(有料)で、保存期間はデフォルト30日間です。 法令順守などのために長期保存が必要な場合は、Cloud StorageやBigQueryへのデータエクスポートが必須となってきますのでその点ご留意ください。 監査ログの有効化方法 # データアクセス監査ログ # データアクセス監査ログの有効化は、Cloud Consoleからであれば「IAMと管理」→「監査ログ」の画面にて、「すべてのサービスに影響を与えるデフォルトの監査構成の変更」と「サービス個別の監査構成の変更」の2つの設定方法があります。\n「デフォルトの監査構成」で有効化した際は、サービス個別に無効化することはできないためご注意ください。\nデフォルトの監査構成 サービス個別の監査構成 結果 「無効」から「有効」に変更 「無効」から「有効」に変更 監査ログを取得する 「無効」から「有効」に変更 「無効」のまま 監査ログを取得する 「無効」のまま 「無効」から「有効」に変更 監査ログを取得する 「無効」のまま 「無効」のまま 監査ログを取得しない ここで改めて設定画面を見ると設定オプションとしては、いずれも「管理読み取り」、「管理書き込み(変更不可)」、「データ読み取り」、「データ書き込み」の4種類で、それぞれの設定項目は次のような内容となってます。\nNo. 設定項目 概要 1 管理読み取り\n(ADMIN_READ) Google Cloudリソースの構成やメタデータを読み取るオペレーションを記録します。デフォルト無効です。 2 管理書き込み\n(ADMIN_WRITE) Google Cloudリソースの構成やメタデータの変更を書き込むオペレーションを記録します。デフォルト有効で無効にすることはできません。 3 データ読み込み\n(DATA_READ) Google Cloudリソースからユーザー提供のデータを読み取るオペレーションを記録します。デフォルト無効です。 4 データ書き込み\n(DATA_WRITE) ユーザー提供データをGoogle Cloudリソースに書き込むオペレーションを記録します。デフォルト無効です。 私は当初、この設定項目と「生成する監査ログの種類」との関係がわからず混乱しましたが、よくよく公式ドキュメントを読むと次のような関係となっていることがわかります。\nということで、データアクセス監査ログはさらに細分化されていて「管理読み取り」、「データ読み込み」、「データ書き込み」のログ有効化を選択できるようになっております。\nアクセスの透過性ログ # アクセスの透過性ログの有効化は、Cloud Consoleからであれば「IAMと管理」→「設定」の画面にて行うことができます。詳細については公式ドキュメントをご参照ください。\nなお、「生成する監査ログの種類」で述べたとおり、組織に属していないプロジェクトについてはアクセスの透過性ログを有効化することはできませんのであしからず。\n監査ログの参照方法 # 監査ログはCloud Loggingに保存されていますので、Cloud Consoleの「ロギング」→「ログエクスプローラ」から参照できます。\nCloud Loggingへの保存期間が過ぎたものについてはこの方法では参照することができません。保存期間を過ぎたログも含めて参照するには、このあとの長期保存方法にてエクスポートした監査ログを用いてBigQueryなどから参照する形になりますのでその点ご承知おきください。 監査ログの長期保存方法 # 「生成した監査ログの保存先」で記載したとおり、Cloud Loggingへの保存期間は一部を除いて変更ができないため長期保存のためにはエクスポートし、別のストレージサービス(Cloud StorageやBigQuery)にて長期保存をしていく形になります。\n監査ログのエクスポート方法については、Cloud Loggingのログルーターとシンクという他のストレージサービスへログを転送する機能を用いて実現できます。\n転送先にはCloud StorageやBigQueryの他にもさまざまな選択肢がありますが、法令順守のための長期保存といった要件があるならば、Cloud Storage(Archive)にエクスポートしてCloud Storageの保持ポリシー機能で指定した期間は変更も削除もできないように保護してくことをオススメします。\nご参考、監査ログを用いた不正監視 # Google Cloudでは、Event Thread Detection(Security Command Centerの一部)というSIEMサービスが提供されています。組織に属していないプロジェクトでは利用できませんが、Google Cloud純正で実装していく際にはこちらを活用して行く形になってくるかと思いますので参考にしてみてはいかがでしょうか。\n終わりに # いまさらの情報でしたがいかがだったでしょうか。こんな記事でもだれかの役に立っていただければ幸いです。以上、「Google Cloud監査ログ(Cloud Audit Logs)の構成および長期保存について」でした。\nGoogle Cloud は、Google LLC の商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"March 31, 2022","permalink":"/posts/2022/03/gcp-cloud-audit-log/","section":"記事一覧","summary":"みなさん、こんにちは。今日はGoogle Cloudの監査ログ(Cloud Audit Logs)についてのお話です。公式ドキュメントでは少しわかりにくかった部分もあったため、個人的に悩んだポイントを含めて紹介していきたいと思います。","title":"Google Cloud監査ログ(Cloud Audit Logs)の構成および長期保存について"},{"content":"","date":"March 31, 2022","permalink":"/tags/%E7%9B%A3%E6%9F%BB%E3%83%AD%E3%82%B0/","section":"タグ一覧","summary":"","title":"監査ログ"},{"content":"","date":"February 16, 2022","permalink":"/tags/amazon-ecr/","section":"タグ一覧","summary":"","title":"Amazon ECR"},{"content":"","date":"February 16, 2022","permalink":"/tags/amazon-eks/","section":"タグ一覧","summary":"","title":"Amazon EKS"},{"content":"みなさん、こんにちは。今回はさまざまなAWSサービスをKubernetesから管理できるようにするAWS Controllers for Kubernetes(ACK)のお話です。\nみなさんはAmazon EKSを活用してKubernetesクラスタをAWS上で動かすとなった際に、他のマネージドサービスの利用はどうされていますか。もちろんすべてKubernetes上で動かしてシステムを完結させるという選択肢もあるかと思いますが、やはり多くの方が他のAWSのマネージドサービスの併用も検討されるのではないでしょうか。その一方で、これら併用環境のコード化 (IaC、Infrastructure as Code) を実現しようとすると、Kubernetesアプリケーションの管理はHelm、AWSリソースの管理はTerraform、などという別々のツールでの管理になってしまいがちです。\nそんな悩みを解決する1つの手段がAWS Load Balancer ControllerやAWS Controllers for KubernetesといったKubernetesクラスタ機能を拡張する各種コントローラの活用です。これらのコントローラを利用することで、AWSリソースについてもKubernetesマニフェストファイルで定義できるようになり、Kubernetes側に運用管理を寄せてシンプル化することが可能です。\n今回はそのうちの1つ、さまざまなAWSサービスを管理できるようにするAWS Controllers for Kubernetesについて、簡単なサンプルを交えて紹介していきたいと思います。これからAmazon EKS上にアプリケーションを展開しようと考えている方は参考にしてみてはいかがでしょうか。\nAWS Controllers for Kubernetes(ACK)とは # AWS Controllers for Kubernetes(ACK)は、さまざまなAWSサービスをKubernetesクラスタから管理するためのKubernetes API拡張コントローラ群の総称です。このコントローラ群を活用することで、Kubernetesクラスタから直接AWSサービスの定義、作成を行うことが可能になり、アプリケーションとその依存関係にあるデータベース、メッセージキュー、オブジェクトストレージなどのマネージドサービスを含むすべてをKubernetesにて一元管理することが可能となります。なお、現時点のACKでは、次のAWSサービス向けコントローラがディベロッパープレビュー機能として利用可能となっています。\nAmazon API Gateway V2 Amazon Application Auto Scaling Amazon DynamoDB Amazon ECR Amazon EKS Amazon ElastiCache Amazon EC2 Amazon MQ Amazon OpenSearch Service Amazon RDS Amazon SageMaker Amazon SNS AWS Step Functions Amazon S3 ACKを導入してみよう # Step1. 作業環境の設定 # 今回はAmazon EKSやACKの管理を行う環境としてAWS CloudShellを利用していきたいと思います。まずは操作に必要な各種ツールの設定をAWS CloudShellにしてきましょう。\n#1. AWS CLIの設定 # AWSリソースの操作を行えるように次のコマンドを実行し、AWS CLIの設定を行いましょう。\n実行例）AWS CLIの設定\naws configure #2. kubectlコマンドのインストール # 次にKubernetes管理ツールのkubectlコマンドをインストールしましょう。\n実行例）kubectlコマンドのインストール\n# kubectl コマンドのダウンロード curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.21.2/2021-07-05/bin/linux/amd64/kubectl # 実行権限の付与 chmod +x kubectl # 実行ファイルのパスを設定 mkdir -p ${HOME}/bin \u0026amp;\u0026amp; mv kubectl ${HOME}/bin \u0026amp;\u0026amp; export PATH=${PATH}:${HOME}/bin # シェルの起動時に $HOME/bin をパスへ追加 echo \u0026#39;export PATH=${PATH}:${HOME}/bin\u0026#39; \u0026gt;\u0026gt; ~/.bashrc # インストールが成功していることを確認 kubectl version --short --client インストールに成功していれば出力例のようにバージョン情報の出力を確認できます。\n出力例）\n$ kubectl version --short --client Client Version: v1.21.2-13+d2965f0db10712 #3. eksctlコマンドのインストール # 続いてAmazon EKS管理ツールのeksctlコマンドをインストールしましょう。\n実行例）eksctlコマンドのインストール\n# eksctl の最新バージョンをダウンロード curl -L \u0026#34;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; | tar xz -C /tmp # 実行ファイルをパスの通ったディレクトリへ移動 mv /tmp/eksctl ${HOME}/bin # インストールが成功していることを確認 eksctl version インストールに成功していれば出力例のようにバージョン情報の出力を確認できます。\n出力例）\n$ eksctl version 0.83.0 #4. helmコマンドのインストール # 最後にKubernetes上で稼働するアプリケーションを管理するためのツールであるhelmコマンドをインストールしましょう。\n実行例）helmコマンドのインストール\n# 前提パッケージのインストール sudo yum install -y openssl # インストールスクリプトのダウンロード curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 \u0026gt; get_helm.sh # 実行権限の付与 chmod 700 get_helm.sh # インストールスクリプトの実行 ./get_helm.sh # インストールが成功していることを確認 helm version --short インストールに成功していれば出力例のようにバージョン情報の出力を確認できます。\n出力例）\n$ helm version --short v3.8.0+gd141386 以上で作業環境(AWS CloudShell)の設定は完了です。\nStep2. EKSクラスタの作成 # 続いてACKを導入する対象のAmazon EKSクラスタを作成していきましょう。今回はあくまで検証なのでeksctlコマンドでサクッと作成していきましょう。\n実行例）EKSクラスタの作成\n# 環境変数の設定 export CLUSTER=\u0026#34;matt-tokyo-cluster\u0026#34; # EKS クラスタの作成 eksctl create cluster --name ${CLUSTER} --version 1.21 # サービスアカウントでの IAM ロール使用を許可 eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER} --approve ここまで終わりましたらACKを利用するための事前準備は完了です。\nStep3. コントローラのデプロイ # それではACKをAmazon EKSクラスタにデプロイしていきましょう。Amazon ECRパブリックギャラリーにて各AWSサービスに対応するコントローラ用Helmチャートが公開されておりますのでこちらを利用してデプロイしていきたいと思います。\n#1. インストールスクリプトの作成 # ACKは各AWSサービスに対応するコントローラをそれぞれ導入し、サービスアカウントの設定をしていく必要があるのですが、公式ドキュメントを見るとかなり煩雑な手順になっていることがわかります。そこでコマンド1つでインストールできるようにスクリプト化してみました。\n再実行を想定してない作りになっているのでご注意ください。 作成例）install.sh\n#!/bin/bash set -eux # 引数の確認 if [ $# -eq 0 ] \u0026amp;\u0026amp; [ -z \u0026#34;${SERVICE}\u0026#34; ]; then echo \u0026#34;Error: usage: ./install.sh \u0026lt;SERVICE_NAME\u0026gt;\u0026#34; exit 1 elif [ $# -eq 1 ] \u0026amp;\u0026amp; [ -n \u0026#34;$1\u0026#34; ]; then SERVICE=$1 fi # 環境変数の設定 export HELM_EXPERIMENTAL_OCI=1 CHART_EXPORT_PATH=\u0026#34;/tmp/chart\u0026#34; ACK_K8S_NAMESPACE=${ACK_K8S_NAMESPACE:-\u0026#34;ack-system\u0026#34;} ACK_SERVICE_CONTROLLER=\u0026#34;ack-${SERVICE}-controller\u0026#34; AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \u0026#34;Account\u0026#34; --output text) CLUSTER=${CLUSTER:-\u0026#34;my-tokyo-cluster\u0026#34;} OIDC_PROVIDER=$(aws eks describe-cluster --name ${CLUSTER} \\ --query \u0026#34;cluster.identity.oidc.issuer\u0026#34; --output text \\ | sed -e \u0026#34;s/^https:\\/\\///\u0026#34;) # 最新リリースバージョン名の取得 RELEASE_VERSION=$(curl -sL \\ https://api.github.com/repos/aws-controllers-k8s/${SERVICE}-controller/releases/latest \\ | grep \u0026#39;\u0026#34;tag_name\u0026#34;:\u0026#39; | cut -d\u0026#39;\u0026#34;\u0026#39; -f4) if [ -z \u0026#34;${RELEASE_VERSION}\u0026#34; ]; then # latestリリースが作成されていない場合は一覧から最新バージョンを取得 RELEASE_VERSION=$(curl -sL \\ https://api.github.com/repos/aws-controllers-k8s/${SERVICE}-controller/releases \\ | grep \u0026#39;\u0026#34;tag_name\u0026#34;:\u0026#39; | head -n 1 | cut -d\u0026#39;\u0026#34;\u0026#39; -f4) # リリースを何も作成されていない場合はv0.0.1を設定(SNSコントローラ向け) RELEASE_VERSION=${RELEASE_VERSION:-v0.0.1} fi # Helmチャートのダウンロードディレクトリの作成 mkdir -p ${CHART_EXPORT_PATH} # Helmチャートのダウンロード helm pull oci://public.ecr.aws/aws-controllers-k8s/${SERVICE}-chart \\ --version $RELEASE_VERSION -d ${CHART_EXPORT_PATH} # アーカイブの解凍 tar xvf ${CHART_EXPORT_PATH}/${SERVICE}-chart-${RELEASE_VERSION}.tgz \\ -C ${CHART_EXPORT_PATH} # 解凍後のパスチェック if [ -d ${CHART_EXPORT_PATH}/${SERVICE}-chart ]; then CHART_PATH=${CHART_EXPORT_PATH}/${SERVICE}-chart else # SNSコントローラ向け CHART_PATH=${CHART_EXPORT_PATH}/${ACK_SERVICE_CONTROLLER} fi # コントローラのインストール helm install --create-namespace ${ACK_SERVICE_CONTROLLER} \\ --namespace ${ACK_K8S_NAMESPACE} \\ --set aws.region=\u0026#34;${AWS_REGION}\u0026#34; ${CHART_PATH} # IAMロール定義ファイルの作成 cat \u0026lt;\u0026lt;EOF \u0026gt; /tmp/${ACK_SERVICE_CONTROLLER}.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Federated\u0026#34;: \u0026#34;arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;${OIDC_PROVIDER}:sub\u0026#34;: \u0026#34;system:serviceaccount:${ACK_K8S_NAMESPACE}:${ACK_SERVICE_CONTROLLER}\u0026#34; } } } ] } EOF # IAMロールの作成 aws iam create-role \\ --role-name ${ACK_SERVICE_CONTROLLER} \\ --assume-role-policy-document file:///tmp/${ACK_SERVICE_CONTROLLER}.json # 推奨IAMポリシーの設定 POLICY_ARN_STRINGS=$(curl -sL \\ https://raw.githubusercontent.com/aws-controllers-k8s/${SERVICE}-controller/main/config/iam/recommended-policy-arn) if [ \u0026#34;404: Not Found\u0026#34; != \u0026#34;${POLICY_ARN_STRINGS}\u0026#34; ]; then while IFS= read -r POLICY_ARN; do aws iam attach-role-policy \\ --role-name \u0026#34;${ACK_SERVICE_CONTROLLER}\u0026#34; \\ --policy-arn \u0026#34;${POLICY_ARN}\u0026#34; done \u0026lt;\u0026lt;\u0026lt; \u0026#34;${POLICY_ARN_STRINGS}\u0026#34; fi # 推奨IAMポリシーの設定(EKSコントローラ向け) INLINE_POLICY=$(curl -sL \\ https://raw.githubusercontent.com/aws-controllers-k8s/${SERVICE}-controller/main/config/iam/recommended-inline-policy) if [ \u0026#34;404: Not Found\u0026#34; != \u0026#34;${INLINE_POLICY}\u0026#34; ]; then aws iam put-role-policy \\ --role-name \u0026#34;${ACK_SERVICE_CONTROLLER}\u0026#34; \\ --policy-name \u0026#34;ack-recommended-policy\u0026#34; \\ --policy-document \u0026#34;${INLINE_POLICY}\u0026#34; fi # サービスアカウントにIAMロールを関連付け kubectl -n ${ACK_K8S_NAMESPACE} annotate serviceaccount \\ ${ACK_SERVICE_CONTROLLER} \\ eks.amazonaws.com/role-arn=$(aws iam get-role \\ --role-name=${ACK_SERVICE_CONTROLLER} --query Role.Arn --output text) # Deploymentリソースを再起動 kubectl -n ${ACK_K8S_NAMESPACE} rollout restart deployment \\ $(kubectl -n ${ACK_K8S_NAMESPACE} get deployment \\ -o custom-columns=Name:metadata.name --no-headers \\ | grep ${ACK_SERVICE_CONTROLLER}) #2. コントローラのインストール # それではコントローラを導入していきたいと思います。本記事では例としてAmazon Simple Storage Service(S3)サービスに対応したコントローラをインストールしていきたいと思います。\n実行例）コントローラのインストール(S3の場合)\n# 環境変数の設定 export SERVICE=\u0026#34;s3\u0026#34; export ACK_K8S_NAMESPACE=\u0026#34;ack-system\u0026#34; export ACK_SERVICE_CONTROLLER=\u0026#34;ack-${SERVICE}-controller\u0026#34; # コントローラのインストール bash install.sh # 確認 helm list -n ${ACK_K8S_NAMESPACE} -o yaml -f ${ACK_SERVICE_CONTROLLER} インストールに成功していれば出力例のようにコントローラが表示されるようになります。\n出力例）\n$ helm list -n ${ACK_K8S_NAMESPACE} -o yaml -f ${ACK_SERVICE_CONTROLLER} - app_version: v0.0.13 chart: s3-chart-v0.0.13 name: ack-s3-controller namespace: ack-system revision: \u0026#34;1\u0026#34; status: deployed updated: 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC なお、今回紹介したS3以外のコントローラを導入する際は、環境変数SERVICEの値を各 AWS サービスに対応する文字列へ置換することでインストールすることが可能です。各サービスに対応する文字列は次の通りです。\nAWS サービス名 SERVICE 値 Amazon API Gateway V2 apigatewayv2 Amazon Application Auto Scaling applicationautoscaling Amazon DynamoDB dynamodb Amazon ECR ecr Amazon EKS eks Amazon ElastiCache elasticache Amazon EC2 ec2 Amazon MQ mq Amazon OpenSearch Service opensearchservice Amazon RDS rds Amazon SageMaker sagemaker Amazon SNS sns AWS Step Functions sfn Amazon S3 s3 ちなみに、全部入れるとこのような感じでコントローラだらけになってしまいます^^;\n出力例）\n$ helm list -n ${ACK_K8S_NAMESPACE} NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION ack-apigatewayv2-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed apigatewayv2-chart-v0.0.15 v0.0.15 ack-applicationautoscaling-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed applicationautoscaling-chart-v0.2.4 v0.2.4 ack-dynamodb-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed dynamodb-chart-v0.0.14 v0.0.14 ack-ec2-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed ec2-chart-v0.0.7 v0.0.7 ack-ecr-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed ecr-chart-v0.0.19 v0.0.19 ack-eks-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed eks-chart-v0.0.8 v0.0.8 ack-elasticache-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed elasticache-chart-v0.0.14 v0.0.14 ack-mq-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed mq-chart-v0.0.12 v0.0.12 ack-opensearchservice-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed opensearchservice-chart-v0.0.9 v0.0.9 ack-rds-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed rds-chart-v0.0.17 v0.0.17 ack-s3-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed s3-chart-v0.0.13 v0.0.13 ack-sagemaker-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed sagemaker-chart-v0.3.0 v0.3.0 ack-sfn-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed sfn-chart-v0.0.11 v0.0.11 ack-sns-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed ack-sns-controller-v0.0.1 v0.0.1 ACKを使ってみよう # 今回は一部のAWSサービス向けコントローラを簡単なサンプルを用いて使い方を紹介していきたいと思います。すべてのコントローラの使い方については公式ドキュメントを参照ください。\nAmazon ECRコントローラの使い方 # ACK環境では、次のサンプルのようにKubernetes Repositoryリソースを定義をすることによってECRリポジトリをプロビジョニングできます。定義可能なスペックの詳細については次の公式リファレンスドキュメントを参照ください。\n作成例）repository.yaml (ECRリポジトリリソース定義サンプル)\napiVersion: ecr.services.k8s.aws/v1alpha1 kind: Repository metadata: name: \u0026#34;matt-ecr-repository\u0026#34; spec: name: \u0026#34;matt-ecr-repository\u0026#34; imageScanningConfiguration: scanOnPush: true それでは次のコマンドを実行してサンプルECRリポジトリリソースをデプロイしていきましょう。\n実行例）サンプルECRリポジトリのデプロイ\nexport NAMESPACE=\u0026#34;sample\u0026#34; # サンプルECRリポジトリ用Namespaceの作成 kubectl create namespace ${NAMESPACE} # サンプルECRリポジトリのデプロイ kubectl apply -n ${NAMESPACE} -f repository.yaml # サンプルECRリポジトリがデプロイされたことを確認 kubectl get -n ${NAMESPACE} repository ECRリポジトリリソースのデプロイに成功した場合は次の出力例のようにRepositoryリソース一覧に表示されます。\n出力例）\n$ kubectl get -n ${NAMESPACE} repository NAME AGE matt-ecr-repository 10s 念のため、AWS CLIを実行してECRリポジトリが作成されているかを確認してみましょう。\n実行例）ECRリポジトリの確認\naws ecr describe-repositories --repository-name matt-ecr-repository AWS CLIでもECRリポジトリが確認できたら成功です。\n出力例）\n$ aws ecr describe-repositories --repository-name matt-ecr-repository repositories: - createdAt: \u0026#39;2022-02-xxTxx:xx:xx+00:00\u0026#39; encryptionConfiguration: encryptionType: AES256 imageScanningConfiguration: scanOnPush: true imageTagMutability: MUTABLE registryId: \u0026#39;xxxxxxxxxxxx\u0026#39; repositoryArn: arn:aws:ecr:ap-northeast-1:xxxxxxxxxxxx:repository/matt-ecr-repository repositoryName: matt-ecr-repository repositoryUri: xxxxxxxxxxxx.dkr.ecr.ap-northeast-1.amazonaws.com/matt-ecr-repository 最後にサンプルを削除して動作確認は終了です。お疲れ様でした。\n実行例）サンプルの削除\nkubectl delete namespace ${NAMESPACE} Amazon S3コントローラの使い方 # ACK環境では、次のサンプルのようにKubernetes Bucketリソースを定義をすることによってS3バケットをプロビジョニングできます。定義可能なスペックの詳細については次の公式リファレンスドキュメントを参照ください。\n作成例）bucket.yaml (S3バケットリソース定義サンプル)\napiVersion: s3.services.k8s.aws/v1alpha1 kind: Bucket metadata: name: \u0026#34;matt-s3-bucket\u0026#34; spec: name: \u0026#34;matt-s3-bucket\u0026#34; publicAccessBlock: blockPublicACLs: true blockPublicPolicy: true versioning: status: Enabled encryption: rules: - bucketKeyEnabled: false applyServerSideEncryptionByDefault: sseAlgorithm: AES256 それでは次のコマンドを実行してサンプルS3バケットリソースをデプロイしていきましょう。\n実行例）サンプルS3バケットのデプロイ\nexport NAMESPACE=\u0026#34;sample\u0026#34; # サンプルS3バケット用Namespaceの作成 kubectl create namespace ${NAMESPACE} # サンプルS3バケットのデプロイ kubectl apply -n ${NAMESPACE} -f bucket.yaml # サンプルS3バケットがデプロイされたことを確認 kubectl get -n ${NAMESPACE} bucket S3バケットリソースのデプロイに成功した場合は次の出力例のようにBucketリソース一覧に表示されます。\n出力例）\n$ kubectl get -n ${NAMESPACE} bucket NAME AGE matt-s3-bucket 1m22s 念のため、AWS CLIを実行してS3バケットが作成されているかを確認してみましょう。\n実行例）S3バケットが作成されているかを確認\naws s3 ls | grep \u0026#34;matt-s3-bucket\u0026#34; AWS CLIでもS3バケットが確認できたら成功です。\n出力例）\n$ aws s3 ls | grep \u0026#34;matt-s3-bucket\u0026#34; 2022-02-xx xx:xx:xx matt-s3-bucket 最後にサンプルを削除して動作確認は終了です。お疲れ様でした。\n実行例）サンプルの削除\nkubectl delete namespace ${NAMESPACE} 終わりに # 今回はさまざまなAWSサービスをAmazon Elastic Kubernetes Service(EKS)上で管理できるようにするAWS Controllers for Kubernetesのご紹介でしたがいかがだったでしょうか。\n現時点ではディベロッパープレビュー機能のためプロダクション用途での活用はまだまだオススメできませんが、もしAWS Controllers for Kubernetesの活用を検討されている方は参考にしていただければ幸いです。\n以上、さまざまなAWSサービスをKubernetesから管理できるようにする「AWS Controllers for Kubernetes」のご紹介でした。\nAWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 Kubernetes は、The Linux Foundation の米国およびその他の国における登録商標または商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。 ","date":"February 16, 2022","permalink":"/posts/2022/02/aws-controllers-for-kubernetes/","section":"記事一覧","summary":"みなさん、こんにちは。今回はさまざまなAWSサービスをKubernetesから管理できるようにするAWS Controllers for Kubernetes(ACK)のお話です。","title":"AWS Controllers for Kubernetesを使って各種AWSサービスをマニフェストファイルで管理しよう"},{"content":"","date":"February 16, 2022","permalink":"/tags/kubernetes/","section":"タグ一覧","summary":"","title":"Kubernetes"},{"content":"","date":"February 14, 2022","permalink":"/tags/anthos-service-mesh/","section":"タグ一覧","summary":"","title":"Anthos Service Mesh"},{"content":"","date":"February 14, 2022","permalink":"/tags/google-kubernetes-enginegke/","section":"タグ一覧","summary":"","title":"Google Kubernetes Engine(GKE)"},{"content":"","date":"February 14, 2022","permalink":"/tags/istio/","section":"タグ一覧","summary":"","title":"Istio"},{"content":"みなさん、こんにちは。以前に「複数リージョンのGKEクラスタとAnthos Service Meshでマルチクラスタメッシュ環境を構築してみた」という記事を書いたのですが、今回はその環境をTerraformを使って構築してみました。もしこれから「ASM環境をTerraformで」と検討している方は参考にしてみてはいかがでしょうか。\nとはいえ、本記事の執筆時点(2022年1月末)ではTerraform公式モジュールがASMのv1.11以降に対応しておらず正直使いモノにならなかったこともあり、やや苦しい実装になってしまっています。素直にASMの導入以降はTerraform以外を使うのが良いかと思いますが、あくまで本記事はご参考ということでその点ご承知おきいただけると幸いです。\n構築するシステムについて # 次の図に示すように限定公開クラスを有効化した複数リージョンのGKEクラスタに対してAnthos Service Mesh(マネージドコントロールプレーン)を導入した環境となっています。なお、アプリケーションのコンテナについてはインフラとは異なるリポジトリで管理するのが一般的かと思うので今回は除外しています。\nTerraformのサンプルコードを書いてみた # それでは今回作成したTerraformのサンプルコードを紹介していきたいと思います。まずはディレクトリ構造ですが、今回はenvironmentsディレクトリ配下へ環境ごとにサブディレクトリを作成し、Workspaceは使わずに別ファイルとして管理する形を想定した作りにしてます。\n作成例）ディレクトリ構成\n. |-- environments | `-- poc | |-- backend.tf | |-- main.tf | `-- variables.tf `-- modules |-- networks | |-- main.tf | |-- variables.tf | `-- outputs.tf |-- gke | |-- main.tf | |-- variables.tf | `-- outputs.tf `-- asm |-- main.tf |-- variables.tf |-- scripts | |-- install.sh | |-- destroy.sh | `-- create-mesh.sh `-- manifests |-- istio-ingressgateway-pods | |-- namespace.yaml | |-- deployment.yaml | |-- serviceaccount.yaml | `-- role.yaml `-- istio-ingressgateway-services |-- multiclusterservice.yaml |-- backendconfig.yaml `-- multiclusteringress.yaml # ファイル名 概要 1 environments/poc/backend.tf PoC環境のtfstateファイル保存先定義 2 environments/poc/main.tf PoC環境の定義 3 environments/pod/variables.tf PoC環境の外部変数定義 4 modules/networks/main.tf ネットワーク設定用モジュールの定義 5 modules/networks/variables.tf ネットワーク設定用モジュールの外部変数定義 6 modules/networks/outputs.tf ネットワーク設定用モジュールのアウトプット定義 7 modules/gke/main.tf GKE設定用モジュールの定義 8 modules/gke/variables.tf GKE設定用モジュールの外部変数定義 9 modules/gke/outputs.tf GKE設定用モジュールのアウトプット定義 10 modules/asm/main.tf ASM設定用モジュールの定義 11 modules/asm/variables.tf ASM設定用モジュールの外部変数定義 12 modules/asm/scripts/install.sh ASMのインストールスクリプト 13 modules/asm/scripts/destroy.sh ASMのアンインストールスクリプト 14 modules/asm/scripts/create-mesh.sh ASMのマルチクラスタメッシュ作成スクリプト 15 modules/asm/manifests/istio-ingressgateway-pods/* Istio IngressGatewayコンテナのKubernetesマニフェストファイル群 16 modules/asm/manifests/istio-ingressgateway-services/* Istio IngressGatewayサービスのKubernetesマニフェストファイル群 PoC環境定義 # environments/poc/backend.tf # PoC環境のtfstateファイルをGoogle Cloud Storage(GCS)上で管理するための設定を定義しています。\n作成例）./environments/poc/backend.tf\nterraform { backend \u0026#34;gcs\u0026#34; { bucket = \u0026#34;matt-gcs-tfstate\u0026#34; prefix = \u0026#34;multi-asm-poc\u0026#34; } } environments/poc/main.tf # PoC環境の定義をしています。実際の処理はモジュール側で定義しており、このファイルではPoC環境固有の設定値定義がメインの役割となっています。\n作成例）./environments/poc/main.tf\nlocals { network = \u0026#34;matt-vpc\u0026#34; tokyo_subnet = \u0026#34;matt-tokyo-priv-snet\u0026#34; tokyo_subnet_ip_range = \u0026#34;172.16.0.0/16\u0026#34; tokyo_router = \u0026#34;matt-tokyo-router\u0026#34; tokyo_nat = \u0026#34;matt-tokyo-nat\u0026#34; osaka_subnet = \u0026#34;matt-osaka-priv-snet\u0026#34; osaka_subnet_ip_range = \u0026#34;172.24.0.0/16\u0026#34; osaka_router = \u0026#34;matt-osaka-router\u0026#34; osaka_nat = \u0026#34;matt-osaka-nat\u0026#34; tokyo_cluster = \u0026#34;matt-tokyo-cluster-1\u0026#34; tokyo_master_ip_range = \u0026#34;192.168.0.0/28\u0026#34; tokyo_pod_ip_range = \u0026#34;10.16.0.0/14\u0026#34; tokyo_service_ip_range = \u0026#34;10.20.0.0/20\u0026#34; osaka_cluster = \u0026#34;matt-osaka-cluster-1\u0026#34; osaka_master_ip_range = \u0026#34;192.168.8.0/28\u0026#34; osaka_pod_ip_range = \u0026#34;10.32.0.0/14\u0026#34; osaka_service_ip_range = \u0026#34;10.36.0.0/20\u0026#34; } module \u0026#34;networks\u0026#34; { source = \u0026#34;../../modules/networks\u0026#34; project_id = var.project_id network = local.network tokyo_subnet = local.tokyo_subnet tokyo_subnet_ip_range = local.tokyo_subnet_ip_range tokyo_subnet_2nd_ip_range_1 = local.tokyo_pod_ip_range tokyo_subnet_2nd_ip_range_2 = local.tokyo_service_ip_range tokyo_router = local.tokyo_router tokyo_nat = local.tokyo_nat osaka_subnet = local.osaka_subnet osaka_subnet_ip_range = local.osaka_subnet_ip_range osaka_subnet_2nd_ip_range_1 = local.osaka_pod_ip_range osaka_subnet_2nd_ip_range_2 = local.osaka_service_ip_range osaka_router = local.osaka_router osaka_nat = local.osaka_nat } module \u0026#34;gke\u0026#34; { source = \u0026#34;../../modules/gke\u0026#34; project_id = var.project_id network = module.networks.network tokyo_cluster = local.tokyo_cluster tokyo_subnet = local.tokyo_subnet tokyo_master_ip_range = local.tokyo_master_ip_range osaka_cluster = local.osaka_cluster osaka_subnet = local.osaka_subnet osaka_master_ip_range = local.osaka_master_ip_range } module \u0026#34;asm\u0026#34; { source = \u0026#34;../../modules/asm\u0026#34; project_id = var.project_id network = module.networks.network tokyo_cluster = module.gke.tokyo_cluster tokyo_pod_ip_range = local.tokyo_pod_ip_range osaka_cluster = module.gke.osaka_cluster osaka_pod_ip_range = local.osaka_pod_ip_range } environments/pod/variables.tf # terraform plan/apply コマンド実行時に -var=\u0026quot;project_id=${PROJECT_ID}\u0026quot; のような形で外部から与える変数を定義しています。\n作成例）./environments/poc/variables.tf\nvariable \u0026#34;project_id\u0026#34; {} ネットワークモジュール定義 # modules/networks/main.tf # ネットワーク設定としてVPCおよびCloud NATの定義をしています。今回の例ではTerraform公式モジュールを活用してみました。\n作成例）./modules/networks/main.tf\nmodule \u0026#34;vpc\u0026#34; { source = \u0026#34;terraform-google-modules/network/google\u0026#34; version = \u0026#34;4.1.0\u0026#34; description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/network/google/4.1.0\u0026#34; project_id = var.project_id network_name = var.network shared_vpc_host = false subnets = [ { subnet_name = var.tokyo_subnet subnet_ip = var.tokyo_subnet_ip_range subnet_region = \u0026#34;asia-northeast1\u0026#34; subnet_private_access = true }, { subnet_name = var.osaka_subnet subnet_ip = var.osaka_subnet_ip_range subnet_region = \u0026#34;asia-northeast2\u0026#34; subnet_private_access = true } ] secondary_ranges = { (var.tokyo_subnet) = [ { range_name = \u0026#34;${var.tokyo_subnet}-pods\u0026#34; ip_cidr_range = var.tokyo_subnet_2nd_ip_range_1 }, { range_name = \u0026#34;${var.tokyo_subnet}-services\u0026#34; ip_cidr_range = var.tokyo_subnet_2nd_ip_range_2 }, ] (var.osaka_subnet) = [ { range_name = \u0026#34;${var.osaka_subnet}-pods\u0026#34; ip_cidr_range = var.osaka_subnet_2nd_ip_range_1 }, { range_name = \u0026#34;${var.osaka_subnet}-services\u0026#34; ip_cidr_range = var.osaka_subnet_2nd_ip_range_2 }, ] } } module \u0026#34;cloud_router_tokyo\u0026#34; { source = \u0026#34;terraform-google-modules/cloud-router/google\u0026#34; version = \u0026#34;1.3.0\u0026#34; description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/cloud-router/google/1.3.0\u0026#34; name = var.tokyo_router project = var.project_id region = \u0026#34;asia-northeast1\u0026#34; network = module.vpc.network_name nats = [{ name = var.tokyo_nat }] } module \u0026#34;cloud_router_osaka\u0026#34; { source = \u0026#34;terraform-google-modules/cloud-router/google\u0026#34; version = \u0026#34;1.3.0\u0026#34; description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/cloud-router/google/1.3.0\u0026#34; name = var.osaka_router project = var.project_id region = \u0026#34;asia-northeast2\u0026#34; network = module.vpc.network_name nats = [{ name = var.osaka_nat }] } modules/networks/variables.tf # ネットワークモジュールの外部変数を定義しています。\n作成例）./modules/networks/variables.tf\nvariable \u0026#34;project_id\u0026#34; {} variable \u0026#34;network\u0026#34; {} variable \u0026#34;tokyo_subnet\u0026#34; {} variable \u0026#34;tokyo_subnet_ip_range\u0026#34; {} variable \u0026#34;tokyo_subnet_2nd_ip_range_1\u0026#34; {} variable \u0026#34;tokyo_subnet_2nd_ip_range_2\u0026#34; {} variable \u0026#34;tokyo_router\u0026#34; {} variable \u0026#34;tokyo_nat\u0026#34; {} variable \u0026#34;osaka_subnet\u0026#34; {} variable \u0026#34;osaka_subnet_ip_range\u0026#34; {} variable \u0026#34;osaka_subnet_2nd_ip_range_1\u0026#34; {} variable \u0026#34;osaka_subnet_2nd_ip_range_2\u0026#34; {} variable \u0026#34;osaka_router\u0026#34; {} variable \u0026#34;osaka_nat\u0026#34; {} modules/networks/outputs.tf # ネットワークモジュールの出力変数を定義しています。\n作成例）./modules/networks/outputs.tf\noutput \u0026#34;network\u0026#34; { value = module.vpc.network_name } GKEモジュール定義 # modules/gke/main.tf # 東京/大阪リージョンのGKEクラスタを定義しています。こちらもネットワークモジュール同様にTerraform公式モジュールを活用してみました。\n記事執筆時点(2022年1月末)では、コントロールプレーンのグローバルアクセスを有効化するオプションがTerraform公式private-clusterサブモジュールv19.0.0(latest)になかったため、Terraform公式beta-private-clusterサブモジュールv19.0.0(latest)を活用しています。 作成例）./modules/gke/main.tf\nmodule \u0026#34;gke_tokyo\u0026#34; { source = \u0026#34;terraform-google-modules/kubernetes-engine/google//modules/beta-private-cluster\u0026#34; version = \u0026#34;19.0.0\u0026#34; description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/kubernetes-engine/google/19.0.0/submodules/beta-private-cluster\u0026#34; project_id = var.project_id name = var.tokyo_cluster region = \u0026#34;asia-northeast1\u0026#34; network = var.network subnetwork = var.tokyo_subnet ip_range_pods = \u0026#34;${var.tokyo_subnet}-pods\u0026#34; ip_range_services = \u0026#34;${var.tokyo_subnet}-services\u0026#34; enable_private_endpoint = false enable_private_nodes = true master_global_access_enabled = true master_ipv4_cidr_block = var.tokyo_master_ip_range release_channel = var.release_channel node_pools = [{ name = \u0026#34;default-tokyo-pool\u0026#34; machine_type = \u0026#34;e2-standard-4\u0026#34; min_count = 1 max_count = 3 initial_node_count = 1 }] remove_default_node_pool = true } module \u0026#34;gke_osaka\u0026#34; { source = \u0026#34;terraform-google-modules/kubernetes-engine/google//modules/beta-private-cluster\u0026#34; version = \u0026#34;19.0.0\u0026#34; description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/kubernetes-engine/google/19.0.0/submodules/beta-private-cluster\u0026#34; project_id = var.project_id name = var.osaka_cluster region = \u0026#34;asia-northeast2\u0026#34; network = var.network subnetwork = var.osaka_subnet ip_range_pods = \u0026#34;${var.osaka_subnet}-pods\u0026#34; ip_range_services = \u0026#34;${var.osaka_subnet}-services\u0026#34; enable_private_endpoint = false enable_private_nodes = true master_global_access_enabled = true master_ipv4_cidr_block = var.osaka_master_ip_range release_channel = var.release_channel node_pools = [{ name = \u0026#34;default-osaka-pool\u0026#34; machine_type = \u0026#34;e2-standard-4\u0026#34; min_count = 1 max_count = 3 initial_node_count = 1 }] remove_default_node_pool = true } modules/gke/variables.tf # GKEモジュールの外部変数を定義しています。\n作成例）./modules/gke/variables.tf\nvariable \u0026#34;project_id\u0026#34; {} variable \u0026#34;network\u0026#34; {} variable \u0026#34;tokyo_cluster\u0026#34; {} variable \u0026#34;tokyo_subnet\u0026#34; {} variable \u0026#34;tokyo_master_ip_range\u0026#34; {} variable \u0026#34;osaka_cluster\u0026#34; {} variable \u0026#34;osaka_subnet\u0026#34; {} variable \u0026#34;osaka_master_ip_range\u0026#34; {} variable \u0026#34;release_channel\u0026#34; { default = \u0026#34;STABLE\u0026#34; } modules/gke/outputs.tf # GKEモジュールの出力変数を定義しています。\n作成例）./modules/gke/outputs.tf\noutput \u0026#34;tokyo_cluster\u0026#34; { value = module.gke_tokyo.name } output \u0026#34;osaka_cluster\u0026#34; { value = module.gke_osaka.name } ASMモジュール定義 # modules/asm/main.tf # 東京/大阪リージョンのGKEクラスタにASMのインストール、マルチクラスタメッシ作成、Ingressゲートウェイのデプロイを定義しています、、、とはいえ、サンプルコードを書いといてなんですが大変苦しい実装になっていますので個人的には現時点では素直にTerraform以外を使用した方が良いと感じてます^^;\n記事執筆時点(2022年1月末)では、Terraform公式asmサブモジュールv19.0.0(latest)がASM v11.0以降に対応できていなかったため、Terraform公式gcloudモジュールおよびkubectl-wrapperサブモジュールv3.1.0(latest)を活用してシェルスクリプトでゴリゴリ実装しており、非常に微妙な作りになっております。 今回の例ではTerraform公式firewall-rulesサブモジュールv4.1.0(latest)を活用してファイアウォールルールを定義していますが、rules内の変数定義が省略できず使い勝手はよろしくないため、google_compute_firewallリソースをそのまま定義した方が個人的には良いと感じてます。 作成例）./modules/asm/main.tf\nmodule \u0026#34;asm_tokyo\u0026#34; { source = \u0026#34;terraform-google-modules/gcloud/google//modules/kubectl-wrapper\u0026#34; version = \u0026#34;3.1.0\u0026#34; #description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/gcloud/google/3.1.0/submodules/kubectl-wrapper\u0026#34; project_id = var.project_id cluster_name = var.tokyo_cluster cluster_location = var.tokyo_location kubectl_create_command = \u0026#34;${path.module}/scripts/install.sh ${var.project_id} ${var.tokyo_cluster} ${var.tokyo_location} ${var.release_channel}\u0026#34; kubectl_destroy_command = \u0026#34;${path.module}/scripts/destroy.sh ${var.project_id} ${var.tokyo_cluster} ${var.tokyo_location}\u0026#34; } module \u0026#34;asm_osaka\u0026#34; { source = \u0026#34;terraform-google-modules/gcloud/google//modules/kubectl-wrapper\u0026#34; version = \u0026#34;3.1.0\u0026#34; #description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/gcloud/google/3.1.0/submodules/kubectl-wrapper\u0026#34; project_id = var.project_id cluster_name = var.osaka_cluster cluster_location = var.osaka_location kubectl_create_command = \u0026#34;${path.module}/scripts/install.sh ${var.project_id} ${var.osaka_cluster} ${var.osaka_location} ${var.release_channel}\u0026#34; kubectl_destroy_command = \u0026#34;${path.module}/scripts/destroy.sh ${var.project_id} ${var.osaka_cluster} ${var.osaka_location}\u0026#34; module_depends_on = [module.asm_tokyo.wait] } module \u0026#34;asm_firewall_rules\u0026#34; { source = \u0026#34;terraform-google-modules/network/google//modules/firewall-rules\u0026#34; version = \u0026#34;4.1.0\u0026#34; #description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/network/google/4.1.0/submodules/firewall-rules\u0026#34; project_id = var.project_id network_name = var.network rules = [{ name = \u0026#34;${var.network}-istio-multicluster-pods\u0026#34; description = null direction = \u0026#34;INGRESS\u0026#34; priority = 900 ranges = [\u0026#34;${var.tokyo_pod_ip_range}\u0026#34;, \u0026#34;${var.osaka_pod_ip_range}\u0026#34;] source_tags = null source_service_accounts = null target_tags = [\u0026#34;gke-${var.tokyo_cluster}\u0026#34;, \u0026#34;gke-${var.osaka_cluster}\u0026#34;] target_service_accounts = null allow = [ { protocol = \u0026#34;tcp\u0026#34; ports = null }, { protocol = \u0026#34;udp\u0026#34; ports = null }, { protocol = \u0026#34;icmp\u0026#34; ports = null }, { protocol = \u0026#34;esp\u0026#34; ports = null }, { protocol = \u0026#34;ah\u0026#34; ports = null }, { protocol = \u0026#34;sctp\u0026#34; ports = null } ] deny = [] log_config = { metadata = \u0026#34;EXCLUDE_ALL_METADATA\u0026#34; } }] } module \u0026#34;asm_multi_mesh\u0026#34; { source = \u0026#34;terraform-google-modules/gcloud/google\u0026#34; version = \u0026#34;3.1.0\u0026#34; #description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/gcloud/google/3.1.0\u0026#34; platform = \u0026#34;linux\u0026#34; additional_components = [\u0026#34;kubectl\u0026#34;, \u0026#34;beta\u0026#34;] create_cmd_entrypoint = \u0026#34;${path.module}/scripts/create-mesh.sh\u0026#34; create_cmd_body = \u0026#34;${var.project_id} ${var.project_id}/${var.tokyo_location}/${var.tokyo_cluster} ${var.project_id}/${var.osaka_location}/${var.osaka_cluster}\u0026#34; module_depends_on = [module.asm_osaka.wait] } module \u0026#34;asm_mcs_api\u0026#34; { source = \u0026#34;terraform-google-modules/gcloud/google\u0026#34; version = \u0026#34;3.1.0\u0026#34; #description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/gcloud/google/3.1.0\u0026#34; platform = \u0026#34;linux\u0026#34; additional_components = [\u0026#34;kubectl\u0026#34;, \u0026#34;beta\u0026#34;] create_cmd_entrypoint = \u0026#34;gcloud\u0026#34; create_cmd_body = \u0026#34;container hub ingress enable --config-membership=${var.tokyo_cluster}\u0026#34; destroy_cmd_entrypoint = \u0026#34;gcloud\u0026#34; destroy_cmd_body = \u0026#34;container hub ingress disable\u0026#34; module_depends_on = [module.asm_multi_mesh.wait] } module \u0026#34;asm_tokyo_ingressgateway\u0026#34; { source = \u0026#34;terraform-google-modules/gcloud/google//modules/kubectl-wrapper\u0026#34; version = \u0026#34;3.1.0\u0026#34; #description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/gcloud/google/3.1.0/submodules/kubectl-wrapper\u0026#34; project_id = var.project_id cluster_name = var.tokyo_cluster cluster_location = var.tokyo_location kubectl_create_command = \u0026#34;kubectl apply -f ${path.module}/manifests/istio-ingressgateway-pods\u0026#34; kubectl_destroy_command = \u0026#34;kubectl delete ns istio-system --ignore-not-found\u0026#34; module_depends_on = [module.asm_mcs_api.wait] } module \u0026#34;asm_osaka_ingressgateway\u0026#34; { source = \u0026#34;terraform-google-modules/gcloud/google//modules/kubectl-wrapper\u0026#34; version = \u0026#34;3.1.0\u0026#34; #description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/gcloud/google/3.1.0/submodules/kubectl-wrapper\u0026#34; project_id = var.project_id cluster_name = var.osaka_cluster cluster_location = var.osaka_location kubectl_create_command = \u0026#34;kubectl apply -f ${path.module}/manifests/istio-ingressgateway-pods\u0026#34; kubectl_destroy_command = \u0026#34;kubectl delete ns istio-system --ignore-not-found\u0026#34; module_depends_on = [module.asm_tokyo_ingressgateway.wait] } module \u0026#34;asm_mcs_ingressgateway\u0026#34; { source = \u0026#34;terraform-google-modules/gcloud/google//modules/kubectl-wrapper\u0026#34; version = \u0026#34;3.1.0\u0026#34; #description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/gcloud/google/3.1.0/submodules/kubectl-wrapper\u0026#34; project_id = var.project_id cluster_name = var.tokyo_cluster cluster_location = var.tokyo_location kubectl_create_command = \u0026#34;kubectl apply -f ${path.module}/manifests/istio-ingressgateway-services\u0026#34; kubectl_destroy_command = \u0026#34;kubectl delete -f ${path.module}/manifests/istio-ingressgateway-services --ignore-not-found\u0026#34; module_depends_on = [module.asm_osaka_ingressgateway.wait] } modules/asm/variables.tf # ASMモジュールの外部変数を定義しています。\n作成例）./modules/asm/variables.tf\nvariable \u0026#34;project_id\u0026#34; {} variable \u0026#34;network\u0026#34; {} variable \u0026#34;tokyo_cluster\u0026#34; {} variable \u0026#34;tokyo_location\u0026#34; { default = \u0026#34;asia-northeast1\u0026#34; } variable \u0026#34;tokyo_pod_ip_range\u0026#34; {} variable \u0026#34;osaka_cluster\u0026#34; {} variable \u0026#34;osaka_location\u0026#34; { default = \u0026#34;asia-northeast2\u0026#34; } variable \u0026#34;osaka_pod_ip_range\u0026#34; {} variable \u0026#34;release_channel\u0026#34; { default = \u0026#34;STABLE\u0026#34; } modules/asm/scripts/install.sh # ASMのインストール処理を定義したスクリプトファイルです。ASM v11.0から正式ツールとなったasmcliコマンドを使用して、マネージドコントロールプレーン構成を作成しています。\n作成例）./modules/asm/scripts/install.sh\n#!/usr/bin/env bash set -e PROJECT_ID=${1} CLUSTER_NAME=${2} CLUSTER_LOCATION=${3} RELEASE_CHANNEL=${4} curl https://storage.googleapis.com/csm-artifacts/asm/asmcli \u0026gt; asmcli chmod +x asmcli ./asmcli install \\ --project_id ${PROJECT_ID} \\ --cluster_name ${CLUSTER_NAME} \\ --cluster_location ${CLUSTER_LOCATION} \\ --managed \\ --channel ${RELEASE_CHANNEL} \\ --enable-all modules/asm/scripts/destroy.sh # ASMの削除処理を定義したスクリプトファイルです。ASM関連のNamespaceを削除し、Anthosクラスタからの登録解除を実行しています。\n作成例）./modules/asm/scripts/destroy.sh\n#!/usr/bin/env bash set -e PROJECT_ID=${1} CLUSTER_NAME=${2} CLUSTER_LOCATION=${3} kubectl delete ns asm-system istio-system --ignore-not-found gcloud container hub memberships unregister ${CLUSTER_NAME} \\ --project=${PROJECT_ID} \\ --gke-cluster=${CLUSTER_LOCATION}/${CLUSTER_NAME} modules/asm/scripts/create-mesh.sh # マルチクラスタメッシュ作成処理を定義したスクリプトファイルです。\n作成例）./modules/asm/scripts/create-mesh.sh\n#!/usr/bin/env bash set -e PROJECT_ID=\u0026#34;${1}\u0026#34; CLUSTER_1=\u0026#34;${2}\u0026#34; CLUSTER_2=\u0026#34;${3}\u0026#34; curl https://storage.googleapis.com/csm-artifacts/asm/asmcli \u0026gt; asmcli chmod +x asmcli ./asmcli create-mesh ${PROJECT_ID} ${CLUSTER_1} ${CLUSTER_2} modules/asm/manifests/istio-ingressgateway-pods/* # Istio IngressゲートウェイコンテナのKubernetesマニフェストファイルです。GitHubにて公開されている次のサンプルをベースにしています。\n作成例）./modules/asm/manifests/istio-ingressgateway-pods/namespace.yaml\napiVersion: v1 kind: Namespace metadata: name: istio-system labels: istio.io/rev: asm-managed-stable 作成例）./modules/asm/manifests/istio-ingressgateway-pods/deployment.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: istio-ingressgateway namespace: istio-system spec: replicas: 3 selector: matchLabels: app: istio-ingressgateway istio: ingressgateway template: metadata: annotations: inject.istio.io/templates: gateway labels: app: istio-ingressgateway istio: ingressgateway spec: containers: - name: istio-proxy image: auto resources: limits: cpu: 2000m memory: 1024Mi requests: cpu: 100m memory: 128Mi serviceAccountName: istio-ingressgateway --- apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: istio-ingressgateway namespace: istio-system spec: maxUnavailable: 1 selector: matchLabels: istio: ingressgateway app: istio-ingressgateway --- apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata: name: istio-ingressgateway namespace: istio-system spec: maxReplicas: 5 metrics: - resource: name: cpu targetAverageUtilization: 80 type: Resource minReplicas: 3 scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: istio-ingressgateway 作成例）./modules/asm/manifests/istio-ingressgateway-pods/serviceaccount.yaml\napiVersion: v1 kind: ServiceAccount metadata: name: istio-ingressgateway namespace: istio-system 作成例）./modules/asm/manifests/istio-ingressgateway-pods/role.yaml\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: istio-ingressgateway namespace: istio-system rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;secrets\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: istio-ingressgateway namespace: istio-system roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: istio-ingressgateway subjects: - kind: ServiceAccount name: istio-ingressgateway modules/asm/manifests/istio-ingressgateway-services/* # Istio Ingressゲートウェイ用マルチクラスタIngress/ServiceのKubernetesマニフェストファイルです。\n作成例）./modules/asm/manifests/istio-ingressgateway-services/multiclusterservice.yaml\napiVersion: networking.gke.io/v1 kind: MultiClusterService metadata: name: istio-ingressgateway namespace: istio-system annotations: cloud.google.com/backend-config: \u0026#39;{\u0026#34;default\u0026#34;: \u0026#34;ingress-backendconfig\u0026#34;}\u0026#39; labels: app: istio-ingressgateway istio: ingressgateway spec: template: spec: ports: - name: status-port port: 15021 protocol: TCP targetPort: 15021 - name: http2 port: 80 - name: https port: 443 selector: istio: ingressgateway app: istio-ingressgateway 作成例）./modules/asm/manifests/istio-ingressgateway-services/backendconfig.yaml\napiVersion: cloud.google.com/v1 kind: BackendConfig metadata: name: ingress-backendconfig namespace: istio-system spec: healthCheck: requestPath: /healthz/ready port: 15021 type: HTTP 作成例）./modules/asm/manifests/istio-ingressgateway-services/multiclusteringress.yaml\napiVersion: networking.gke.io/v1beta1 kind: MultiClusterIngress metadata: name: istio-ingressgateway namespace: istio-system labels: app: istio-ingressgateway istio: ingressgateway spec: template: spec: backend: serviceName: istio-ingressgateway servicePort: 80 デプロイ用のCloud Buildパイプラインも書いてみたけれど、、、 # terraform init/plan/applyコマンドを順に実行するだけですが、手動だとどんなに簡単なコマンドであってもミスが生じてしまう可能性はあるためパイプライン化してみました。環境名のブランチpocに対してPushが入ったら起動するといったイメージにしております。\n、、、と本来であればこれでもパイプラインは動くはずなのですが、残念なことにTerraform公式asmサブモジュールv19.0.0(latest)、gcloudモジュールおよびkubectl-wrapperサブモジュールv3.1.0(latest)をTerraform公式Dockerイメージ上で動かすとエラーが発生してしまいます。非常に微妙ですが、今回のサンプルコードではDockerイメージをカスタマイズするか、あきらめて手動で実行をする必要がございます(TT)\n記事執筆時点(2022年1月末)では、Terraform公式asmサブモジュールv19.0.0(latest)、gcloudモジュールおよびkubectl-wrapperサブモジュールv3.1.0(latest)をTerraform公式Dockerイメージ上で動かすとエラーになりますのでご注意ください。 作成例）cloudbuild.yaml\nsubstitutions: _TERRAFORM_VERSION: 1.1.4 steps: - id: \u0026#34;terraform init\u0026#34; name: \u0026#34;hashicorp/terraform:${_TERRAFORM_VERSION}\u0026#34; entrypoint: \u0026#34;sh\u0026#34; args: - \u0026#34;-cx\u0026#34; - | cd environments/${BRANCH_NAME} terraform init -reconfigure - id: \u0026#34;terraform plan\u0026#34; name: \u0026#34;hashicorp/terraform:${_TERRAFORM_VERSION}\u0026#34; entrypoint: \u0026#34;sh\u0026#34; args: - \u0026#34;-cx\u0026#34; - | cd environments/${BRANCH_NAME} terraform plan -var=\u0026#34;project_id=${PROJECT_ID}\u0026#34; - id: \u0026#34;terraform apply\u0026#34; name: \u0026#34;hashicorp/terraform:${_TERRAFORM_VERSION}\u0026#34; entrypoint: \u0026#34;sh\u0026#34; args: - \u0026#34;-cx\u0026#34; - | cd environments/${BRANCH_NAME} terraform apply -auto-approve -var=\u0026#34;project_id=${PROJECT_ID}\u0026#34; エラーメッセージ出力例）\nmodule.asm.module.asm_tokyo.module.gcloud_kubectl.null_resource.additional_components[0]: Creating... module.asm.module.asm_tokyo.module.gcloud_kubectl.null_resource.additional_components[0]: Provisioning with \u0026#39;local-exec\u0026#39;... module.asm.module.asm_tokyo.module.gcloud_kubectl.null_resource.additional_components[0] (local-exec): Executing: [\u0026#34;/bin/sh\u0026#34; \u0026#34;-c\u0026#34; \u0026#34;.terraform/modules/asm.asm_tokyo/scripts/check_components.sh gcloud kubectl\u0026#34;] module.asm.module.asm_tokyo.module.gcloud_kubectl.null_resource.additional_components[0] (local-exec): /bin/sh: .terraform/modules/asm.asm_tokyo/scripts/check_components.sh: not found ╷ │ Error: local-exec provisioner error │ │ with module.asm.module.asm_tokyo.module.gcloud_kubectl.null_resource.additional_components[0], │ on .terraform/modules/asm.asm_tokyo/main.tf line 174, in resource \u0026#34;null_resource\u0026#34; \u0026#34;additional_components\u0026#34;: │ 174: provisioner \u0026#34;local-exec\u0026#34; { │ │ Error running command │ \u0026#39;.terraform/modules/asm.asm_tokyo/scripts/check_components.sh gcloud │ kubectl\u0026#39;: exit status 127. Output: /bin/sh: │ .terraform/modules/asm.asm_tokyo/scripts/check_components.sh: not found │ ╵ 終わりに # 今回はGKE+ASMのマルチクラスタメッシュ環境をTerraformを使って、しかも普段あまり積極的な活用はしないTerraform公式モジュールをあえて多用して^^; 構築してみましたがいかがだったでしょうか。もしこれから「ASM環境をTerraformで」と検討している方は参考にしてみてはいかがでしょうか。\nとはいえ、サンプルコードを書いといてなんですがASMの導入からはシェルスクリプトを多用した大変苦しい実装になっておりますし、個人的には現時点ではASMの導入以降は素直にTerraform以外を使用した方が良いと感じてます^^; とりあえず、苦しいことだけは伝わったかと思います。あくまで本記事はご参考ということで、その点ご承知おきいただけると幸いです。\nGoogle Cloud は、Google LLC の商標または登録商標です。 Terraform は、HashiCorp, Inc. の米国およびその他の国における商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"February 14, 2022","permalink":"/posts/2022/02/gcp-deploy-asm-with-terraform/","section":"記事一覧","summary":"みなさん、こんにちは。以前に「複数リージョンのGKEクラスタとAnthos Service Meshでマルチクラスタメッシュ環境を構築してみた」という記事を書いたのですが、今回はその環境をTerraformを使って構築してみました。もしこれから「ASM環境をTerraformで」と検討している方は参考にしてみてはいかがでしょうか。","title":"Terraformを使ってGKE+ASMのマルチクラスタメッシュ環境を構築してみた"},{"content":"","date":"January 13, 2022","permalink":"/tags/aws-management-console/","section":"タグ一覧","summary":"","title":"AWS Management Console"},{"content":"みなさん、こんにちは。AWS マネジメントコンソールを使っていると、ごくごく稀に表示言語を代えたくなることはありませんか。私は日本語⇔英語を切り替えたくなるケースがたまにあります。\nただ、個人的に「設定」と言えばなんとなく右上の項目からできそうなイメージがあり、「あれ、設定ってどこから変えるんだっけ、、、アカウントへ飛んだ先にあったっけ、、、(答え、アカウントへ飛んだ先にはない)」みたいにウッカリ設定方法を忘れて途方にくれてしまうことがあります。みなさんはそんな経験ございませんかね？ということで自分への備忘も込め、今回は表示言語の設定方法について紹介していきたいと思います。\n表示言語の変更方法 # では早速答えですが、言語の設定方法はAWS マネジメントコンソールの左下部分にございます。(なるほど、ここだったか、、、)\n終わりに # いまさらの情報でしたがいかがだったでしょうか。こんな記事でもだれかの役に立っていただければ幸いです。以上、AWS マネジメントコンソールの表示言語を変更する方法でした。\nAWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。 ","date":"January 13, 2022","permalink":"/posts/2022/01/aws-console-language-settings/","section":"記事一覧","summary":"みなさん、こんにちは。AWS マネジメントコンソールを使っていると、ごくごく稀に表示言語を代えたくなることはありませんか。私は日本語⇔英語を切り替えたくなるケースがたまにあります。","title":"AWSマネジメントコンソールの表示言語を変更する方法"},{"content":"","date":"December 20, 2021","permalink":"/tags/aws-cloudshell/","section":"タグ一覧","summary":"","title":"AWS CloudShell"},{"content":"みなさん、こんにちは。AWS CloudShellを使っているとファイルにペーストした際、勝手にインデントが入って「あー！」っとなったことの一度くらいはあるのではないでしょうか。今回はそんなときの解決方法を紹介していきたいと思います。\nインデントの自動挿入なしでペーストする方法 # いくつか方法はありますが、解の1つは「ペーストモードを使う」です。編集モードへ入る前に :set paste もしくは :set paste! を実行しましょう。ペーストモードにすると出力例のようにインデントが追加されずに期待通りの動きになりますね。\n起動時に自動で設定する方法 # とはいえ、エディタを起動するたびに毎回ペーストモードの設定をするのは面倒です。そんなときは vim の設定ファイル ~/.vimrc を作成しましょう。これでエディタが起動した際に自動で適用されるようになります。めでたしめでたし。\n作成例）~/.vimrc\nset paste 終わりに # いまさらの情報でしたがいかがだったでしょうか。もちろんコードを編集するときは自動でインデントを追加してくれるのはうれしいのですが、個人的にはAWS CloudShell上ではクリップボードからコピーしてくることも結構多いのでデフォルトペーストモードにしておき、必要に応じて :set nopaste もしくは :set paste! しております。よろしければ参考にしていただければと思います。\n以上、AWS CloudShell上のviエディタでインデントの自動挿入なしでペーストする方法でした。\nAWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。 ","date":"December 20, 2021","permalink":"/posts/2021/12/aws-cloudshell-autoindent-settings/","section":"記事一覧","summary":"みなさん、こんにちは。AWS CloudShellを使っているとファイルにペーストした際、勝手にインデントが入って「あー！」っとなったことの一度くらいはあるのではないでしょうか。今回はそんなときの解決方法を紹介していきたいと思います。","title":"AWS CloudShell上のviエディタでインデントの自動挿入なしでペーストする方法"},{"content":"","date":"December 20, 2021","permalink":"/tags/vim/","section":"タグ一覧","summary":"","title":"Vim"},{"content":"みなさん、こんにちは。今回はタイトルに掲げたとおり、最近の更新で Google Cloud コンソール(GUI)から Google Kubernetes Engine(GKE) Standard クラスタを作成する際に、次のようなすてきなオプションがプレビュー機能として追加されました。今回はこのオプションを使ってどのような構成が作られるのか実験したので共有したいと思います。\nいきなりですが、結論です！ # Q1. In-cluster とマネージドコントロールプレーンのどちらで構築される？ # 答え、マネージドコントロールプレーンにて構築されます。現時点でこの設定を変更することはできません。\nQ2. Anthos Service Mesh のバージョンは？ # 答え、GKE でリリースチャンネルを採用した場合は GKE と同じチャンネルになります。GKE で静的リリースを採用した場合は Reguler チャンネルとなります。現時点でこの設定を変更することはできません。\nQ3. カスタム CA を扱うことはできるか？ # 答え、扱えません。マネージドコントロールプレーンでの導入となるため、カスタム CA を扱うことができる Istio CA を選択することはできません。\nQ4. 限定クラスタにした場合は別途 15017/TCP を許可する必要があるか？ # 答え、不要です。ルールを追加しなくてもサイドカー自動インジェクションは問題なく動きます。\nQ5. Ingress ゲートウェイはデフォルトで作られるか？ # 答え、Ingress ゲートウェイはデフォルトでは作られません。別途ユーザにてデプロイする必要があります。\n終わりに # 今回は GKE クラスタ作成時の Anthos Service Mesh 有効化オプションの実験結果の共有でしたがいかがだったでしょうか。\nGKE と Anthos Service Mesh で採用するリリースチャンネルを変えたい、カスタム CA を使いたいといったケースでは従来どおり CLI を利用する必要がありますが、これらの要件がなければ GUI からポチポチするだけでとっても簡単に環境を作れるようになりそうですね。\n2021 年 12 月時点ではGUI の Anthos Service Mesh 有効化オプションはプレビュー段階であり、今回紹介した挙動から変わる可能性がありますのでご注意ください。 Google Cloud は、Google LLC の商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"December 16, 2021","permalink":"/posts/2021/12/gcp-new-asm-installation-options/","section":"記事一覧","summary":"みなさん、こんにちは。今回はタイトルに掲げたとおり、最近の更新で Google Cloud コンソール(GUI)から Google Kubernetes Engine(GKE) Standard クラスタを作成する際に、次のようなすてきなオプションがプレビュー機能として追加されました。今回はこのオプションを使ってどのような構成が作られるのか実験したので共有したいと思います。","title":"GKE クラスタ作成時のオプションで Anthos Service Mesh を有効化できるようになりました"},{"content":"みなさん、こんにちは。今回は単一リージョンに展開した複数 GKE クラスタを単一の Anthos Service Mesh 環境に追加し、GKE クラスタ間で負荷分散を行う方法についてご紹介していきたいと思います。\n複数 GKE クラスタでマルチクラスタメッシュを構築することにより、片方の GKE クラスタを先にバージョンアップし、サービスメッシュのトラフィック制御を使ってバージョンアップしたクラスタ側に少量のトラフィックを流して問題がないことを確認しながら段階的に比重をあげていく、といった「基盤部分も含めたカナリアリリース」のユースケースも容易に実現できるようになる見込みです。\nもちろん公式ドキュメントにもマルチクラスタメッシュの構築に関する記載はあるのですが、単にクラスタ間で分散されたことを確認しただけで終わっており、ルーティングの設定やメッシュの外からの通信に関する記載はなかったため、今回はここら辺も含めて一気通貫でご紹介したいと思います。もしこれから Anthos Service Mesh 環境の利用を検討している方は参考にしてみてはいかがでしょうか。\n構築するシステムについて # 次の図に示すように限定公開クラスタおよび承認済みネットワーク機能を有効化した単一リージョンの複数 GKE クラスタに対して Anthos Service Mesh (マネージドコントロールプレーン)を導入し、サービスメッシュ上でサンプルアプリケーションを動かしていきたいと思います。なお、今回の例では GKE、Anthos Service Mesh のいずれのリリースチャンネルについても安定性重視の Stable チャンネルを採用しています。\nそれでは構築していきましょう # 公式ドキュメントを参考にしつつ、公式ドキュメントに書かれていない部分を補足しながら構築をしていきたいと思います。\nStep1. VPC ネットワークの作成 # まずは GKE ノードを配置する VPC ネットワークおよび東京リージョンにサブネットを作成します。今回の例では GKE ノードからプライベートネットワーク経由で Artifact Registry などの他のマネージドサービスへアクセスできるように限定公開の Google アクセスをオンにしています。\n実行例）VPCネットワークの作成\n# 環境変数の設定 export NETWORK=\u0026#34;matt-vpc\u0026#34; export SUBNET=\u0026#34;matt-private-vm\u0026#34; export LOCATION=\u0026#34;asia-northeast1\u0026#34; export IP_RANGE=\u0026#34;172.16.0.0/16\u0026#34; # VPC ネットワークの作成 gcloud compute networks create ${NETWORK} --subnet-mode=custom # サブネットの作成 gcloud compute networks subnets create ${SUBNET} \\ --network=${NETWORK} --range=${IP_RANGE} --region=${LOCATION} \\ --enable-private-ip-google-access プライベートネットワーク経由でインターネット上の Docker Hub などへ接続できるよう Cloud NAT も作成しておきます。\n実行例）Cloud NATの作成\n# 環境変数の設定 export NAT_GATEWAY=\u0026#34;matt-tokyo-nat\u0026#34; export NAT_ROUTER=\u0026#34;matt-tokyo-router\u0026#34; # Cloud Router の作成 (東京リージョン) gcloud compute routers create ${NAT_ROUTER} \\ --network=${NETWORK} --region=${LOCATION} # Cloud NAT の作成 (東京リージョン) gcloud compute routers nats create ${NAT_GATEWAY} \\ --router=${NAT_ROUTER} \\ --router-region=${LOCATION} \\ --auto-allocate-nat-external-ips \\ --nat-all-subnet-ip-ranges \\ --enable-logging Step2. GKE クラスタの作成 # 次に GKE クラスタを作成していきましょう。 Anthos Service Mesh の導入には次のような前提条件を満たす必要があるため、今回はこちらを満たした上で、セキュリティの観点から限定公開クラスタおよび承認済みネットワークの有効化、安定性の観点から Stable チャンネルを指定しています。\n4 vCPU 以上を搭載したノード 合計 8 vCPU 以上を搭載したノードプール GKE Workload Identity の有効化 GKE リリースチャンネルへの登録 (※1) ※1: Anthos Service Mesh のマネージドコントロールプレーン機能を使う場合のみ\n実行例）GKEクラスタの作成\n# 環境変数の設定 export PROJECT_ID=`gcloud config list --format \u0026#34;value(core.project)\u0026#34;` export CLUSTER_1=\u0026#34;matt-tokyo-cluster-1\u0026#34; export CLUSTER_2=\u0026#34;matt-tokyo-cluster-2\u0026#34; export MASTER_IP_RANGE_1=\u0026#34;192.168.0.0/28\u0026#34; export MASTER_IP_RANGE_2=\u0026#34;192.168.8.0/28\u0026#34; export CTX_1=\u0026#34;gke_${PROJECT_ID}_${LOCATION}_${CLUSTER_1}\u0026#34; export CTX_2=\u0026#34;gke_${PROJECT_ID}_${LOCATION}_${CLUSTER_2}\u0026#34; # GKE クラスタ #1 の作成 gcloud container clusters create ${CLUSTER_1} \\ --region=${LOCATION} \\ --machine-type=\u0026#34;e2-standard-4\u0026#34; \\ --num-nodes=\u0026#34;1\u0026#34; \\ --enable-autoscaling --min-nodes=\u0026#34;1\u0026#34; --max-nodes=\u0026#34;3\u0026#34; \\ --enable-private-nodes --master-ipv4-cidr=${MASTER_IP_RANGE_1} \\ --enable-master-global-access \\ --enable-ip-alias --network=${NETWORK} --subnetwork=${SUBNET} \\ --enable-master-authorized-networks \\ --workload-pool=\u0026#34;${PROJECT_ID}.svc.id.goog\u0026#34; \\ --release-channel=\u0026#34;stable\u0026#34; # GKE クラスタ #2 の作成 gcloud container clusters create ${CLUSTER_2} \\ --region=${LOCATION} \\ --machine-type=\u0026#34;e2-standard-4\u0026#34; \\ --num-nodes=\u0026#34;1\u0026#34; \\ --enable-autoscaling --min-nodes=\u0026#34;1\u0026#34; --max-nodes=\u0026#34;3\u0026#34; \\ --enable-private-nodes --master-ipv4-cidr=${MASTER_IP_RANGE_2} \\ --enable-master-global-access \\ --enable-ip-alias --network=${NETWORK} --subnetwork=${SUBNET} \\ --enable-master-authorized-networks \\ --workload-pool=\u0026#34;${PROJECT_ID}.svc.id.goog\u0026#34; \\ --release-channel=\u0026#34;stable\u0026#34; 前提条件の詳細については次の公式ドキュメントをご参照ください。\nStep3. Anthos Service Mesh のインストール # (1) 管理ツールのダウンロード # 最初に Anthos Service Mesh v1.11 から正式な管理ツールとなった asmcli をダウンロードします。\n実行例）asmcliツールのダウンロード\ncurl https://storage.googleapis.com/csm-artifacts/asm/asmcli_1.11 \u0026gt; asmcli # 実行権限の付与 chmod +x asmcli (2) GKE クラスタ #1 へのインストール # まずは GKE クラスタ #1 に Anthos Service Mesh をインストールしていきましょう。Kubernetes API へ接続できるように GKE コントロールプレーンの承認済みネットワークに Cloud Shell の IP アドレスを登録し、kubectl を実行できるようにクラスタ認証情報を取得します。\n実行例）クラスタ認証情報の取得(クラスタ#1)\n# CloudShellの承認済みネットワーク登録 gcloud container clusters update ${CLUSTER_1} \\ --region ${LOCATION} \\ --enable-master-authorized-networks \\ --master-authorized-networks \\ \u0026#34;$(dig +short myip.opendns.com @resolver1.opendns.com)/32\u0026#34; # クラスタ認証情報の取得 gcloud container clusters get-credentials ${CLUSTER_1} \\ --region ${LOCATION} 次に asmcli を使って Anthos Service Mesh をインストールします。コマンドが完了するまでおおよそ 5 分程度かかりました。\n実行例）Anthos Service Meshのインストール(クラスタ#1)\n./asmcli install \\ --project_id ${PROJECT_ID} \\ --cluster_location ${LOCATION} \\ --cluster_name ${CLUSTER_1} \\ --managed \\ --channel \u0026#34;stable\u0026#34; \\ --enable-all \\ --output_dir ${CLUSTER_1} 次のようなメッセージが出力されましたらインストールに成功です。\n出力例）\nasmcli: Successfully installed ASM. (3) GKE クラスタ #2 へのインストール # 同様に GKE クラスタ #2 にも Anthos Service Mesh をインストールしましょう。\n実行例）Anthos Service Meshのインストール(クラスタ#2)\n# CloudShellの承認済みネットワーク登録 gcloud container clusters update ${CLUSTER_2} \\ --region ${LOCATION} \\ --enable-master-authorized-networks \\ --master-authorized-networks \\ \u0026#34;$(dig +short myip.opendns.com @resolver1.opendns.com)/32\u0026#34; # クラスタ認証情報の取得 gcloud container clusters get-credentials ${CLUSTER_2} \\ --region ${LOCATION} # Anthos Service Mesh のインストール ./asmcli install \\ --project_id ${PROJECT_ID} \\ --cluster_location ${LOCATION} \\ --cluster_name ${CLUSTER_2} \\ --managed \\ --channel \u0026#34;stable\u0026#34; \\ --enable-all \\ --output_dir ${CLUSTER_2} (4) ファイアウォールルールの更新 (限定公開クラスタ時のみ) # 限定公開クラスタに Anthos Service Mesh をインストールした場合は、コントロールプレーンからのポート 15017 による通信を追加で許可する必要があります。次のコマンド実行してコントロールプレーンからのポート 15017 による通信を許可します。\n実行例）ファイアウォールルールの更新(限定公開クラスタ時のみ)\n# 既存のファイアウォールルールに 15017/TCP の許可ルールを追加 (東京リージョン) gcloud compute firewall-rules update \\ $(gcloud compute firewall-rules list \\ --filter=\u0026#34;name~${CLUSTER_1}-.*-master\u0026#34; --format=\u0026#34;value(name)\u0026#34;) \\ --allow tcp:10250,tcp:443,tcp:15017 # 既存のファイアウォールルールに 15017/TCP の許可ルールを追加 (大阪リージョン) gcloud compute firewall-rules update \\ $(gcloud compute firewall-rules list \\ --filter=\u0026#34;name~${CLUSTER_2}-.*-master\u0026#34; --format=\u0026#34;value(name)\u0026#34;) \\ --allow tcp:10250,tcp:443,tcp:15017 Step4. マルチクラスタメッシュの設定 # (1) クラスタ間通信の許可 # クラスタをまたがってのサービス間通信ができるように次のコマンドを実行してファイアウォールルール \u0026quot;VPCネットワーク名\u0026quot;-istio-multicluster-pods を新たに作成します。\n実行例）クラスタ間通信の許可\n# 環境変数の設定 CLUSTER_1_CIDR=$(gcloud container clusters list \\ --filter=\u0026#34;name~${CLUSTER_1}\u0026#34; --format=\u0026#39;value(clusterIpv4Cidr)\u0026#39;) CLUSTER_2_CIDR=$(gcloud container clusters list \\ --filter=\u0026#34;name~${CLUSTER_2}\u0026#34; --format=\u0026#39;value(clusterIpv4Cidr)\u0026#39;) CLUSTER_1_NETTAG=$(gcloud compute instances list \\ --filter=\u0026#34;name~${CLUSTER_1::20}\u0026#34; --format=\u0026#39;value(tags.items.[0])\u0026#39; | \\ grep ${CLUSTER_1} | sort -u) CLUSTER_2_NETTAG=$(gcloud compute instances list \\ --filter=\u0026#34;name~${CLUSTER_2::20}\u0026#34; --format=\u0026#39;value(tags.items.[0])\u0026#39; | \\ grep ${CLUSTER_2} | sort -u) # クラスタ間通信を許可するファイアウォールルールの作成 gcloud compute firewall-rules create \u0026#34;${NETWORK}-istio-multicluster-pods\u0026#34; \\ --network=${NETWORK} \\ --allow=tcp,udp,icmp,esp,ah,sctp \\ --direction=INGRESS \\ --priority=900 \\ --source-ranges=\u0026#34;${CLUSTER_1_CIDR},${CLUSTER_2_CIDR}\u0026#34; \\ --target-tags=\u0026#34;${CLUSTER_1_NETTAG},${CLUSTER_2_NETTAG}\u0026#34; (2) クラスタ間サービスディスカバリの設定 # 次のコマンドを実行し、クラスタ間でサービスの自動検出ができるように asmcli を使って設定を行います。\n実行例）クラスタ間サービスディスカバリの設定\n./asmcli create-mesh ${PROJECT_ID} \\ ${PROJECT_ID}/${LOCATION}/${CLUSTER_1} \\ ${PROJECT_ID}/${LOCATION}/${CLUSTER_2} (3) シークレット情報の更新 (限定公開クラスタ時のみ) # 限定公開クラスタで構築した場合は、Anthos Service Mesh コントロールプレーンから他の GKE クラスタコントロールプレーンへプライベートネットワーク経由でアクセスできるようにシークレット情報を書き換えましょう。\n実行例）シークレット情報の更新(限定公開クラスタ時のみ)\n# 環境変数の設定 CLUSTER_1_PRIV_IP=$(gcloud container clusters describe \u0026#34;${CLUSTER_1}\u0026#34; \\ --region \u0026#34;${LOCATION}\u0026#34; --format \u0026#34;value(privateClusterConfig.privateEndpoint)\u0026#34;) CLUSTER_2_PRIV_IP=$(gcloud container clusters describe \u0026#34;${CLUSTER_2}\u0026#34; \\ --region \u0026#34;${LOCATION}\u0026#34; --format \u0026#34;value(privateClusterConfig.privateEndpoint)\u0026#34;) # プライベートエンドポイントに書き換えたシークレット情報の作成 (クラスタ#1) ./${CLUSTER_1}/istioctl x create-remote-secret \\ --context=${CTX_1} --name=${CLUSTER_1} \\ --server=https://${CLUSTER_1_PRIV_IP} \u0026gt; ${CTX_1}.secret # プライベートエンドポイントに書き換えたシークレット情報の作成 (クラスタ#2) ./${CLUSTER_1}/istioctl x create-remote-secret \\ --context=${CTX_2} --name=${CLUSTER_2} \\ --server=https://${CLUSTER_2_PRIV_IP} \u0026gt; ${CTX_2}.secret # シークレット情報の更新 (クラスタ#1) kubectl apply -f ${CTX_2}.secret --context=${CTX_1} # シークレット情報の更新 (クラスタ#2) kubectl apply -f ${CTX_1}.secret --context=${CTX_2} ここまで終わりましたらクラスタ間で Kubernetes サービスがロードバランシングされるようになります。\n(4) クラスタ間ロードバランシングの動作確認 # 構築はまだ続きますがいったんこの状態で、Anthos Service Mesh をインストールした際に \u0026ndash;output_dir で指定したディレクトリへ格納されているサンプルアプリケーションの中から HelloWorld と Sleep というアプリケーションを使用して、クラスタ間で負荷が分散されることを実際に確認していきたいと思います。サンプルアプリケーションの詳細につきましては次の URL をご参照ください。\n現時点では何もルーティング設定をしていないため、次の図のように 50% ずつトラフィックが振り分けられる状態を確認できるかと思います。\n(a) サンプルアプリケーションのデプロイ\nそれではサンプルアプリケーションをデプロイしていきましょう。まずは次のコマンドでサンプルアプリケーション用の Namespace を新たに作成します。\n実行例）サンプルアプリケーション用Namespaceの作成\n# 環境変数の設定 export SAMPLE_NAMESPACE=\u0026#34;sample\u0026#34; # 両クラスタにサンプルアプリケーション用 Namespace リソースの作成 for CTX in ${CTX_1} ${CTX_2} do kubectl create --context=${CTX} namespace ${SAMPLE_NAMESPACE} kubectl label --context=${CTX} namespace ${SAMPLE_NAMESPACE} \\ istio.io/rev=asm-managed-stable --overwrite done 次に HelloWorld および Sleep アプリケーションをデプロイしましょう。どちらのクラスタ上の Pod に振り分けられたかをわかりやすくするため、クラスタ #1 に HelloWorld アプリケーションの v1、クラスタ #2 に v2 をデプロイしています。\n実行例）サンプルアプリケーションのデプロイ\n# 両クラスタに HelloWorld サービスのデプロイ for CTX in ${CTX_1} ${CTX_2} do kubectl apply --context=${CTX} -n ${SAMPLE_NAMESPACE} \\ -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\ -l service=\u0026#34;helloworld\u0026#34; done # クラスタ #1 に HelloWorld アプリケーションの v1 をデプロイ kubectl apply --context=${CTX_1} -n ${SAMPLE_NAMESPACE} \\ -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\ -l version=\u0026#34;v1\u0026#34; # クラスタ #2 に HelloWorld アプリケーションの v2 をデプロイ kubectl apply --context=${CTX_2} -n ${SAMPLE_NAMESPACE} \\ -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\ -l version=\u0026#34;v2\u0026#34; # 両クラスタに Sleep サービス、アプリケーションのデプロイ for CTX in ${CTX_1} ${CTX_2} do kubectl apply --context=${CTX} -n ${SAMPLE_NAMESPACE} \\ -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/sleep/sleep.yaml done (b) サービス間通信の実行\nそれでは Sleep アプリケーションから HelloWorld アプリケーションへのサービス間通信をしてみましょう。次のコマンドでは各クラスタ上の Sleep アプリケーションからそれぞれ 10 回ずつ HelloWorld サービスへの通信を実施しています。\n実行例）サービス間通信の実行例\nfor CTX in ${CTX_1} ${CTX_2} do for x in `seq 1 10` do kubectl exec --context=\u0026#34;${CTX}\u0026#34; -n sample -c sleep \\ \u0026#34;$(kubectl get pod --context=\u0026#34;${CTX}\u0026#34; -n sample -l \\ app=sleep -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;)\u0026#34; \\ -- curl -sS helloworld.${SAMPLE_NAMESPACE}:5000/hello done echo \u0026#39;---\u0026#39; done 次の出力例のように両クラスタから v1 と v2 の Pod へランダムで 50% ずつトラフィックが振り分けられる状態を確認できるかと思います。\n出力例）\nHello version: v1, instance: helloworld-v1-776f57d5f6-62c9f Hello version: v1, instance: helloworld-v1-776f57d5f6-62c9f Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb : --- : Hello version: v1, instance: helloworld-v1-776f57d5f6-62c9f Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v1, instance: helloworld-v1-776f57d5f6-62c9f 以上でクラスタ間ロードバランシングの動作確認は完了です。\nStep5. 高度なクラスタ間ロードバランシングの設定 # 次にもう少し高度なクラスタ間ロードバランシングの設定をしていきましょう。ユースケースはいくつかありますが、今回は次の図のように GKE クラスタ #2 側の GKE クラスタのアップグレード後にアプリケーションの動作に影響がないかを少量のトラフィックを流して確認し、問題ないことを確認できたらその比重を段階的にあげていく、といったカナリアリリースのシナリオを想定した振り分け制御を行っていきたいと思います。\n(1) Istio リソースのデプロイ # それでは設定していきましょう。まずは Istio VirtualService リソース1および Istio DestinationRule リソース2の定義ファイルを作成しましょう。例のように VirtualService リソースにサブセットごとの振り分けの重みづけを、DestinationRule リソースにサブセットの定義をします。\n作成例）helloworld-virtualservice.yaml\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: helloworld spec: hosts: - helloworld http: - route: - destination: host: helloworld subset: v1 weight: 80 - destination: host: helloworld subset: v2 weight: 20 作成例）helloworld-destinationrule.yaml\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: helloworld spec: host: helloworld subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 次のコマンドで両クラスタに Istio リソースをデプロイしましょう。これで設定は終わりです。\n実行例）Istioリソースのデプロイ\n# 両クラスタに VirtualService リソースをデプロイ for CTX in ${CTX_1} ${CTX_2} do kubectl apply --context=${CTX} -n ${SAMPLE_NAMESPACE} \\ -f helloworld-virtualservice.yaml done # 両クラスタに DestinationRule リソースをデプロイ for CTX in ${CTX_1} ${CTX_2} do kubectl apply --context=${CTX} -n ${SAMPLE_NAMESPACE} \\ -f helloworld-destinationrule.yaml done (2) カナリアリリースの動作確認 # 構築はまだ続きますがいったんこの状態で、クラスタ間で負荷が設定どおりの比重で分散されることを実際に確認していきたいと思います。クラスタ間ロードバランシングの動作確認と同様に Sleep アプリケーションから HelloWorld アプリケーションへのサービス間通信をしてみましょう。\n実行例）サービス間通信の実行例\nfor x in `seq 1 10` do kubectl exec --context=\u0026#34;${CTX_1}\u0026#34; -n sample -c sleep \\ \u0026#34;$(kubectl get pod --context=\u0026#34;${CTX_1}\u0026#34; -n sample -l \\ app=sleep -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;)\u0026#34; \\ -- curl -sS helloworld.${SAMPLE_NAMESPACE}:5000/hello done 10 回の実行では試行回数が少ないため誤差はあるかと思いますが、出力例のように v1 への振り分けが約 80%、v2 への振り分けが約 20% となることが確認できるかと思います。\n出力例）\nHello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v 以上で少し高度なクラスタ間ロードバランシング設定の動作確認もできました。\nStep6. Ingress ゲートウェイの設定 # さてここからは話題がガラッと変わり、メッシュの外からの通信を受け入れるための Ingress ゲートウェイの設定をしていきたいと思います。今回は次の図のように各クラスタに配置された Ingress ゲートウェイアプリケーションを束ねるようにマルチクラスタ Ingress およびマルチクラスタ Service を配置する構成を作っていきます。\n(1) マルチクラスタ Ingress 機能の有効化 # 最初に次のコマンドを実行し、マルチクラスタ Ingress 機能を有効化しましょう。なお、今回はマルチクラスタ Ingress の設定を行うメインの GKE クラスタ(=構成クラスタ)として、GKE クラスタ #1 を登録しています。\n実行例）マルチクラスタIngress機能の有効化\ngcloud container hub ingress enable \\ --config-membership=${CLUSTER_1} (2) Ingress ゲートウェイ定義ファイルの作成 # 今回は Anthos Service Mesh をインストールした際に --output_dir で指定したディレクトリへ Ingress ゲートウェイのサンプル定義ファイルが配置されているのでこちらをベースに作成していきたいと思います。まずはサンプル定義ファイルを複製し、マルチクラスタ Ingress 構成向けに MultiClusterService3、BackendConfig4、MultiClusterIngress5 の 3 種類のリソース定義ファイルを追加していきましょう。\n実行例）Ingressゲートウェイ定義ファイルの作成準備\n# サンプル定義ファイルを複製 cp -r ${CLUSTER_1}/samples/gateways/istio-ingressgateway . # マルチクラスタ Ingress 定義ファイルを格納するディレクトリを作成 mkdir -p istio-ingressgateway/multicluster # Service リソース定義から MultiClusterService リソース定義ファイルに書き換え mv istio-ingressgateway/service.yaml istio-ingressgateway/multicluster/multiclusterservice.yaml # 新たな定義ファイルを 2 種類作成 touch istio-ingressgateway/multicluster/backendconfig.yaml touch istio-ingressgateway/multicluster/multiclusteringress.yaml それでは MultiClusterService3、BackendConfig4、MultiClusterIngress5 の 3 種類のリソース定義ファイルを編集していきましょう。MultiClusterService は Service リソースをマルチクラスタに対応させたリソースという位置づけのため、基本的に Service リソースの設定値とほぼ変わりません。今回は Ingress をフロントに配置するので LoadBalancer タイプの定義を削除し、デフォルトの Cluster IP に変更しています。\n作成例）istio-ingressgateway/multicluster/multiclusterservice.yaml（差分）\n- apiVersion: v1 + apiVersion: networking.gke.io/v1 - kind: Service + kind: MultiClusterService metadata: name: istio-ingressgateway + annotations: + cloud.google.com/backend-config: \u0026#39;{\u0026#34;default\u0026#34;: \u0026#34;ingress-backendconfig\u0026#34;}\u0026#39; labels: app: istio-ingressgateway istio: ingressgateway spec: - ports: - # status-port exposes a /healthz/ready endpoint that can be used with GKE Ingress health checks - - name: status-port - port: 15021 - protocol: TCP - targetPort: 15021 - # Any ports exposed in Gateway resources should be exposed here. - - name: http2 - port: 80 - - name: https - port: 443 - selector: - istio: ingressgateway - app: istio-ingressgateway - type: LoadBalancer + template: + spec: + ports: + # status-port exposes a /healthz/ready endpoint that can be used with GKE Ingress health checks + - name: status-port + port: 15021 + protocol: TCP + targetPort: 15021 + # Any ports exposed in Gateway resources should be exposed here. + - name: http2 + port: 80 + - name: https + port: 443 + selector: + istio: ingressgateway + app: istio-ingressgateway BackendConfig リソースではバックエンドサービスである Ingress ゲートウェイアプリケーションのヘルスチェックに関する定義を記載します。Ingress ゲートウェイはヘルスチェック用パスとして /healthz/ready:15021 を用意しているため、こちらを設定しましょう。\n作成例）istio-ingressgateway/multicluster/backendconfig.yaml（差分）\n+ apiVersion: cloud.google.com/v1 + kind: BackendConfig + metadata: + name: ingress-backendconfig + spec: + healthCheck: + requestPath: /healthz/ready + port: 15021 + type: HTTP MultiClusterIngress は Ingress リソースをマルチクラスタに対応させたリソースという位置づけであり、基本的に Ingress リソースを定義するときと設定値はほぼ同じです。\n作成例）istio-ingressgateway/multicluster/multiclusteringress.yaml（差分）\n+ apiVersion: networking.gke.io/v1beta1 + kind: MultiClusterIngress + metadata: + name: istio-ingressgateway + labels: + app: istio-ingressgateway + istio: ingressgateway + spec: + template: + spec: + backend: + serviceName: istio-ingressgateway + servicePort: 80 (3) Ingress ゲートウェイのデプロイ # まずは Ingress ゲートウェイリソースをデプロイする Namespace を新たに作成します。今回の例では istio-gateway という名前の Namespace を作成しています。\n実行例）Ingress Gateway用のNamespace作成\n# 環境変数の設定 export GATEWAY_NAMESPACE=\u0026#34;istio-gateway\u0026#34; # 両クラスタにサンプルアプリケーション用 Namespace リソースの作成 for CTX in ${CTX_1} ${CTX_2} do kubectl create --context=${CTX} namespace ${GATEWAY_NAMESPACE} kubectl label --context=${CTX} namespace ${GATEWAY_NAMESPACE} \\ istio.io/rev=asm-managed-stable --overwrite done 次のコマンドを実行して Ingress ゲートウェイアプリケーションを両クラスタにデプロイしましょう。\n実行例）Ingress Gatewayアプリケーションのデプロイ\nfor CTX in ${CTX_1} ${CTX_2} do kubectl apply -n ${GATEWAY_NAMESPACE} --context=${CTX} \\ -f istio-ingressgateway done 最後にマルチクラスタ Ingress リソースを構成クラスタである GKE クラスタ #1 に対してデプロイをしましょう。\n実行例）Ingress Gatewayアプリケーションのデプロイ\nkubectl apply -n ${GATEWAY_NAMESPACE} --context=${CTX_1} \\ -f istio-ingressgateway/multicluster 以上で Ingress ゲートウェイのデプロイは終わりです。\n(4) Istio リソースのデプロイ # Ingress ゲートウェイを通じてメッシュの外から HelloWorld アプリケーションへ通信ができるように Istio リソースの定義を行っていきたいと思います。まずは Istio Gateway リソース6および Istio VirtualService リソース1の定義ファイルを作成しましょう。例のように Gateway リソースにメッシュ外から受け付けるポートとプロトコルを定義し、VirtualService リソースには Gateway に入ってきた通信のパターンマッチ条件と振り分け先バックエンドの指定をします。\n作成例）helloworld-gateway.yaml\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: helloworld-gateway spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;*\u0026#34; --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: helloworld-gateway spec: hosts: - \u0026#34;*\u0026#34; gateways: - helloworld-gateway http: - match: - uri: exact: /hello route: - destination: host: helloworld port: number: 5000 次のコマンドで両クラスタに Istio リソースをデプロイし、アプリケーションへのインバウンド通信ができるように設定しましょう。\n実行例）Istioリソースのデプロイ\nfor CTX in ${CTX_1} ${CTX_2} do kubectl apply -n ${SAMPLE_NAMESPACE} --context=${CTX} \\ -f helloworld-gateway.yaml done 以上でメッシュ外からのアプリケーションへのインバウンド通信もできるようになりました。\n(5) インバウンド通信の動作確認 # それではメッシュ外からのアプリケーションへのインバウンド通信ができることを確認していきましょう。実行例のように Ingress ゲートウェイの外部 IP アドレスに対して curl コマンドを実行し、アクセスをしてみましょう。\n実行例）インバウンド通信の実行\n# Ingress ゲートウェイの外部 IP アドレスの取得 INGRESS_GATEWAY_IP=$(kubectl --context=${CTX_1} \\ -n ${GATEWAY_NAMESPACE} get MultiClusterIngress \\ -o custom-columns=VIP:status.VIP --no-headers) for x in `seq 1 10` do curl http://${INGRESS_GATEWAY_IP}/hello done 次の出力例のように両クラスタから v1 と v2 の Pod へランダムで 50% ずつトラフィックが振り分けられる状態を確認できるかと思います。\n出力例）\nHello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Step7. インバウンド通信に対する高度なロードバランシングの設定 # メッシュの外からアプリケーションへのインバウンド通信に対する高度なロードバランシングの設定をしていきましょう。今回は Step5 のときとは逆に 80% を v2、20% を v1 に割り振るように設定していきたいと思います。\n(1) Istio リソースの更新 # それでは先ほど作成した Istio リソース定義ファイル helloworld-gateway.yaml の VirtualService リソース部分を編集し、サブセットごとの振り分け比重の定義を追加します。なお、サブセットの定義(DestinationRule リソース)については「Step5. 高度なクラスタ間ロードバランシングの設定」にて設定済みとなりますのでここでは省略します。\n作成例）helloworld-gateway.yaml（差分）\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: helloworld-gateway spec: hosts: - \u0026#34;*\u0026#34; gateways: - helloworld-gateway http: - match: - uri: exact: /hello route: - destination: host: helloworld port: number: 5000 + subset: v1 + weight: 20 + - destination: + host: helloworld + port: + number: 5000 + subset: v2 + weight: 80 それでは次のコマンドで Istio リソースを更新しましょう。これで設定は終わりです。\n実行例）Istioリソースの更新\nfor CTX in ${CTX_1} ${CTX_2} do kubectl apply -n ${SAMPLE_NAMESPACE} --context=${CTX} \\ -f helloworld-gateway.yaml done (2) カナリアリリースの動作確認 # それでは HelloWorld アプリケーションへの振り分けが設定どおりの比重で分散されることを実際に確認していきたいと思います。実行例のように Ingress ゲートウェイの外部 IP アドレスに対して curl コマンドを実行し、アクセスをしてみましょう。\n実行例）インバウンド通信の実行\n# Ingress ゲートウェイの外部 IP アドレスの取得 INGRESS_GATEWAY_IP=$(kubectl --context=${CTX_1} \\ -n ${GATEWAY_NAMESPACE} get MultiClusterIngress \\ -o custom-columns=VIP:status.VIP --no-headers) for x in `seq 1 10` do curl http://${INGRESS_GATEWAY_IP}/hello done 10 回の実行では試行回数が少ないため誤差はあるかと思いますが、出力例のように v1 への振り分けが約 20%、v2 への振り分けが約 80% となることが確認できるかと思います。\n出力例）\nHello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb 以上でメッシュの外からアプリケーションへのインバウンド通信に対する高度なロードバランシングの動作確認も終了です。お疲れ様でした。\n終わりに # 今回は単一リージョンに展開した複数 GKE クラスタを単一の Anthos Service Mesh 環境に追加し、GKE クラスタ間で負荷分散を行う方法についてご紹介でしたがいかがだったでしょうか。\n複数 GKE クラスタでマルチクラスタメッシュを構築することにより、片方の GKE クラスタを先にバージョンアップし、サービスメッシュのトラフィック制御を使ってバージョンアップしたクラスタ側に少量のトラフィックを流して問題がないことを確認しながら段階的に比重をあげていく、といった「基盤部分も含めたカナリアリリース」のユースケースも容易に実現できるようになる見込みです。もしこれから Anthos Service Mesh 環境の利用を検討している方はマルチクラスタメッシュ構成についても検討してみてはいかがでしょうか。\nGoogle Cloud は、Google LLC の商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 VirtualService はトラフィックの振り分け、ルーティングを定義する Istio リソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDestinationRule は転送先サービスのサブセット化や各種トラフィックポリシーを定義する Istio リソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMultiClusterService は Service リソースを複数のクラスタ上に展開する GKE 独自のカスタムリソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBackendConfig はバックエンドサービスのヘルスチェックを定義する GKE 独自のカスタムリソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMultiClusterIngress はマルチクラスタに対応した Ingress リソースを定義する GKE 独自のカスタムリソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGateway は Ingress/Egress ゲートウェイで受け付けるポート、プロトコルを定義する Istio リソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"December 15, 2021","permalink":"/posts/2021/12/gcp-multi-asm-cluster/","section":"記事一覧","summary":"みなさん、こんにちは。今回は単一リージョンに展開した複数 GKE クラスタを単一の Anthos Service Mesh 環境に追加し、GKE クラスタ間で負荷分散を行う方法についてご紹介していきたいと思います。","title":"単一リージョンの複数 GKE クラスタと Anthos Service Mesh でマルチクラスタメッシュ環境を構築してみた"},{"content":"みなさん、こんにちは。今回は Amazon Elastic Kubernetes Service(EKS) を利用する際に併せて利用したい AWS Load Balancer Controller のお話です。\nみなさんは Amazon EKS を活用して Kubernetes クラスタを AWS 上で動かすとなった際に、他のマネージドサービスの利用はどうされていますか。もちろんすべて Kubernetes 上で動かしてシステムを完結させるという選択肢もあるかと思いますが、やはり多くの方が他の AWS のマネージドサービスの併用も検討されるのではないでしょうか。その一方で、これら併用環境のコード化 (IaC、Infrastructure as Code) を実現しようとすると、Kubernetes アプリケーションの管理は Helm で、AWS リソースの管理は Terraform で、などという別々のツールでの管理になってしまいがちです。\nそんな悩みを解決する1つの手段が AWS Load Balancer Controller や AWS Controllers for Kubernetes といった Kubernetes クラスタ機能を拡張する各種コントローラの活用です。これらのコントローラを利用することで、AWS リソースについても Kubernetes マニフェストファイルで定義できるようになり、Kubernetes 側に運用管理を寄せてシンプル化できます。\n今回はそのうちの1つ、Elastic Load Balancing(ELB) を Kubernetes クラスタで管理できるようにする AWS Load Balancer Controller について、簡単なサンプルアプリケーションを交えて紹介していきたいと思います。これから Amazon EKS 上にアプリケーションを展開しようと考えている方は参考にしてみてはいかがでしょうか。\nAWS Load Balancer Controller とは # AWS Load Balancer Controller (旧AWS ALB Ingress Controller) は、ELB を Kubernetes クラスタから管理するためのコントローラです。このコントローラを活用することで、Kubernetes Ingress リソースとして L7 ロードバランサの Application Load Balancer(ALB) を、Kuberntes Service リソースとして L4 ロードバランサの Network Load Balancer(NLB) を利用することができるようになります。\nKubernetes Service/Ingress リソースでの処理を外部ロードバランサである ELB へ切り出すことによって、ワークロードへの性能影響の低減、ノードリソースの利用効率の向上、Service/Ingress リソースのスケーリングなどを AWS 側へ任せることで運用負荷の低減といったことができる見込みです。\nAWS Load Balancer Controller を導入してみよう # Step1. 作業環境の設定 # 今回は Amazon EKS や AWS Load Balancer Controller の管理を行う環境として AWS CloudShell を利用していきたいと思います。まずは操作に必要な各種ツールの設定を AWS CloudShell にしてきましょう。\nAWS CLI の設定 # AWS リソースの操作を行えるように次のコマンドを実行し、AWS CLI の設定を行いましょう。\n実行例）AWS CLIの設定\naws configure kubectl コマンドのインストール # 次に Kubernetes 管理ツールの kubectl コマンドをインストールしましょう。\n実行例）kubectlコマンドのインストール\n# kubectl コマンドのダウンロード curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.21.2/2021-07-05/bin/linux/amd64/kubectl # 実行権限の付与 chmod +x kubectl # 実行ファイルのパスを設定 mkdir -p ${HOME}/bin \u0026amp;\u0026amp; mv kubectl ${HOME}/bin \u0026amp;\u0026amp; export PATH=${PATH}:${HOME}/bin # シェルの起動時に $HOME/bin をパスへ追加 echo \u0026#39;export PATH=${PATH}:${HOME}/bin\u0026#39; \u0026gt;\u0026gt; ~/.bashrc # インストールが成功していることを確認 kubectl version --short --client インストールに成功していれば出力例のようにバージョン情報の出力を確認できます。\n出力例）\n$ kubectl version --short --client Client Version: v1.21.2-13+d2965f0db10712 eksctl コマンドのインストール # 続いて Amazon EKS 管理ツールの eksctl コマンドをインストールしましょう。\n実行例）eksctlコマンドのインストール\n# eksctl の最新バージョンをダウンロード curl -L \u0026#34;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; | tar xz -C /tmp # 実行ファイルをパスの通ったディレクトリへ移動 mv /tmp/eksctl ${HOME}/bin # インストールが成功していることを確認 eksctl version インストールに成功していれば出力例のようにバージョン情報の出力を確認できます。\n出力例）\n$ eksctl version 0.76.0 helm コマンドのインストール # 最後に Kubernetes 上で稼働するアプリケーションを管理するためのツールである helm コマンドをインストールしましょう。\n実行例）helmコマンドのインストール\n# 前提パッケージのインストール sudo yum install -y openssl # インストールスクリプトのダウンロード curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 \u0026gt; get_helm.sh # 実行権限の付与 chmod 700 get_helm.sh # インストールスクリプトの実行 ./get_helm.sh # インストールが成功していることを確認 helm version --short インストールに成功していれば出力例のようにバージョン情報の出力を確認できます。\n出力例）\n$ helm version --short v3.7.2+g663a896 以上で作業環境(AWS CloudShell)の設定は完了です。\nStep2. EKS クラスタの作成 # 続いて AWS Load Balancer Controller を導入する対象の Amazon EKS クラスタを作成していきましょう。今回は Kubernetes ノードには AWS Fargate を使用していきたいと思います。それでは eksctl コマンドを実行してクラスタを作成しましょう。\n実行例）EKSクラスタの作成\n# 環境変数の設定 export CLUSTER=\u0026#34;my-tokyo-cluster\u0026#34; # EKS クラスタの作成 eksctl create cluster --name ${CLUSTER} --version 1.21 --fargate # サービスアカウントでの IAM ロール使用を許可 eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER} --approve ここまで終わりましたら AWS Load Balancer Controller を利用するための事前準備は完了です。\nStep3. コントローラのデプロイ # それでは AWS Load Balancer Controller を Amazon EKS クラスタにデプロイしていきましょう。\nサービスアカウントの作成 # まずは AWS Load Balancer Controller 用のサービスアカウントの作成を行っていきます。今回は kube-system Namespace に aws-load-balancer-controller という名前でサービスアカウントを作成して行きたいと思います。それでは次のコマンドを実行してサービスアカウントを作成しましょう。\n実行例）サービスアカウントの作成\nAWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \u0026#34;Account\u0026#34; --output text) POLICY_NAME=\u0026#34;myAWSLoadBalancerControllerIAMPolicy\u0026#34; SERVICE_ACCOUNT=\u0026#34;aws-load-balancer-controller\u0026#34; # IAM ポリシー定義ファイルのダウンロード curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json # IAM ポリシーの作成 aws iam create-policy \\ --policy-name ${POLICY_NAME} \\ --policy-document file://iam_policy.json # サービスアカウントの作成 eksctl create iamserviceaccount \\ --name=${SERVICE_ACCOUNT} \\ --cluster=${CLUSTER} \\ --namespace=\u0026#34;kube-system\u0026#34; \\ --attach-policy-arn=arn:aws:iam::${AWS_ACCOUNT_ID}:policy/${POLICY_NAME} \\ --override-existing-serviceaccounts \\ --approve コントローラのインストール # 次に AWS Load Balancer Controller をインストールしていきましょう。なお、AWS Load Balancer Controller のインストール方法はいくつか用意されていますが、AWS Fargate 環境の場合は Helm を利用したインストール方法を選択する必要があるためご注意ください。\n実行例）コントローラのインストール\n# EKS 用の Helm レポジトリを追加 helm repo add eks https://aws.github.io/eks-charts # TargetGroupBinding カスタムリソースをインストール kubectl apply -k \u0026#34;github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\u0026#34; # Helm チャートからインストール helm install aws-load-balancer-controller eks/aws-load-balancer-controller \\ --set clusterName=${CLUSTER} \\ --set serviceAccount.create=false \\ --set vpcId=$(aws cloudformation describe-stacks \\ --stack-name \u0026#34;eksctl-${CLUSTER}-cluster\u0026#34; \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`VPC`].OutputValue\u0026#39; \\ --output text) \\ --set serviceAccount.name=${SERVICE_ACCOUNT} \\ -n kube-system # 確認 kubectl get deployment -n kube-system aws-load-balancer-controller インストールに成功していれば出力例のようにコントローラが一覧へ表示されるようになります。\n出力例）\n$ kubectl get deployment -n kube-system aws-load-balancer-controller NAME READY UP-TO-DATE AVAILABLE AGE aws-load-balancer-controller 2/2 2 2 42s 以上で AWS Load Balancer Controller の導入は完了です。\nAWS Load Balancer Controller を使ってみよう # Network Load Balancer (Serviceリソース) の使い方 # AWS Load Balancer Controller 環境では、次のサンプルのように Kubernetes Service リソース定義にて「LoadBalancer タイプの指定」と「アノテーションとして各種パラメータを指定」をすることによって Network Load Balancer(NLB) をプロビジョニングできます。\n作成例）Serviceリソース定義サンプル\napiVersion: v1 kind: Service metadata: name: sample-service annotations: service.beta.kubernetes.io/aws-load-balancer-type: external service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip spec: type: LoadBalancer ports: - port: 80 protocol: TCP selector: app: sample-app アノテーションに指定する主要なパラメータとしては次の通りです。アノテーションに指定可能なパラメータの一覧につきましては次の URL を参照してください。\nアノテーション名 説明 service.beta.kubernetes.io/aws-load-balancer-type ロードバランサのプロビジョニングに使用するコントローラを指定します。AWS Load Balancer Controller の場合は \u0026ldquo;external\u0026rdquo; を指定します。 service.beta.kubernetes.io/aws-load-balancer-scheme ロードバランサのスキームを指定します。内部向けの場合は \u0026ldquo;internal\u0026quot;、インターネット向けの場合は \u0026ldquo;internet-facing\u0026rdquo; を指定します。 service.beta.kubernetes.io/aws-load-balancer-nlb-target-type バックエンドに指定するターゲットタイプを指定します。トラフィックを直接 Pod にルーティングするには \u0026ldquo;ip\u0026rdquo; を指定します。 service.beta.kubernetes.io/aws-load-balancer-healthcheck-port バックエンドのヘルスチェック用ポート(\u0026rdquo;traffic-port\u0026quot;/\u0026quot;ポート番号\u0026quot;)を指定します。デフォルトは \u0026ldquo;traffic-port\u0026rdquo; です。 service.beta.kubernetes.io/aws-load-balancer-healthcheck-protocol バックエンドのヘルスチェック用プロトコル(\u0026quot;tcp\u0026quot;/\u0026quot;http\u0026quot;/\u0026quot;https\u0026quot;)を指定します。デフォルトは \u0026ldquo;tcp\u0026rdquo; です。 service.beta.kubernetes.io/aws-load-balancer-healthcheck-path バックエンドの HTTP/HTTPS ヘルスチェック用パスを指定します。デフォルトは \u0026ldquo;/\u0026rdquo; です。 NLB Service のサンプル # それではサンプルアプリケーションをデプロイして NLB Service リソースを実際に動かしてみましょう。今回は次のサンプルアプリケーションをベースに少しだけ手を加えたものを用いて動作確認をしていきたいと思います。\n作成例）helloworld-nlb-sample.yaml (サンプルアプリケーション定義ファイル)\napiVersion: v1 kind: Service metadata: name: helloworld labels: app: helloworld annotations: service.beta.kubernetes.io/aws-load-balancer-type: external service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip service.beta.kubernetes.io/aws-load-balancer-healthcheck-protocol: http service.beta.kubernetes.io/aws-load-balancer-healthcheck-path: /health spec: type: LoadBalancer ports: - port: 80 targetPort: 5000 name: http selector: app: helloworld --- apiVersion: apps/v1 kind: Deployment metadata: name: helloworld-v1 labels: app: helloworld version: v1 spec: replicas: 1 selector: matchLabels: app: helloworld version: v1 template: metadata: labels: app: helloworld version: v1 spec: containers: - name: helloworld image: docker.io/istio/examples-helloworld-v1 ports: - containerPort: 5000 それでは次のコマンドを実行してサンプルアプリケーションをデプロイしていきましょう。\n実行例）サンプルアプリケーションのデプロイ\nexport FARGATEPROFILE=\u0026#34;sample-app\u0026#34; export NAMESPACE=\u0026#34;sample\u0026#34; # Fargate プロファイルの作成 eksctl create fargateprofile --cluster ${CLUSTER} --name ${FARGATEPROFILE} --namespace ${NAMESPACE} # サンプルアプリケーション用 Namespace の作成 kubectl create namespace ${NAMESPACE} # サンプルアプリケーションのデプロイ kubectl apply -n ${NAMESPACE} -f helloworld-nlb-sample.yaml # サービスがデプロイされたことを確認 kubectl get -n ${NAMESPACE} service helloworld NLB Service のデプロイに成功した場合は次の出力例のように Service リソース一覧に表示されます。\n出力例）\n$ kubectl get -n ${NAMESPACE} service helloworld NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE helloworld LoadBalancer 10.100.114.79 k8s-sample-hellowor-xxxxxxxxxx-xxxxxxxxxxxxxxxx.elb.ap-northeast-1.amazonaws.com 80:32642/TCP 32h NLB のヘルスチェックが正常になるまで数分待った後、NLB のパブリックエンドポイントにアクセスしてみましょう。\n実行例）サンプルアプリケーションへアクセス\n# NLB の DNS 名を取得 ENDPOINT=$(kubectl get -n ${NAMESPACE} service helloworld \\ -o custom-columns=HOSTNAME:status.loadBalancer.ingress[0].hostname \\ --no-headers) # テストの実行 curl http://${ENDPOINT}/hello 期待通りのレスポンスが返ってくることが確認できたら成功です。\n出力例）\n$ curl http://${ENDPOINT}/hello Hello version: v1, instance: helloworld-v1-b9d9d6679-hdqsq 最後にサンプルアプリケーションを削除して動作確認は終了です。お疲れ様でした。\n実行例）サンプルアプリケーションの削除\nkubectl delete namespace ${NAMESPACE} Application Load Balancer (Ingressリソース) の使い方 # AWS Load Balancer Controller 環境では、次のサンプルのように「Kubernetes IngressClass リソースにてコントローラの指定」を、「Kubernetes Ingress リソース定義のアノテーションとして各種パラメータを指定」をすることによって Application Load Balancer(ALB) をプロビジョニングできます。\nKubernetes Ingress リソースのアノテーションに kubernetes.io/ingress.class: alb を指定することでも作成できますが、Kubernetes 1.18 以降は kubernetes.io/ingress.class の利用は非推奨となっておりますのでご注意ください。 作成例）Ingressリソース定義サンプル\napiVersion: networking.k8s.io/v1 kind: IngressClass metadata: name: sample-ingress-class spec: controller: ingress.k8s.aws/alb --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: sample-ingress annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip spec: ingressClassName: sample-ingress-class rules: - http: paths: - path: /hello pathType: Exact backend: service: name: sample-service port: number: 80 アノテーションに指定する主要なパラメータとしては次の通りです。なお、ALB Ingress は AWS WAF や AWS Shield による脅威からの保護、Amazon Cognito 認証などさまざまなマネージドサービスとの連携が可能です。アノテーションに指定可能なパラメータの一覧につきましては次の URL を参照してください。\nアノテーション名 説明 alb.ingress.kubernetes.io/group.name 複数の Ingress リソースを 1 つの ALB に統合する際にグループ名を指定します。 alb.ingress.kubernetes.io/scheme ロードバランサのスキームを指定します。内部向けの場合は \u0026ldquo;internal\u0026quot;、インターネット向けの場合は \u0026ldquo;internet-facing\u0026rdquo; を指定します。 alb.ingress.kubernetes.io/listen-ports ロードバランサの受付ポートを指定します。デフォルトは \u0026rsquo;[{\u0026ldquo;HTTP\u0026rdquo;: 80}]\u0026rsquo; | \u0026lsquo;[{\u0026ldquo;HTTPS\u0026rdquo;: 443}]\u0026rsquo; です。 alb.ingress.kubernetes.io/target-type バックエンドに指定するターゲットタイプを指定します。トラフィックを直接 Pod にルーティングするには \u0026ldquo;ip\u0026rdquo; を指定します。 alb.ingress.kubernetes.io/healthcheck-port バックエンドのヘルスチェック用ポート(\u0026rdquo;traffic-port\u0026quot;/\u0026quot;ポート番号\u0026quot;)を指定します。デフォルトは \u0026ldquo;traffic-port\u0026rdquo; です。 alb.ingress.kubernetes.io/healthcheck-protocol バックエンドのヘルスチェック用プロトコル(\u0026quot;http\u0026quot;/\u0026quot;https\u0026quot;)を指定します。デフォルトは \u0026ldquo;http\u0026rdquo; です。 alb.ingress.kubernetes.io/healthcheck-path バックエンドの HTTP/HTTPS ヘルスチェック用パスを指定します。デフォルトは \u0026ldquo;/\u0026rdquo; です。 ALB Ingress のサンプル # それではサンプルアプリケーションをデプロイして ALB Ingress リソースを実際に動かしてみましょう。今回は次のサンプルアプリケーションをベースに少しだけ手を加えたものを用いて動作確認をしていきたいと思います。\n作成例）helloworld-alb-sample.yaml (サンプルアプリケーション定義ファイル)\napiVersion: networking.k8s.io/v1 kind: IngressClass metadata: name: alb-ingress spec: controller: ingress.k8s.aws/alb --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: helloworld annotations: alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/healthcheck-path: /health spec: ingressClassName: alb-ingress rules: - http: paths: - path: /hello pathType: Exact backend: service: name: helloworld port: number: 80 --- apiVersion: v1 kind: Service metadata: name: helloworld labels: app: helloworld service: helloworld spec: ports: - port: 80 targetPort: 5000 name: http selector: app: helloworld --- apiVersion: apps/v1 kind: Deployment metadata: name: helloworld-v1 labels: app: helloworld version: v1 spec: replicas: 1 selector: matchLabels: app: helloworld version: v1 template: metadata: labels: app: helloworld version: v1 spec: containers: - name: helloworld image: docker.io/istio/examples-helloworld-v1 ports: - containerPort: 5000 それでは次のコマンドを実行してサンプルアプリケーションをデプロイしていきましょう。\n実行例）サンプルアプリケーションのデプロイ\nexport FARGATEPROFILE=\u0026#34;sample-app\u0026#34; export NAMESPACE=\u0026#34;sample\u0026#34; # Fargate プロファイルの作成 eksctl create fargateprofile --cluster ${CLUSTER} --name ${FARGATEPROFILE} --namespace ${NAMESPACE} # サンプルアプリケーション用 Namespace の作成 kubectl create namespace ${NAMESPACE} # サンプルアプリケーションのデプロイ kubectl apply -n ${NAMESPACE} -f helloworld-alb-sample.yaml # Ingress がデプロイされたことを確認 kubectl get -n ${NAMESPACE} ingress helloworld ALB Ingress のデプロイに成功した場合は次の出力例のように Ingress リソース一覧に表示されます。\n出力例）\n$ kubectl get -n ${NAMESPACE} ingress helloworld NAME CLASS HOSTS ADDRESS PORTS AGE helloworld \u0026lt;none\u0026gt; * k8s-sample-hellowor-xxxxxxxxxx-xxxxxxxx.ap-northeast-1.elb.amazonaws.com 80 70s DNS への登録などが反映されるまで数分待った後、ALB のパブリックエンドポイントにアクセスしてみましょう。\n実行例）サンプルアプリケーションへアクセス\n# NLB の DNS 名を取得 ENDPOINT=$(kubectl get -n ${NAMESPACE} ingress helloworld \\ -o custom-columns=HOSTNAME:status.loadBalancer.ingress[0].hostname \\ --no-headers) # テストの実行 curl http://${ENDPOINT}/hello 期待通りのレスポンスが返ってくることが確認できたら成功です。\n出力例）\n$ curl http://${ENDPOINT}/hello Hello version: v1, instance: helloworld-v1-b9d9d6679-rpfzl 最後にサンプルアプリケーションを削除して動作確認は終了です。お疲れ様でした。\n実行例）サンプルアプリケーションの削除\nkubectl delete namespace ${NAMESPACE} 終わりに # 今回は Amazon Elastic Kubernetes Service(EKS) を利用する際に併せて利用したい AWS Load Balancer Controller のご紹介でしたが、いかがだったでしょうか？\n今回は IaC を実現する 1 つの手段として、というカットでの紹介でしたが Kubernetes Ingress/Service リソースを ELB にすることでさまざまなメリットが期待できますので、Amazon EKS をご利用の際は AWS Load Balancer Controller の併用も視野に入れてみてはいかがでしょうか。\nなお、今回は紹介しませんでしたが AWS Load Balancer Controller は使いたいけれど、ELB リソースのライフサイクルは Kubernetes からは切り離したいというケースもあるかと思います。そういった場合は AWS Load Balancer Controller の TargetGroupBinding 機能を利用することで実現可能なので参考にしていただければと思います。TargetGroupBinding 機能の詳細については公式ドキュメントを参照してください。\n以上、Kubernetes Service/Ingress リソースと Elastic Load Balancing(ELB) との統合を実現する「AWS Load Balancer Controller」のご紹介でした。\nAWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 Kubernetes は、The Linux Foundation の米国およびその他の国における登録商標または商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。 ","date":"December 14, 2021","permalink":"/posts/2021/12/aws-load-balancer-controller/","section":"記事一覧","summary":"みなさん、こんにちは。今回は Amazon Elastic Kubernetes Service(EKS) を利用する際に併せて利用したい AWS Load Balancer Controller のお話です。","title":"AWS Load Balancer Controller を使って ELB を Kubernetes のマニフェストファイルで管理しよう"},{"content":"みなさん、こんにちは。今回は複数のリージョンに展開する各 GKE クラスタを単一の Anthos Service Mesh 環境に追加し、GKE クラスタ間で負荷分散を行う方法についてご紹介していきたいと思います。\n複数リージョンの GKE クラスタでマルチクラスタメッシュを構築することにより、予期しない大規模災害の発生にも耐えうる高い可用性と回復力の実現、エンドユーザからより近い位置への振り分けによるレイテンシの改善といったことが期待できるようになる見込みです。\nもちろん公式ドキュメントにもマルチクラスタメッシュの構築に関する記載はあるのですが、単にクラスタ間で分散されたことを確認しただけで終わっており、ローカリティを意識したルーティングの設定やメッシュの外からの通信に関する記載はなかったため、今回はここら辺も含めて一気通貫でご紹介したいと思います。もしこれからリージョンをまたがった Anthos Service Mesh 環境の利用を検討している方は参考にしてみてはいかがでしょうか。\n構築するシステムについて # 次の図に示すように限定公開クラスタおよび承認済みネットワーク機能を有効化した複数リージョンの GKE クラスタに対して Anthos Service Mesh (マネージドコントロールプレーン)を導入しています。サービスメッシュ上ではサンプルアプリケーションを動かし、ローカリティを意識した負荷分散についても設定をしていきたいと思います。なお、今回の例では GKE、Anthos Service Mesh のいずれのリリースチャンネルについても安定性重視の Stable チャンネルを採用しています。\nそれでは構築していきましょう # いつも通り公式ドキュメントを参考にしつつ、公式ドキュメントに書かれていない部分を補足しながら構築をしていきたいと思います。\nStep1. VPC ネットワークの作成 # まずは GKE ノードを配置する VPC ネットワークおよび東京リージョンと大阪リージョンにサブネットを作成します。今回の例では GKE ノードからプライベートネットワーク経由で Artifact Registry などの他のマネージドサービスへアクセスできるように限定公開の Google アクセスをオンにしています。\n実行例）VPCネットワークの作成\n# 環境変数の設定 export NETWORK=\u0026#34;matt-vpc\u0026#34; export SUBNET=\u0026#34;matt-private-vm\u0026#34; export LOCATION_1=\u0026#34;asia-northeast1\u0026#34; export LOCATION_2=\u0026#34;asia-northeast2\u0026#34; export IP_RANGE_1=\u0026#34;172.16.0.0/16\u0026#34; export IP_RANGE_2=\u0026#34;172.24.0.0/16\u0026#34; # VPC ネットワークの作成 gcloud compute networks create ${NETWORK} --subnet-mode=custom # サブネットの作成 (東京リージョン) gcloud compute networks subnets create ${SUBNET} \\ --network=${NETWORK} --range=${IP_RANGE_1} --region=${LOCATION_1} \\ --enable-private-ip-google-access # サブネットの作成 (大阪リージョン) gcloud compute networks subnets create ${SUBNET} \\ --network=${NETWORK} --range=${IP_RANGE_2} --region=${LOCATION_2} \\ --enable-private-ip-google-access プライベートネットワーク経由でインターネット上の Docker Hub などへ接続できるよう Cloud NAT も作成しておきます。\n実行例）Cloud NATの作成\n# 環境変数の設定 export NAT_GATEWAY_1=\u0026#34;matt-tokyo-nat\u0026#34; export NAT_GATEWAY_2=\u0026#34;matt-osaka-nat\u0026#34; export NAT_ROUTER_1=\u0026#34;matt-tokyo-router\u0026#34; export NAT_ROUTER_2=\u0026#34;matt-osaka-router\u0026#34; # Cloud Router の作成 (東京リージョン) gcloud compute routers create ${NAT_ROUTER_1} \\ --network=${NETWORK} --region=${LOCATION_1} # Cloud Router の作成 (大阪リージョン) gcloud compute routers create ${NAT_ROUTER_2} \\ --network=${NETWORK} --region=${LOCATION_2} # Cloud NAT の作成 (東京リージョン) gcloud compute routers nats create ${NAT_GATEWAY_1} \\ --router=${NAT_ROUTER_1} \\ --router-region=${LOCATION_1} \\ --auto-allocate-nat-external-ips \\ --nat-all-subnet-ip-ranges \\ --enable-logging # Cloud NAT の作成 (大阪リージョン) gcloud compute routers nats create ${NAT_GATEWAY_2} \\ --router=${NAT_ROUTER_2} \\ --router-region=${LOCATION_2} \\ --auto-allocate-nat-external-ips \\ --nat-all-subnet-ip-ranges \\ --enable-logging Step2. GKE クラスタの作成 # 次に GKE クラスタを作成していきましょう。 Anthos Service Mesh の導入には次のような前提条件を満たす必要があるため、今回はこちらを満たした上で、セキュリティの観点から限定公開クラスタおよび承認済みネットワークの有効化、安定性の観点から Stable チャンネルを指定しています。\n4 vCPU 以上を搭載したノード 合計 8 vCPU 以上を搭載したノードプール GKE Workload Identity の有効化 GKE リリースチャンネルへの登録 (※1) ※1: Anthos Service Mesh のマネージドコントロールプレーン機能を使う場合のみ\n実行例）GKEクラスタの作成\n# 環境変数の設定 export PROJECT_ID=`gcloud config list --format \u0026#34;value(core.project)\u0026#34;` export CLUSTER_1=\u0026#34;matt-tokyo-cluster-1\u0026#34; export CLUSTER_2=\u0026#34;matt-osaka-cluster-1\u0026#34; export MASTER_IP_RANGE_1=\u0026#34;192.168.0.0/28\u0026#34; export MASTER_IP_RANGE_2=\u0026#34;192.168.8.0/28\u0026#34; export CTX_1=\u0026#34;gke_${PROJECT_ID}_${LOCATION_1}_${CLUSTER_1}\u0026#34; export CTX_2=\u0026#34;gke_${PROJECT_ID}_${LOCATION_2}_${CLUSTER_2}\u0026#34; # GKE クラスタ(東京リージョン)の作成 gcloud container clusters create ${CLUSTER_1} \\ --region=${LOCATION_1} \\ --machine-type=\u0026#34;e2-standard-4\u0026#34; \\ --num-nodes=\u0026#34;1\u0026#34; \\ --enable-autoscaling --min-nodes=\u0026#34;1\u0026#34; --max-nodes=\u0026#34;3\u0026#34; \\ --enable-private-nodes --master-ipv4-cidr=${MASTER_IP_RANGE_1} \\ --enable-master-global-access \\ --enable-ip-alias --network=${NETWORK} --subnetwork=${SUBNET} \\ --enable-master-authorized-networks \\ --workload-pool=\u0026#34;${PROJECT_ID}.svc.id.goog\u0026#34; \\ --release-channel=\u0026#34;stable\u0026#34; # GKE クラスタ(大阪リージョン)の作成 gcloud container clusters create ${CLUSTER_2} \\ --region=${LOCATION_2} \\ --machine-type=\u0026#34;e2-standard-4\u0026#34; \\ --num-nodes=\u0026#34;1\u0026#34; \\ --enable-autoscaling --min-nodes=\u0026#34;1\u0026#34; --max-nodes=\u0026#34;3\u0026#34; \\ --enable-private-nodes --master-ipv4-cidr=${MASTER_IP_RANGE_2} \\ --enable-master-global-access \\ --enable-ip-alias --network ${NETWORK} --subnetwork=${SUBNET} \\ --enable-master-authorized-networks \\ --workload-pool=\u0026#34;${PROJECT_ID}.svc.id.goog\u0026#34; \\ --release-channel=\u0026#34;stable\u0026#34; 前提条件の詳細については次の公式ドキュメントをご参照ください。\nStep3. Anthos Service Mesh のインストール # (1) 管理ツールのダウンロード # 最初に Anthos Service Mesh v1.11 から正式な管理ツールとなった asmcli をダウンロードします。\n実行例）asmcliツールのダウンロード\ncurl https://storage.googleapis.com/csm-artifacts/asm/asmcli_1.11 \u0026gt; asmcli # 実行権限の付与 chmod +x asmcli (2) 東京 GKE クラスタへのインストール # まずは東京リージョンの GKE クラスタに Anthos Service Mesh をインストールしていきましょう。Kubernetes API へ接続できるように GKE コントロールプレーンの承認済みネットワークに Cloud Shell の IP アドレスを登録し、kubectl を実行できるようにクラスタ認証情報を取得します。\n実行例）クラスタ認証情報の取得(東京リージョン)\n# CloudShellの承認済みネットワーク登録 gcloud container clusters update ${CLUSTER_1} \\ --region ${LOCATION_1} \\ --enable-master-authorized-networks \\ --master-authorized-networks \\ \u0026#34;$(dig +short myip.opendns.com @resolver1.opendns.com)/32\u0026#34; # クラスタ認証情報の取得 gcloud container clusters get-credentials ${CLUSTER_1} \\ --region ${LOCATION_1} 次に asmcli を使って Anthos Service Mesh をインストールします。コマンドが完了するまでおおよそ 5 分程度かかりました。\n実行例）Anthos Service Meshのインストール(東京リージョン)\n./asmcli install \\ --project_id ${PROJECT_ID} \\ --cluster_location ${LOCATION_1} \\ --cluster_name ${CLUSTER_1} \\ --managed \\ --channel \u0026#34;stable\u0026#34; \\ --enable-all \\ --output_dir ${CLUSTER_1} 次のようなメッセージが出力されましたらインストールに成功です。\n出力例）インストール成功時\nasmcli: Successfully installed ASM. (3) 大阪 GKE クラスタへのインストール # 同様に大阪リージョンの GKE クラスタにもインストールをしましょう。\n実行例）Anthos Service Meshのインストール(大阪リージョン)\n# CloudShellの承認済みネットワーク登録 gcloud container clusters update ${CLUSTER_2} \\ --region ${LOCATION_2} \\ --enable-master-authorized-networks \\ --master-authorized-networks \\ \u0026#34;$(dig +short myip.opendns.com @resolver1.opendns.com)/32\u0026#34; # クラスタ認証情報の取得 gcloud container clusters get-credentials ${CLUSTER_2} \\ --region ${LOCATION_2} # Anthos Service Mesh のインストール ./asmcli install \\ --project_id ${PROJECT_ID} \\ --cluster_location ${LOCATION_2} \\ --cluster_name ${CLUSTER_2} \\ --managed \\ --channel \u0026#34;stable\u0026#34; \\ --enable-all \\ --output_dir ${CLUSTER_2} (4) ファイアウォールルールの更新 (限定公開クラスタ時のみ) # 限定公開クラスタに Anthos Service Mesh をインストールした場合は、コントロールプレーンからのポート 15017 による通信を追加で許可する必要があります。次のコマンド実行してコントロールプレーンからのポート 15017 による通信を許可します。\n実行例）ファイアウォールルールの更新\n# 既存のファイアウォールルールに 15017/TCP の許可ルールを追加 (東京リージョン) gcloud compute firewall-rules update \\ `gcloud compute firewall-rules list --filter=\u0026#34;name~${CLUSTER_1}-.*-master\u0026#34; --format=\u0026#34;value(name)\u0026#34;` \\ --allow tcp:10250,tcp:443,tcp:15017 # 既存のファイアウォールルールに 15017/TCP の許可ルールを追加 (大阪リージョン) gcloud compute firewall-rules update \\ `gcloud compute firewall-rules list --filter=\u0026#34;name~${CLUSTER_2}-.*-master\u0026#34; --format=\u0026#34;value(name)\u0026#34;` \\ --allow tcp:10250,tcp:443,tcp:15017 Step4. マルチクラスタメッシュの設定 # (1) クラスタ間通信の許可 # クラスタをまたがってのサービス間通信ができるように次のコマンドを実行してファイアウォールルール \u0026quot;VPCネットワーク名\u0026quot;-istio-multicluster-pods を新たに作成します。\n実行例）クラスタ間通信の許可\n# 環境変数の設定 CLUSTER_1_CIDR=$(gcloud container clusters list \\ --filter=\u0026#34;name~${CLUSTER_1}\u0026#34; --format=\u0026#39;value(clusterIpv4Cidr)\u0026#39;) CLUSTER_2_CIDR=$(gcloud container clusters list \\ --filter=\u0026#34;name~${CLUSTER_2}\u0026#34; --format=\u0026#39;value(clusterIpv4Cidr)\u0026#39;) CLUSTER_1_NETTAG=$(gcloud compute instances list \\ --filter=\u0026#34;name~${CLUSTER_1::16}\u0026#34; --format=\u0026#39;value(tags.items.[0])\u0026#39; | \\ grep ${CLUSTER_1} | sort -u) CLUSTER_2_NETTAG=$(gcloud compute instances list \\ --filter=\u0026#34;name~${CLUSTER_2::16}\u0026#34; --format=\u0026#39;value(tags.items.[0])\u0026#39; | \\ grep ${CLUSTER_2} | sort -u) # クラスタ間通信を許可するファイアウォールルールの作成 gcloud compute firewall-rules create \u0026#34;${NETWORK}-istio-multicluster-pods\u0026#34; \\ --network=${NETWORK} \\ --allow=tcp,udp,icmp,esp,ah,sctp \\ --direction=INGRESS \\ --priority=900 \\ --source-ranges=\u0026#34;${CLUSTER_1_CIDR},${CLUSTER_2_CIDR}\u0026#34; \\ --target-tags=\u0026#34;${CLUSTER_1_NETTAG},${CLUSTER_2_NETTAG}\u0026#34; (2) 承認済みネットワークの追加 (In-cluster かつ承認済みネットワーク有効時のみ) # 今回はマネージドコントロールプレーンを利用しているため設定は不要ですが、Anthos Service Mesh を In-cluster で構築した場合は Anthos Service Mesh コントロールプレーンから他 GKE クラスタのコントロールプレーンにアクセスする必要があるため承認済みネットワークを更新します。\n実行例）承認済みネットワークの追加（In-cluster時のみ）\n# 環境変数の設定 POD_IP_CIDR_1=$(gcloud container clusters describe ${CLUSTER_1} \\ --region ${LOCATION_1} --format \u0026#34;value(ipAllocationPolicy.clusterIpv4CidrBlock)\u0026#34;) POD_IP_CIDR_2=$(gcloud container clusters describe ${CLUSTER_2} \\ --region ${LOCATION_2} --format \u0026#34;value(ipAllocationPolicy.clusterIpv4CidrBlock)\u0026#34;) # 大阪リージョンの Pod アドレス範囲を、東京リージョンの承認済みネットワークに追加 gcloud container clusters update ${CLUSTER_1} \\ --region ${LOCATION_1} \\ --enable-master-authorized-networks \\ --master-authorized-networks \\ \u0026#34;${POD_IP_CIDR_2},$(dig +short myip.opendns.com @resolver1.opendns.com)/32\u0026#34; # 東京リージョンの Pod アドレス範囲を、大阪リージョンの承認済みネットワークに追加 gcloud container clusters update ${CLUSTER_2} \\ --region ${LOCATION_2} \\ --enable-master-authorized-networks \\ --master-authorized-networks \\ \u0026#34;${POD_IP_CIDR_1},$(dig +short myip.opendns.com @resolver1.opendns.com)/32\u0026#34; (3) クラスタ間サービスディスカバリの設定 # クラスタ間でサービスの自動検出ができるように asmcli を使って設定を行います。\n実行例）クラスタ間サービスディスカバリの設定\n./asmcli create-mesh ${PROJECT_ID} \\ ${PROJECT_ID}/${LOCATION_1}/${CLUSTER_1} \\ ${PROJECT_ID}/${LOCATION_2}/${CLUSTER_2} (4) シークレット情報の更新 (限定公開クラスタ時のみ) # 限定公開クラスタで構築した場合は、Anthos Service Mesh コントロールプレーンから他の GKE クラスタコントロールプレーンへプライベートネットワーク経由でアクセスできるようにシークレット情報を書き換えましょう。\n実行例）シークレット情報の更新(限定公開クラスタ時のみ)\n# 環境変数の設定 CLUSTER_1_PRIV_IP=$(gcloud container clusters describe \u0026#34;${CLUSTER_1}\u0026#34; \\ --region \u0026#34;${LOCATION_1}\u0026#34; --format \u0026#34;value(privateClusterConfig.privateEndpoint)\u0026#34;) CLUSTER_2_PRIV_IP=$(gcloud container clusters describe \u0026#34;${CLUSTER_2}\u0026#34; \\ --region \u0026#34;${LOCATION_2}\u0026#34; --format \u0026#34;value(privateClusterConfig.privateEndpoint)\u0026#34;) # プライベートエンドポイントに書き換えたシークレット情報の作成 (東京リージョン) ./${CLUSTER_1}/istioctl x create-remote-secret \\ --context=${CTX_1} --name=${CLUSTER_1} \\ --server=https://${CLUSTER_1_PRIV_IP} \u0026gt; ${CTX_1}.secret # プライベートエンドポイントに書き換えたシークレット情報の作成 (大阪リージョン) ./${CLUSTER_1}/istioctl x create-remote-secret \\ --context=${CTX_2} --name=${CLUSTER_2} \\ --server=https://${CLUSTER_2_PRIV_IP} \u0026gt; ${CTX_2}.secret # シークレット情報の更新 (東京リージョン) kubectl apply -f ${CTX_2}.secret --context=${CTX_1} # シークレット情報の更新 (大阪リージョン) kubectl apply -f ${CTX_1}.secret --context=${CTX_2} ここまで終わりましたらクラスタ間で Kubernetes サービスがロードバランシングされるようになります。\n(5) クラスタ間ロードバランシングの動作確認 # 構築はまだ続きますがいったんこの状態で、Anthos Service Mesh をインストールした際に \u0026ndash;output_dir で指定したディレクトリへ格納されているサンプルアプリケーションの中から HelloWorld と Sleep というアプリケーションを使用して、クラスタ間で負荷が分散されることを実際に確認していきたいと思います。サンプルアプリケーションの詳細につきましては次の URL をご参照ください。\n現時点では何もルーティング設定をしていないため、次の図のように 50% ずつトラフィックが振り分けられる状態を確認できるかと思います。\n(a) サンプルアプリケーションのデプロイ\nそれではサンプルアプリケーションをデプロイしていきましょう。まずは次のコマンドでサンプルアプリケーション用の Namespace を新たに作成します。\n実行例）サンプルアプリケーション用Namespaceの作成\n# 環境変数の設定 export SAMPLE_NAMESPACE=\u0026#34;sample\u0026#34; # 両クラスタにサンプルアプリケーション用 Namespace リソースの作成 for CTX in ${CTX_1} ${CTX_2} do kubectl create --context=${CTX} namespace ${SAMPLE_NAMESPACE} kubectl label --context=${CTX} namespace ${SAMPLE_NAMESPACE} \\ istio.io/rev=asm-managed-stable --overwrite done 次に HelloWorld および Sleep アプリケーションをデプロイしましょう。どちらのクラスタ上の Pod に振り分けられたかをわかりやすくするため、東京 GKE クラスタに HelloWorld アプリケーションの v1、大阪 GKE クラスタに v2 をデプロイしています。\n実行例）サンプルアプリケーションのデプロイ\n# 両クラスタに HelloWorld サービスのデプロイ for CTX in ${CTX_1} ${CTX_2} do kubectl apply --context=${CTX} -n ${SAMPLE_NAMESPACE} \\ -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\ -l service=\u0026#34;helloworld\u0026#34; done # 東京 GKE クラスタに HelloWorld アプリケーションの v1 をデプロイ kubectl apply --context=${CTX_1} -n ${SAMPLE_NAMESPACE} \\ -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\ -l version=\u0026#34;v1\u0026#34; # 大阪 GKE クラスタに HelloWorld アプリケーションの v2 をデプロイ kubectl apply --context=${CTX_2} -n ${SAMPLE_NAMESPACE} \\ -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\ -l version=\u0026#34;v2\u0026#34; # 両クラスタに Sleep サービス、アプリケーションのデプロイ for CTX in ${CTX_1} ${CTX_2} do kubectl apply --context=${CTX} -n ${SAMPLE_NAMESPACE} \\ -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/sleep/sleep.yaml done (b) サービス間通信の実行\nそれでは Sleep アプリケーションから HelloWorld アプリケーションへのサービス間通信をしてみましょう。次のコマンドでは各クラスタ上の Sleep アプリケーションからそれぞれ 10 回ずつ HelloWorld サービスへの通信を実施しています。\n実行例）サービス間通信の実行\nfor CTX in ${CTX_1} ${CTX_2} do echo \u0026#34;--- ${CTX} ---\u0026#34; for x in `seq 1 10` do kubectl exec --context=\u0026#34;${CTX}\u0026#34; -n sample -c sleep \\ \u0026#34;$(kubectl get pod --context=\u0026#34;${CTX}\u0026#34; -n sample -l \\ app=sleep -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;)\u0026#34; \\ -- curl -sS helloworld.${SAMPLE_NAMESPACE}:5000/hello \\ -w \u0026#34;time_total:%{time_total}\\n\u0026#34; done done 次の出力例のように両クラスタから v1 と v2 の Pod へランダムで約 50% ずつトラフィックが振り分けられる状態を確認できるかと思います。また、応答時間については若干ですが東京からのアクセスは東京に配置される v1 の方が良く、大阪については v2 の方が良いことも確認できるかと思います。\n出力例）\n--- gke_${PROJECT_ID}_asia-northeast1_matt-tokyo-cluster-1 --- Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.135174 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.132497 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.175281 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.127934 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.157005 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.141171 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.132390 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.141507 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.129160 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.141614 --- gke_${PROJECT_ID}_asia-northeast2_matt-osaka-cluster-1 --- Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.171108 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.157021 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.137668 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.131152 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.135779 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.141066 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.135495 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.131946 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.135268 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.131380 以上でクラスタ間ロードバランシングの動作確認は完了です。\nStep5. ローカリティロードバランシングの設定 # 次にもう少し高度なクラスタ間ロードバランシングの設定をしていきましょう。ユースケースはいくつかありますが、今回は次の図のようにサービス間通信は基本的にリージョン内のアプリケーションへルーティングされるようにローカリティを意識した振り分け制御を行っていきたいと思います。\n(1) Istio リソースのデプロイ # それでは設定していきましょう。まずは Istio DestinationRule リソース1の定義ファイルを作成しましょう。例のように DestinationRule リソースにて Outlier Detection(外れ値検知)の設定をすることでローカリティロードバランシングを有効化できます。\n作成例）helloworld-destinationrule.yaml\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: helloworld spec: host: helloworld trafficPolicy: outlierDetection: consecutive5xxErrors: 5 interval: 10s baseEjectionTime: 30s 次のコマンドで両クラスタに Istio リソースをデプロイしましょう。これで設定は終わりです。\n実行例）Istioリソースのデプロイ\n# 両クラスタに DestinationRule リソースをデプロイ for CTX in ${CTX_1} ${CTX_2} do kubectl apply --context=${CTX} -n ${SAMPLE_NAMESPACE} \\ -f helloworld-destinationrule.yaml done for CTX in ${CTX_1} ${CTX_2} do kubectl delete --context=${CTX} -n ${SAMPLE_NAMESPACE} \\ -f helloworld-destinationrule.yaml done (2) ローカリティロードバランシングの動作確認 # 構築はまだ続きますがいったんこの状態で、クラスタ間で負荷が設定どおりの比重で分散されることを実際に確認していきたいと思います。クラスタ間ロードバランシングの動作確認と同様に Sleep アプリケーションから HelloWorld アプリケーションへのサービス間通信をしてみましょう。\n実行例）サービス間通信の実行\nfor CTX in ${CTX_1} ${CTX_2} do echo \u0026#34;--- ${CTX} ---\u0026#34; for x in `seq 1 5` do kubectl exec --context=\u0026#34;${CTX}\u0026#34; -n sample -c sleep \\ \u0026#34;$(kubectl get pod --context=\u0026#34;${CTX}\u0026#34; -n sample -l \\ app=sleep -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;)\u0026#34; \\ -- curl -sS helloworld.${SAMPLE_NAMESPACE}:5000/hello \\ -w \u0026#34;time_total:%{time_total}\\n\u0026#34; done done 出力例のように東京リージョンの Sleep アプリケーションからのアクセスは v1 のみ、大阪リージョンからのアクセスは v2 のみに振り分けられることが確認できるかと思います。応答時間についてはあまり大きな差は見られないものの、ローカリティを意識しない場合と比較すると若干バラツキが少なくなったように感じます。\n出力例）\n--- gke_${PROJECT_ID}_asia-northeast1_matt-tokyo-cluster-1 --- Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.132195 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.134340 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.127857 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.142889 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.128125 --- gke_${PROJECT_ID}_asia-northeast2_matt-osaka-cluster-1 --- Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.136323 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.131857 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.130870 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.130531 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.131932 以上でローカリティロードバランシング設定の動作確認もできました。\nStep6. Ingress ゲートウェイの設定 # さてここからは話題がガラッと変わり、メッシュの外からの通信を受け入れるための Ingress ゲートウェイの設定をしていきたいと思います。今回は次の図のように各クラスタに配置された Ingress ゲートウェイアプリケーションを束ねるようにマルチクラスタ Ingress およびマルチクラスタ Service を配置する構成を作っていきます。\n(1) マルチクラスタ Ingress 機能の有効化 # 最初に次のコマンドを実行し、マルチクラスタ Ingress 機能を有効化しましょう。なお、今回はマルチクラスタ Ingress の設定を行うメインの GKE クラスタ(=構成クラスタ)として、GKE クラスタ #1 を登録しています。\n実行例）マルチクラスタIngress機能の有効化\ngcloud container hub ingress enable \\ --config-membership=${CLUSTER_1} (2) Ingress ゲートウェイ定義ファイルの作成 # 今回は Anthos Service Mesh をインストールした際に --output_dir で指定したディレクトリへ Ingress ゲートウェイのサンプル定義ファイルが配置されているのでこちらをベースに作成していきたいと思います。まずはサンプル定義ファイルを複製し、マルチクラスタ Ingress 構成向けに MultiClusterService2、BackendConfig3、MultiClusterIngress4 の 3 種類のリソース定義ファイルを追加していきましょう。\n実行例）Ingressゲートウェイ定義ファイルの作成準備\n# サンプル定義ファイルを複製 cp -r ${CLUSTER_1}/samples/gateways/istio-ingressgateway . # マルチクラスタ Ingress 定義ファイルを格納するディレクトリを作成 mkdir -p istio-ingressgateway/multicluster # Service リソース定義から MultiClusterService リソース定義ファイルに書き換え mv istio-ingressgateway/service.yaml istio-ingressgateway/multicluster/multiclusterservice.yaml # 新たな定義ファイルを 2 種類作成 touch istio-ingressgateway/multicluster/backendconfig.yaml touch istio-ingressgateway/multicluster/multiclusteringress.yaml それでは MultiClusterService2、BackendConfig3、MultiClusterIngress4 の 3 種類のリソース定義ファイルを編集していきましょう。MultiClusterService は Service リソースをマルチクラスタに対応させたリソースという位置づけのため、基本的に Service リソースの設定値とほぼ変わりません。今回は Ingress をフロントに配置するので LoadBalancer タイプの定義を削除し、デフォルトの Cluster IP に変更しています。\n作成例）istio-ingressgateway/multicluster/multiclusterservice.yaml（差分）\n- apiVersion: v1 + apiVersion: networking.gke.io/v1 - kind: Service + kind: MultiClusterService metadata: name: istio-ingressgateway + annotations: + cloud.google.com/backend-config: \u0026#39;{\u0026#34;default\u0026#34;: \u0026#34;ingress-backendconfig\u0026#34;}\u0026#39; labels: app: istio-ingressgateway istio: ingressgateway spec: - ports: - # status-port exposes a /healthz/ready endpoint that can be used with GKE Ingress health checks - - name: status-port - port: 15021 - protocol: TCP - targetPort: 15021 - # Any ports exposed in Gateway resources should be exposed here. - - name: http2 - port: 80 - - name: https - port: 443 - selector: - istio: ingressgateway - app: istio-ingressgateway - type: LoadBalancer + template: + spec: + ports: + # status-port exposes a /healthz/ready endpoint that can be used with GKE Ingress health checks + - name: status-port + port: 15021 + protocol: TCP + targetPort: 15021 + # Any ports exposed in Gateway resources should be exposed here. + - name: http2 + port: 80 + - name: https + port: 443 + selector: + istio: ingressgateway + app: istio-ingressgateway BackendConfig リソースではバックエンドサービスである Ingress ゲートウェイアプリケーションのヘルスチェックに関する定義を記載します。Ingress ゲートウェイはヘルスチェック用パスとして /healthz/ready:15021 を用意しているため、こちらを設定しましょう。\n作成例）istio-ingressgateway/multicluster/backendconfig.yaml（差分）\n+ apiVersion: cloud.google.com/v1 + kind: BackendConfig + metadata: + name: ingress-backendconfig + spec: + healthCheck: + requestPath: /healthz/ready + port: 15021 + type: HTTP MultiClusterIngress は Ingress リソースをマルチクラスタに対応させたリソースという位置づけであり、基本的に Ingress リソースを定義するときと設定値はほぼ同じです。\n作成例）istio-ingressgateway/multicluster/multiclusteringress.yaml（差分）\n+ apiVersion: networking.gke.io/v1beta1 + kind: MultiClusterIngress + metadata: + name: istio-ingressgateway + labels: + app: istio-ingressgateway + istio: ingressgateway + spec: + template: + spec: + backend: + serviceName: istio-ingressgateway + servicePort: 80 (3) Ingress ゲートウェイのデプロイ # まずは Ingress ゲートウェイリソースをデプロイする Namespace を新たに作成します。今回の例では istio-gateway という名前の Namespace を作成しています。\n実行例）Ingress Gateway用のNamespace作成\n# 環境変数の設定 export GATEWAY_NAMESPACE=\u0026#34;istio-gateway\u0026#34; # 両クラスタにサンプルアプリケーション用 Namespace リソースの作成 for CTX in ${CTX_1} ${CTX_2} do kubectl create --context=${CTX} namespace ${GATEWAY_NAMESPACE} kubectl label --context=${CTX} namespace ${GATEWAY_NAMESPACE} \\ istio.io/rev=asm-managed-stable --overwrite done 次のコマンドを実行して Ingress ゲートウェイアプリケーションを両クラスタにデプロイしましょう。\n実行例）Ingress Gatewayアプリケーションのデプロイ\nfor CTX in ${CTX_1} ${CTX_2} do kubectl apply -n ${GATEWAY_NAMESPACE} --context=${CTX} \\ -f istio-ingressgateway done 最後にマルチクラスタ Ingress リソースを構成クラスタである GKE クラスタ #1 に対してデプロイをしましょう。\n実行例）Ingress Gatewayアプリケーションのデプロイ\nkubectl apply -n ${GATEWAY_NAMESPACE} --context=${CTX_1} \\ -f istio-ingressgateway/multicluster 以上で Ingress ゲートウェイのデプロイは終わりです。\n(4) Istio リソースのデプロイ # Ingress ゲートウェイを通じてメッシュの外から HelloWorld アプリケーションへ通信ができるように Istio リソースの定義を行っていきたいと思います。まずは Istio Gateway リソース5および Istio VirtualService リソース1の定義ファイルを作成しましょう。例のように Gateway リソースにメッシュ外から受け付けるポートとプロトコルを定義し、VirtualService リソースには Gateway に入ってきた通信のパターンマッチ条件と振り分け先バックエンドの指定をします。\n作成例）helloworld-gateway.yaml\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: helloworld-gateway spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;*\u0026#34; --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: helloworld-gateway spec: hosts: - \u0026#34;*\u0026#34; gateways: - helloworld-gateway http: - match: - uri: exact: /hello route: - destination: host: helloworld port: number: 5000 次のコマンドで両クラスタに Istio リソースをデプロイし、アプリケーションへのインバウンド通信ができるように設定しましょう。\n実行例）Istioリソースのデプロイ\nfor CTX in ${CTX_1} ${CTX_2} do kubectl apply -n ${SAMPLE_NAMESPACE} --context=${CTX} \\ -f helloworld-gateway.yaml done 以上でメッシュ外からのアプリケーションへのインバウンド通信もできるようになりました。\n(5) インバウンド通信の動作確認 # それではメッシュ外からのアプリケーションへのインバウンド通信ができることを確認していきましょう。ローカリティロードバランシングの設定は「Step5. ローカリティロードバランシングの設定」にて実施済みですので次のような挙動となることが想定されます。\n(a) パブリックエンドポイントの取得\nまずは Ingress ゲートウェイの外部 IP アドレスを取得しましょう。\n実行例）Ingressゲートウェイの外部IPアドレスの取得\nkubectl --context=${CTX_1} \\ -n ${GATEWAY_NAMESPACE} get MultiClusterIngress \\ -o custom-columns=VIP:status.VIP --no-headers (b) インバウンド通信の実行\nでは東京リージョン上に仮想マシンなどを立てて Ingress ゲートウェイの外部 IP アドレスに対して curl コマンドを実行し、アクセスをしてみましょう。実行例のように v1 に振り分けが 100% されることを確認できるかと思います。\n出力例）ローカリティロードバランシング有効時\n$ for x in `seq 1 10`; do curl http://\u0026lt;Ingress Gateway\u0026#39;s External IP\u0026gt;/hello -w \u0026#34;time_total:%{time_total}\\n\u0026#34;; done Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.139677 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.139758 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.138518 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.142124 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.137289 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.151358 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.133507 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.139144 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.133560 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.135621 次に大阪リージョン上に仮想マシンなどを立てて Ingress ゲートウェイの外部 IP アドレスに対して curl コマンドを実行し、アクセスをしてみましょう。先ほどとは異なり、実行例のように v2 に振り分けが 100% されることを確認できるかと思います。\n出力例）ローカリティロードバランシング有効時\n$ for x in `seq 1 10`; do curl http://\u0026lt;Ingress Gateway\u0026#39;s External IP\u0026gt;/hello -w \u0026#34;time_total:%{time_total}\\n\u0026#34;; done Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.139 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.133 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.134 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.130 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.129 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.141 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.133 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.176 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.137 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.138 ご参考、ローカリティロードバランシング無効時の動作\nローカリティロードバランシングを無効(=DestinationRule リソースを削除)にした状態で、各リージョンからアクセスした結果も取得してみました。メッシュ内の通信では差が見えにくかったのですが、インバウンド通信にするとローカリティによるレスポンスの改善がはっきりとわかる結果になりました。\n出力例）ローカリティロードバランシング無効時\n$ for x in `seq 1 10`; do curl http://\u0026lt;Ingress Gateway\u0026#39;s External IP\u0026gt;/hello -w \u0026#34;time_total:%{time_total}\\n\u0026#34;; done Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.221164 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.142753 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.224596 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.160720 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.188251 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.145336 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.173262 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.138860 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.139937 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.170250 出力例）ローカリティロードバランシング無効時\n$ for x in `seq 1 10`; do curl http://\u0026lt;Ingress Gateway\u0026#39;s External IP\u0026gt;/hello -w \u0026#34;time_total:%{time_total}\\n\u0026#34;; done Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 response_time:0.147 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t response_time:0.225 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t response_time:0.211 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 response_time:0.141 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 response_time:0.134 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t response_time:0.182 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 response_time:0.133 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 response_time:0.134 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 response_time:0.143 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t response_time:0.187 以上でメッシュの外からアプリケーションへのインバウンド通信に対する動作確認も終了です。お疲れ様でした。\n終わりに # 今回は複数リージョンに展開した複数 GKE クラスタを単一の Anthos Service Mesh 環境に追加し、GKE クラスタ間で負荷分散を行う方法についてご紹介でしたがいかがだったでしょうか。\n複数リージョンの GKE クラスタでマルチクラスタメッシュを構築することにより、予期しない大規模災害の発生にも耐えうる高い可用性と回復力の実現、エンドユーザからより近い位置への振り分けによるレイテンシの改善といったことが期待できるようになる見込みです。もしこれから Anthos Service Mesh 環境の利用を検討している方はマルチクラスタメッシュ構成についても検討してみてはいかがでしょうか。\nGoogle Cloud は、Google LLC の商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 DestinationRule は転送先サービスのサブセット化や各種トラフィックポリシーを定義する Istio リソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMultiClusterService は Service リソースを複数のクラスタ上に展開する GKE 独自のカスタムリソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBackendConfig はバックエンドサービスのヘルスチェックを定義する GKE 独自のカスタムリソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMultiClusterIngress はマルチクラスタに対応した Ingress リソースを定義する GKE 独自のカスタムリソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGateway は Ingress/Egress ゲートウェイで受け付けるポート、プロトコルを定義する Istio リソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"December 14, 2021","permalink":"/posts/2021/12/gcp-multi-region-asm-cluster/","section":"記事一覧","summary":"みなさん、こんにちは。今回は複数のリージョンに展開する各 GKE クラスタを単一の Anthos Service Mesh 環境に追加し、GKE クラスタ間で負荷分散を行う方法についてご紹介していきたいと思います。","title":"複数リージョンの GKE クラスタと Anthos Service Mesh でマルチクラスタメッシュ環境を構築してみた"},{"content":"","date":"December 13, 2021","permalink":"/tags/gke-autopilot/","section":"タグ一覧","summary":"","title":"GKE Autopilot"},{"content":"みなさん、こんにちは。今回は Google Cloud が提供するマネージドサービスメッシュサービスの Anthos Service Mesh に関するお話です。\nAnthos Service Mesh はここ半年で「マネージドコントロールプレーン機能の一般公開」、「マネージドデータプレーン機能のプレビュー公開」と Google マネージドの範囲を徐々に広げてきましたが、2021 年 11 月 19 日の更新でプレビュー段階ではありますが「Google Kubernetes Engine(GKE) Autopilot 上でも Anthos Service Mesh を利用できる」ようになりました。\n今回はそんなプレビュー公開されたばかりの GKE Autopilot と Anthos Service Mesh(ASM) を使った Kubernetes 部分も含めてフルマネージドなサービスメッシュ環境を構築していきたいと思います。\n構築するシステムについて # 次の図に示すように限定公開クラスタおよび承認済みネットワーク機能を有効化した GKE Autopilot クラスタに対して Anthos Service Mesh を導入し、サービスメッシュ上でサンプルアプリケーションを動かしていきたいと思います。\nそれでは構築していきましょう # いつも通り公式ドキュメントを参考にしつつ、公式ドキュメントに書かれていない部分を補足しながら構築をしていきたいと思います。\nStep1. VPC ネットワークの作成 # まずは GKE ノードを配置する VPC ネットワークおよび東京リージョンにサブネットを作成します。今回の例では GKE ノードからプライベートネットワーク経由で Artifact Registry などの他のマネージドサービスへアクセスできるように限定公開の Google アクセスをオンにしています。\nプライベートネットワークからインターネット上の Docker Hub などへ接続できるよう Cloud NAT リソースも作成しておきたいと思います。\nStep2. GKE Autopilot クラスタの作成 # 次に GKE Autopilot クラスタを作成していきましょう。Google Cloud コンソールから操作する場合は GKE クラスタ画面の「+作成」ボタンをクリックし、GKE Autopilot クラスタの作成画面へと遷移します。\nAutopilot クラスタの作成画面では、クラスタを展開するネットワークの選択とリリースチャンネルの選択をしていきましょう。今回はセキュリティの観点から限定公開クラスタに設定し、GKE Autopilot リリースチャンネルには Anthos Service Mesh が現時点で唯一サポート対象としている Rapid チャンネルを指定しています。作成ボタンを押してからクラスタの作成が完了するまで 5 分程度かかりました。\nなお、Anthos Service Mesh がサポートする GKE Autopilot リリースチャンネルについては近い将来変更が生じる可能性が高いため、構築を行う際には最新のサポート状況を公式ドキュメントから確認するようにしましょう。\nStep3. Anthos Service Mesh のインストール # ここからは Cloud Shell から操作を実施していきたいと思います。まずは Kubernetes API へ接続できるように GKE コントロールプレーンの承認済みネットワークに Cloud Shell の IP アドレスを登録し、kubectl を実行できるようにクラスタ認証情報を取得します。\n実行例）クラスタ認証情報の取得\nexport PROJECT_ID=`gcloud config list --format \u0026#34;value(core.project)\u0026#34;` export CLUSTER_NAME=\u0026#34;matt-tokyo-autopilot-cluster-001\u0026#34; export CLUSTER_LOCATION=\u0026#34;asia-northeast1\u0026#34; # CloudShellの承認済みネットワーク登録 gcloud container clusters update ${CLUSTER_NAME} \\ --region ${CLUSTER_LOCATION} \\ --enable-master-authorized-networks \\ --master-authorized-networks \\ \u0026#34;`dig +short myip.opendns.com @resolver1.opendns.com`/32\u0026#34; # クラスタ認証情報の取得 gcloud container clusters get-credentials ${CLUSTER_NAME} \\ --region ${CLUSTER_LOCATION} 次に Anthos Service Mesh v1.11 から正式な管理ツールとなった asmcli をダウンロードします。\n実行例）asmcliツールのダウンロード\ncurl https://storage.googleapis.com/csm-artifacts/asm/asmcli_1.11 \u0026gt; asmcli # 実行権限の付与 chmod +x asmcli asmcli を使って GKE Autopilot クラスタに Rapid チャンネル(※1)の Anthos Service Mesh をインストールします。コマンドが完了するまでおおよそ 5 分程度かかりました。\n※1: 現時点では GKE Autopilot は Anthos Service Mesh の Rapid チャンネルでのみサポートされているため\n実行例）Anthos Service Meshのインストール\n./asmcli x install \\ --project_id ${PROJECT_ID} \\ --cluster_location ${CLUSTER_LOCATION} \\ --cluster_name ${CLUSTER_NAME} \\ --managed \\ --use_managed_cni \\ --channel \u0026#34;rapid\u0026#34; \\ --enable-all \\ --output_dir ${CLUSTER_NAME} インストールに成功した場合は次のようなメッセージが出力されます。\n出力例）\nasmcli: Successfully installed ASM. Step4. ファイアウォールルールの更新 (限定公開クラスタ時のみ) # 限定公開クラスタに Anthos Service Mesh をインストールした場合は、コントロールプレーンからのポート 15017 による通信を追加で許可する必要があります。まず次のコマンドを実行し、既存のファイアウォールルール名を取得します。\n実行例）ファイアウォールルールの確認\ngcloud compute firewall-rules list --filter=\u0026#34;name~${CLUSTER_NAME}-.*-master\u0026#34; 次のコマンドのファイアウォールルール名 ${FIREWALL_RULE_NAME} を前のコマンドで取得した名前に置き替え、コマンド実行してコントロールプレーンからのポート 15017 による通信を許可します。\n実行例）ファイアウォールルールの更新\ngcloud compute firewall-rules update ${FIREWALL_RULE_NAME} \\ --allow tcp:10250,tcp:443,tcp:15017 Step5. マネージドデータプレーンの有効化 (Namespace 毎に設定) # マネージドデータプレーンは Kubernetes Namespace リソースごとにアノテーションを設定して有効化を行います。次のコマンドの ${NAMESPACE_NAME} を設定対象の Namespace 名に置き替え、コマンド実行してマネージドデータプレーンを有効化します。\n実行例）マネージドデータプレーンの有効化\nkubectl annotate --overwrite namespace ${NAMESPACE_NAME} \\ mesh.cloud.google.com/proxy=\u0026#39;{\u0026#34;managed\u0026#34;:\u0026#34;true\u0026#34;}\u0026#39; Step6. Ingress Gateway のデプロイ # asmcli でインストールした場合は自動で Ingress Gateway はデプロイされないため、メッシュ外からのアクセスをさせるためには Ingress Gateway をデプロイする必要があります。まず次のコマンドで istio-gateway Namespace を新たに作成します。\nexport GATEWAY_NAMESPACE=\u0026ldquo;istio-gateway\u0026rdquo;\nkubectl apply -f - \u0026laquo;EOF kind: Namespace metadata: name: ${GATEWAY_NAMESPACE} annotations: # マネージドデータプレーン有効化の設定 mesh.cloud.google.com/proxy: \u0026lsquo;{\u0026ldquo;managed\u0026rdquo;:\u0026ldquo;true\u0026rdquo;}\u0026rsquo; labels: # 自動サイドカーインジェクション(Rapid)有効化の設定 istio.io/rev: asm-managed-rapid EOF\n次のコマンドを実行して Ingress Gateway をデプロイします。なお、今回は Anthos Service Mesh をインストールした際に `--output_dir` で指定したディレクトリに Ingress Gateway の定義ファイルが配置されているのでこちらをそのまま利用しています。 **実行例）Ingress Gatewayのデプロイ** ```bash kubectl apply -n ${GATEWAY_NAMESPACE} \\ -f ${CLUSTER_NAME}/samples/gateways/istio-ingressgateway なお、今回利用した Ingress Gateway の詳細については次の URL をご参照ください。\n以上でアプリケーションをデプロイする準備が整いました。\nStep7. サンプルアプリケーションのデプロイ # 最後にサンプルアプリケーションをデプロイし、環境に問題がないかを確認していきましょう。まず次のコマンドでサンプルアプリケーション用の Namespace を新たに作成します。\n実行例）Namespaceの作成\nexport SAMPLE_NAMESPACE=\u0026#34;sample\u0026#34; # サンプルアプリケーション用 Namespace リソースの作成 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Namespace metadata: annotations: # マネージドデータプレーン有効化の設定 mesh.cloud.google.com/proxy: \u0026#39;{\u0026#34;managed\u0026#34;:\u0026#34;true\u0026#34;}\u0026#39; labels: # 自動サイドカーインジェクション(Rapid)有効化の設定 istio.io/rev: asm-managed-rapid EOF サンプルアプリケーションをデプロイします。今回は Anthos Service Mesh をインストールした際に --output_dir で指定したディレクトリへ格納されているサンプルアプリケーションの中から HelloWorld というサンプルアプリケーションを使用しています。\n実行例）HelloWorldアプリケーションのデプロイ\n# Kubernetes Service リソースのデプロイ kubectl apply -n ${SAMPLE_NAMESPACE} \\ -f ${CLUSTER_NAME}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\ -l service=helloworld # Kubernetes Deployment リソースのデプロイ kubectl apply -n ${SAMPLE_NAMESPACE} \\ -f ${CLUSTER_NAME}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\ -l version=v1 # Istio Gateway/VirtualService リソースのデプロイ kubectl apply -n ${SAMPLE_NAMESPACE} \\ -f ${CLUSTER_NAME}/istio-1.11.2-asm.17/samples/helloworld/helloworld-gateway.yaml アプリケーションのデプロイが終わったので Ingress Gateway 経由でアプリケーションにアクセスできるかを確認していきましょう。まず次のコマンドを実行して Ingress Gateway の External IP アドレスを取得します。\n実行例）IngressGatewayの設定\nkubectl -n ${GATEWAY_NAMESPACE} get service istio-ingressgateway External IP アドレスが取得できたら curl コマンドなどで http://\u0026lt;EXTERNAL_IP\u0026gt;/hello にアクセスしてみましょう。次のようなメッセージが表示されていれば成功です。\n出力例）\nHello version: v1, instance: helloworld-v1-xxxxxxxxxx-xxxxx 以上で構築は終わりです。お疲れ様でした。\n終わりに # さて今回はプレビュー公開されたばかりの Google Kubernetes Engine(GKE) Autopilot と Anthos Service Mesh(ASM) を使ったフルマネージドなサービスメッシュ環境を構築する方法のご紹介でした。いかがだったでしょうか。\nこれで晴れて Kubernetes 部分も含めフルマネージドなサービスメッシュ環境が実現できるようになり、こちらの活用により以前よりも運用負荷を大きく軽減できる未来が近づいてきましたね。\nもちろん安定性、保守性といった観点からプロダクション用途へのプレビュー段階の機能の適用は、現時点ではオススメできませんが、もしこれから検証目的で試してみたいという方は参考にしてみてはいかがでしょうか。\nGoogle Cloud は、Google LLC の商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"December 13, 2021","permalink":"/posts/2021/12/gcp-asm-with-gke-autopilot/","section":"記事一覧","summary":"みなさん、こんにちは。今回は Google Cloud が提供するマネージドサービスメッシュサービスの Anthos Service Mesh に関するお話です。","title":"GKE Autopilot と Anthos Service Mesh を使ってフルマネージドなサービスメッシュ環境を構築してみた"},{"content":"みなさん、こんにちは。今回は GitHub Enterprise Cloud(GHEC) の監査ログ(Audit Log) の取得方法についてのお話です。\nGHEC 監査ログの取得方法としてはいくつか方法はあるのですが、この記事では GHEC の監査ログを取得するためのコマンドラインインタフェースである GHEC Audit Log CLI を使った方法をご紹介していきたいと思います。\n今回は Linux(AWS CloudShell) 上に環境を作って試しに実行してみるところからはじめて、定期的に監査ログ取得を行う自動化フローの構築まで紹介していきたいと思います。\nまずは手動で GHEC Audit Log CLI を実行してみよう # Step1. 前提パッケージのインストール # GHEC Audit Log CLI の前提パッケージとして Node.js が必要となります。GitHub からソースコードを入手する必要があるため、git コマンドと併せてインストールしましょう。\n実行例）前提パッケージの導入\n$ curl --silent --location https://rpm.nodesource.com/setup_16.x | sudo bash - $ sudo yum install -y nodejs git Step2. GHEC Audit Log CLI のインストール # GitHub からソースコードを取得し、npm コマンドを使って GHEC Audit Log CLI をインストールします。最後の ghce-audit-log-cli -v コマンドにてバージョン情報が出力されれば CLI のインストールは完了です。\n実行例）ソースコード取得とインストール\n$ git clone https://github.com/github/ghec-audit-log-cli.git $ cd ghec-audit-log-cli $ sudo npm link $ ghec-audit-log-cli -v 2.1.2 $ ghec-audit-log-cli --help Usage: ghec-audit-log-cli [options] Options: -v, --version Output the current version -t, --token \u0026lt;string\u0026gt; the token to access the API (mandatory) -o, --org \u0026lt;string\u0026gt; the organization we want to extract the audit log from -cfg, --config \u0026lt;string\u0026gt; location for the config yaml file. Default \u0026#34;.ghec-audit-log\u0026#34; (default: \u0026#34;./.ghec-audit-log\u0026#34;) -p, --pretty prints the json data in a readable format (default: false) -l, --limit \u0026lt;number\u0026gt; a maximum limit on the number of items retrieved -f, --file \u0026lt;string\u0026gt; the output file where the result should be printed -a, --api \u0026lt;string\u0026gt; the version of GitHub API to call (default: \u0026#34;v4\u0026#34;) -at, --api-type \u0026lt;string\u0026gt; Only if -a is v3. API type to bring, either all, web or git (default: \u0026#34;all\u0026#34;) -c, --cursor \u0026lt;string\u0026gt; if provided, this cursor will be used to query the newest entries from the cursor provided. If not present, the result will contain all the audit log from the org -s, --source \u0026lt;string\u0026gt; the source of the audit log. The source can be either a GitHub Enterprise or a GitHub Enterprise Organization. Accepts the following values: org | enterprise. Defaults to org (default: \u0026#34;org\u0026#34;) -h, --help display help for command Step3. GitHub アクセストークンの取得 # 次に GitHub のサイトへ移り、GitHub API へアクセスする際に利用するアクセストークンを作成します。Organization の Owner 権限を持つユーザで、Settings \u0026gt; Developer settings \u0026gt; Personal access tokens \u0026gt; Generate new tokens から作成しましょう。付与するスコープについては筆者の環境では次の 4 つの権限を付与することで動作を確認することができました。\npublic_repo admin:org read:user admin:enterprise Step4. GHEC Audit Log CLI の動作確認 # ここまで来たら実際にコマンドを実行して監査ログが取得できるか試してみましょう。次のコマンドを実行して監査ログが出力されれば成功です。\n実行例）監査ログの取得\n$ export ORG_NAME=\u0026#34;Your GitHub organization account name\u0026#34; $ export AUDIT_LOG_TOKEN=\u0026#34;Your GitHub access token\u0026#34; $ ghec-audit-log-cli --org ${ORG_NAME} --token ${AUDIT_LOG_TOKEN} --api \u0026#34;v3\u0026#34; --pretty なお、GitHub アクセストークンに付与した権限が足りない場合は次のようなメッセージを出力してコマンドがエラー終了します。エラーメッセージに不足している権限についての情報がありますので Step3 へ戻り、アクセストークンへ不足する権限を追加して再度 CLI を実行してください。\n出力例）コマンド失敗時\nGraphqlError: Your token has not been granted the required scopes to execute this query. The \u0026#39;organizationBillingEmail\u0026#39; field requires one of the following scopes: [\u0026#39;admin:org\u0026#39;], but your token has only been granted the: [\u0026#39;admin:enterprise\u0026#39;, \u0026#39;read:org\u0026#39;] scopes. Please modify your token\u0026#39;s scopes at: https://github.com/settings/tokens. 以上で無事にローカル環境で GHEC Audit Log CLI を実行することができました。\n監査ログを取得するフローを自動化しよう # 次は定期的に監査ログ取得を行う自動化フローの構築を行っていきたいと思います。今回はサンプルとして用意されているワークフロー定義を少し改造し、「1 時間ごと GHEC Audit Log CLI を使って監査ログを取得し、監査ログ格納用のリポジトリに追加する」といったフローを GitHub Actions を使って自動化していきたいと思います。\nStep1. 監査ログ格納用リポジトリの作成 # まずは対象 Organization に監査ログを取得、格納するためのリポジトリを作成しましょう。今回は ghec-audit-log-cli という名前のリポジトリを作成しています。\nStep2. シークレットの登録 # 次にアクセストークンなどの機密性の高い情報をワークフローへ渡すために Organization 設定にてシークレットの登録を行います。シークレットは次の 3 種類を登録しましょう。\nシークレット名 説明 ORG_NAME 監査ログを取得する対象の Organization アカウント名を指定 AUDIT_LOG_TOKEN GitHub Audit Log API へアクセスする際に利用するアクセストークンを指定 COMMITTER_EMAIL 監査ログなどをリポジトリに Push するアカウントのメールアドレスを指定 Step3. GHEC Audit Log CLI のソースコード登録 # GHEC Audit Log CLI のソースコードを取得し、新しく作成したリポジトリへソースコードを登録をしましょう。\n実行例）ソースコードの登録\n$ export ORG_NAME=\u0026#34;Your GitHub organization account name\u0026#34; $ git clone https://github.com/github/ghec-audit-log-cli.git $ cd ghec-audit-log-cli $ git remote add logging https://github.com/${ORG_NAME}/ghec-audit-log-cli.git $ git push -u logging main Step4. ワークフロー定義の作成 # 次にサンプルのワークフロー定義が workflows ディレクトリに用意されていますので、こちらをベースにワークフロー定義を作成していきたいと思います。なお、v3 と v4 の 2 種類のサンプルが用意されていますが記事の執筆時点では ​v4 に不具合があったため v3 を利用しています。\nまずはサンプル定義ファイルを GitHub Actions 所定のディレクトリ .github/workflows へ複製します。\n実行例）サンプルワークフローを複製\n$ cp workflows/forward-v3-workflow.yml .github/workflows/ghec-audit-log.yml 次に複製した .github/workflows/ghec-audit-log.yml ファイルを編集します。サンプルでは取得した監査ログを指定した URL へ POST するように定義されていますが、今回はこちらのリポジトリへコミットするように書き換えています。\n変更例）.github/workflows/ghec-audit-log.yml（差分）\n############################################ # Github Action Workflow to poll and aggregate logs # ############################################ name: POLL/POST Audit Log Data from v3 API ############################################## # Run once an hour and when pushed to main # ############################################## on: push: branches: main schedule: - cron: \u0026#39;59 * * * *\u0026#39; ################# # Build the job # ################# jobs: poll: runs-on: ubuntu-latest strategy: matrix: node-version: [12.x] steps: # Clone source code - name: Checkout source code uses: actions/checkout@v2 # Install congiure NodeJS - name: Use Node.js ${{ matrix.node-version }} uses: actions/setup-node@v1 with: node-version: ${{ matrix.node-version }} # Need to install NPM - name: NPM Install run: npm install # If this is the first time we poll, then do a fresh poll. If not, poll from latest cursor. - name: Poll from Cursor run: | + FILE_SUFFIIX=$(date +%Y%m%d-%H%M) + mkdir -p audit-logs if [ -f \u0026#34;.last-v3-cursor-update\u0026#34; ]; then LAST_CURSOR=$(cat .last-v3-cursor-update) fi if [ -z \u0026#34;$LAST_CURSOR\u0026#34; ]; then echo \u0026#34;FIRST TIME RUNNING AUDIT LOG POLL\u0026#34; - npm start -- --token ${{secrets.AUDIT_LOG_TOKEN}} --org ${{secrets.ORG_NAME}} --api \u0026#39;v3\u0026#39; --api-type \u0026#39;all\u0026#39; --file \u0026#34;audit-log-output.json\u0026#34; + npm start -- --token ${{secrets.AUDIT_LOG_TOKEN}} \\ + --org ${{secrets.ORG_NAME}} --api \u0026#39;v3\u0026#39; --api-type \u0026#39;all\u0026#39; \\ + --file \u0026#34;audit-logs/audit-log-output-${FILE_SUFFIIX}.json\u0026#34; --pretty else echo \u0026#34;RUNNING AUDIT LOG POLL FROM $LAST_CURSOR\u0026#34; - npm start -- --token ${{secrets.AUDIT_LOG_TOKEN}} --org ${{secrets.ORG_NAME}} --api \u0026#39;v3\u0026#39; --api-type \u0026#39;all\u0026#39; --cursor $LAST_CURSOR --file \u0026#34;audit-log-output.json\u0026#34; + npm start -- --token ${{secrets.AUDIT_LOG_TOKEN}} \\ + --org ${{secrets.ORG_NAME}} --api \u0026#39;v3\u0026#39; --api-type \u0026#39;all\u0026#39; \\ + --cursor $LAST_CURSOR \\ + --file \u0026#34;audit-logs/audit-log-output-${FILE_SUFFIIX}.json\u0026#34; --pretty fi - curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d @audit-log-output.json ${{secrets.WEBHOOK_URL}} # Commit the cursor back to source - name: Commit cursor uses: EndBug/add-and-commit@v5 with: author_name: Audit Log Integration author_email: ${{ secrets.COMMITTER_EMAIL }} message: \u0026#34;Updating cursor for audit log\u0026#34; add: \u0026#34;.last-v3-cursor-update --force\u0026#34; env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} + - name: Commit audit log + uses: EndBug/add-and-commit@v5 + with: + author_name: Audit Log Integration + author_email: ${{ secrets.COMMITTER_EMAIL }} + message: \u0026#34;Adding audit log\u0026#34; + add: \u0026#34;audit-logs/audit-log-output-*.json --force\u0026#34; + env: + GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} 最後に定義ファイルをコミットして、リモートリポジトリへ登録しましょう。\n実行例）リポジトリにワークフローの登録\n$ git add .github/workflows/ghec-audit-log.yml $ git commit -m \u0026#34;add ghec-audit-log workflow\u0026#34; $ git push -u logging main このリポジトリへの Push をトリガーに GitHub Actions が起動しますので、この後はワークフローが期待通り動作しているかを確認していきましょう。\nStep5. ワークフローの動作確認 # まずは GitHub Actions 画面へ遷移し、ワークフローの起動および処理が成功していることを確認します。出力例のように GitHub Actions のワークフロー実行履歴にて成功が記録されていれば OK です。\n次に監査ログが正しく格納されているか確認しましょう。出力例のように監査ログの格納が確認できれば OK です。\n以上でワークフローの設定も完了です。お疲れ様でした。以降はワークフローで定義したスケジュールに沿って 1 時間ごとに監査ログがエクスポートされるようになるかと思います。\n終わりに # 今回は GitHub Enterprise Cloud (GHEC) の監査ログを取得する方法でした。GHEC の監査ログを長期(90 日以上)保存したい方はエクスポートが必須となってきますので参考にしてみてはいかがでしょうか。\nGitHub は、GitHub Inc. の商標または登録商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。 ","date":"December 7, 2021","permalink":"/posts/2021/12/ghec-audit-log-cli/","section":"記事一覧","summary":"みなさん、こんにちは。今回は GitHub Enterprise Cloud(GHEC) の監査ログ(Audit Log) の取得方法についてのお話です。","title":"GHEC Audit Log CLI を使って GitHub Enterprise Cloud の監査ログを取得してみた"},{"content":"","date":"December 7, 2021","permalink":"/tags/github-actions/","section":"タグ一覧","summary":"","title":"GitHub Actions"},{"content":"","date":"December 7, 2021","permalink":"/tags/github-enterprise-cloud/","section":"タグ一覧","summary":"","title":"GitHub Enterprise Cloud"},{"content":"","date":"December 4, 2021","permalink":"/tags/azure-logic-apps/","section":"タグ一覧","summary":"","title":"Azure Logic Apps"},{"content":"","date":"December 4, 2021","permalink":"/tags/microsoft-sentinel/","section":"タグ一覧","summary":"","title":"Microsoft Sentinel"},{"content":"みなさん、こんにちは。今回は GitHub Enterprise Cloud(GHEC) の各種ログを SIEM1 マネージドサービスである Microsoft Sentinel (旧称 Azure Sentinel) に集約し、サイバー攻撃の兆候を検知できるようにする方法をご紹介していきたいと思います。\nMicrosoft Sentinel と GHEC との連携方法はいくつかあるのですが、この記事では Microsoft Sentinel コミュニティで開発している Azure Logic Apps(ロジックアプリ)、Azure Functions(関数アプリ)の 2 種類のカスタムデータコネクタの内、「Azure Logic Apps コネクタ」を使った方法をご紹介していきたいと思います。\nこれから GHEC の利用を検討している方や、GHEC は利用しているけれど SIEM システムの導入まではしていないという方は、セキュリティ強化策のひとつとして参考にしてみてはいかがでしょうか。\n構築するシステムについて # 今回は Azure Logic Apps(ロジックアプリ)を定期的に起動し、GHEC から監査ログなどを取得して Microsoft Sentinel ワークスペースへ格納、格納されたログに対して Microsoft Sentinel が自動的に相関分析をかけていく、といった流れで処理を行うシステムを構築していきたいと思います。\nAzure Sentinel とのコネクタについては、今回は Microsoft Sentinel コミュニティで公開されている次のカスタムデータコネクタを利用していきましょう。\n本カスタムコネクタをデプロイすると、次の 3 種類の Azure Logic Apps リソースが動作するようになります。\nリソース種別 説明 Audit Playbook 監査ログを定期的に収集する自動ワークフロー (デフォルト 5 分間隔) Repo Playbook 各リポジトリに対するフォーク、クローン、コミットなどの操作ログを定期的に収集する自動ワークフロー (デフォルト 1 時間間隔) Vulnerability Alert Playbook 各リポジトリに対するセキュリティ脆弱性診断ログを定期的に収集する自動ワークフロー (デフォルト 1 日間隔) また、本カスタムコネクタで取得した各種ログデータについては、Log Analytics ワークスペースの次のカスタムテーブルへ格納されるようになります。\nテーブル名 説明 GitHub_CL 監査ログのデータを格納するテーブル GitHubRepoLogs_CL 各リポジトリに対するフォーク、クローン、コミットなどの操作ログやリポジトリに対するセキュリティ脆弱性診断ログのデータを格納するテーブル それでは構築していきましょう # 今回は GitHub Enterprise Cloud → Microsoft Sentinel ワークスペース → カスタムコネクタ → Microsoft Sentinel の順で設定していきましょう。\nStep1. GitHub Enterprise Cloud を設定しよう # ここではログを収集する先の GitHub Organization の作成と、Azure Logic Apps から GitHub API へアクセスする際に利用するアクセストークンの作成を実施していきましょう。\n(1) Organization(Enterprise)の作成 # 監査ログなどを取得する対象の Organization を作成しましょう。今回は GitHub Enterprise Cloud とするため課金プランは「Enterprise」を選択します。\n(2) アクセストークンの作成 # 次に Azure Logic Apps から GitHub API へアクセスする際に利用するアクセストークンを作成します。Organization の Owner 権限を持つユーザで、Settings \u0026gt; Developer settings \u0026gt; Personal access tokens \u0026gt; Generate new tokens から作成しましょう。\n付与するスコープについては Microsoft Sentinel コミュニティでは言及されておりませんが、筆者の環境では必要最低限のスコープとして次の 5 つの権限を付与することで動作を確認することができました。\npublic_repo admin:org read:user user:email admin:enterprise 作成後はデータコネクタをデプロイする際に利用するためトークン情報をコピーしておきます。\nトークン情報が漏れると大変なことになりますので絶対に漏らさないように注意しましょう。 Step2. Microsoft Sentinel ワークスペースを設定しよう # 次に Microsoft Sentinel の設定です。ここでは Microsoft Sentinel 用の Log Analytics ワークスペースの作成および Microsoft Sentinel リソースの作成を実施していきましょう。\n(1) ワークスペースの作成 # Azure ポータルなどから Microsoft Sentinel および Microsoft Sentinel 用の Log Analytics ワークスペースを作成しましょう。\n(2) ワークスペース情報の取得 # Microsoft Sentinel リソースの作成後は、データコネクタをデプロイする際に利用するため Log Analytics ワークスペース画面のエージェント管理などから「ワークスペース ID」と「主キー」をコピーしておきます。\nStep3. カスタムコネクタを設定しよう # ここでは GitHub 用カスタムデータコネクタを Azure Resource Manager(ARM) テンプレートからデプロイして、対象 GitHub Organization からログデータを取得できるようにデータコネクタの諸設定まで実施していきましょう。\n(1) ARM テンプレートのデプロイ # それでは GitHub 用カスタムデータコネクタをデプロイしていきましょう。データコネクタの「Readme」に記載されている Deploy to Azureボタンをクリックします。\nAzure のカスタムテンプレートのデプロイ画面へ遷移したらパラメータ値を入力して作成をしましょう。\nパラメータ 説明 Personal Access Token GitHub のアクセストークンを指定する User Name GitHub アクセストークンなどを格納する Azure Key Vault キーコンテナへのアクセスを許可する Azure AD ユーザ名を指定する Principal Id 上記 Azure AD ユーザのオブジェクト ID を指定する Workspace Id Microsoft Sentinel ワークスペースのワークスペース ID を指定する Workspace Key Microsoft Sentinel ワークスペースの主キーを指定する デプロイにかかる時間は環境により誤差はあると思いますがおおよそ 2 分でした。デプロイが終わると Azure Logic Apps 以外にもストレージアカウントや Azure Key Vault などのリソースも作成されていることがわかります。\n(2) Azure Key Vault API 接続の設定 # Azure Logic Apps から Azure Key Vault キーコンテナに格納された GitHub アクセストークンへのアクセスを可能とするため、keyvault-GitHubPlaybooks リソースの設定画面から API 接続への承認処理を行います。\n(3) 設定ファイルの作成 # データコネクタの 2 種類の設定ファイル ORGS.json と lastrun-Audit.json を作成します。各ファイルの概要およびフォーマットは次の通りです。\nファイル名 説明 ORGS.json ログ取得対象の GitHub Organization を定義するファイル lastrun-Audit.json 最終実行時刻を管理するファイル(新しいレコードのみを収集するために利用される) 作成例）ORGS.json\n[ {​​​​​​​ \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your 1st organization account name\u0026gt;\u0026#34; }​​​​​​​, {​​​​​​​​​​​​​​​​​​​​​ \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your 2nd organization account name\u0026gt;\u0026#34; }​​​​​​​​​​​​​​​​​​​​​, {​​​​​​​​​​​​​​​​​​​​​ \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your Nth organization account name\u0026gt;\u0026#34; }​​​​​​​​​​​​​​​​​​​​​ ] 作成例）lastrun-Audit.json\n{​​​​​​​​​​​​​​​​​​​​​ \u0026#34;lastContext\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;lastRun\u0026#34;: \u0026#34;\u0026#34; }​​​​​​​​​​​​​​​​​​​​​ ORGS.json ファイルを編集し、org パラメータ値をログ取得対象の GitHub Organization アカウント名に置きかえます。フォーマットのように、複数の GitHub Organization を指定することもできますので、実際の GitHub Organization の数にあわせて増減させてください。\nlastrun-Audit.json ファイルの各パラメータ値については、カスタムコネクタが動作すると自動的に値が更新されるためユーザ側で意識する必要はありません。\n(4) 設定ファイルの配置 # 2 種類のファイルを作成したら、ARM テンプレートから作成されたストレージアカウントの githublogicapp コンテナにアップロードしましょう。\n(5) Azure Blob Storage API 接続の設定 # Azure Logic Apps からストレージアカウントに設定ファイルへのアクセスを可能とするため、ストレージアカウントのアクセスキー情報を取得し、azureblob-GitHubPlaybooks リソースの設定画面から API 接続を行います。\n(6) Azure Logic Apps の有効化 # 最後に ARM テンプレートで作成された 3 つの Azure Logic Apps を有効化していきましょう。\nここまででひととおりの設定は終わりましたので、あとはスケジュールに沿ってログの取得が行われるようになっていることを確認していきましょう。\n(7) 動作確認 # まずは Azure Logic Apps から見ていきましょう。Azure Logic Apps では各アプリケーションが正しくスケジュール実行されていること、実行されたアプリケーションが正常終了していることを確認します。出力例のように実行の履歴にて成功と記録されていることが確認できれば OK です。\n次に Log Analytics ワークスペースを確認していきましょう。ここではカスタムログに GitHub Enterprise Cloud から取得したログが格納されていることを確認します。出力例のように「GitHub_CL」と「GitHubRepoLogs_CL」の 2 種類のカスタムテーブルが作成されていれば OK です。\n最後に Microsoft Sentinel を確認してみましょう。出力例のように「GitHub_CL」および「GitHubRepoLogs_CL」からのイベントが記録されていれば OK です。\n以上でカスタムコネクタのデプロイは終わりです。お疲れ様でした。\nStep4. Microsoft Sentinel を設定しよう # さてログデータが収集できるようになりましたので、ここからは収集したログデータを自動で分析して脅威を検出できるように Microsoft Sentinel の設定をしていきましょう。今回は Microsoft Sentinel が標準で用意している GitHub 固有の分析ルールを有効化し、次のイベントを自動で検出できるようにしていきたいと思います。\nGitHub アカウントへのブルートフォース攻撃 (※1) 複数の異なるロケーションからのサインインバースト (※1) 新しい国からのアクティビティ TI インジケータに登録した IP アドレスからのアクセス (※2) 二要素認証の無効化 リポジトリ内にセキュリティ脆弱性 ※1: Azure Active Directory(Azure AD) とのシングルサインオン(SSO)設定、Azure AD ログの収集が必要\n※2: Microsoft Sentinel の脅威インテリジェンス設定にて TI インジケータの事前登録が必要\n(1) データの解析および正規化 # まずは GitHub_CL テーブルおよび GitHubRepoLogs_CL テーブルに格納されているデータを、Microsoft Sentinel で分析しやすいように加工をしていきましょう。なお、Microsoft Sentinel コミュニティにてデータ加工用のパーサ関数(GitHubAudit 関数、GitHubRepo 関数)が公開されておりますので、今回はこちらを利用していきましょう。\n作成例）GitHubAudit関数\nGitHub_CL | project TimeGenerated=node_createdAt_t, Organization=columnifexists(\u0026#39;node_organizationName_s\u0026#39;, \u0026#34;\u0026#34;), Action=node_action_s, OperationType=node_operationType_s, Repository=columnifexists(\u0026#39;node_repositoryName_s\u0026#39;,\u0026#34;\u0026#34;), Actor=columnifexists(\u0026#39;node_actorLogin_s\u0026#39;, \u0026#34;\u0026#34;), IPaddress=columnifexists(\u0026#39;node_actorIp_s\u0026#39;, \u0026#34;\u0026#34;), City=columnifexists(\u0026#39;node_actorLocation_city_s\u0026#39;, \u0026#34;\u0026#34;), Country=columnifexists(\u0026#39;node_actorLocation_country_s\u0026#39;, \u0026#34;\u0026#34;), ImpactedUser=columnifexists(\u0026#39;node_userLogin_s\u0026#39;, \u0026#34;\u0026#34;), ImpactedUserEmail=columnifexists(\u0026#39;node_user_email_s\u0026#39;, \u0026#34;\u0026#34;), InvitedUserPermission=columnifexists(\u0026#39;node_permission_s\u0026#39;, \u0026#34;\u0026#34;), Visibility=columnifexists(\u0026#39;node_visibility_s\u0026#39;, \u0026#34;\u0026#34;), OauthApplication=columnifexists(\u0026#39;node_oauthApplicationName_s\u0026#39;, \u0026#34;\u0026#34;), OauthApplicationUrl=columnifexists(\u0026#39;node_applicationUrl_s\u0026#39;, \u0026#34;\u0026#34;), OauthApplicationState=columnifexists(\u0026#39;node_state_s\u0026#39;, \u0026#34;\u0026#34;), UserCanInviteCollaborators=columnifexists(\u0026#39;node_canInviteOutsideCollaboratorsToRepositories_b\u0026#39;, \u0026#34;\u0026#34;), MembershipType=columnifexists(\u0026#39;node_membershipTypes_s\u0026#39;, \u0026#34;\u0026#34;), CurrentPermission=columnifexists(\u0026#39;node_permission_s\u0026#39;, \u0026#34;\u0026#34;), PreviousPermission=columnifexists(\u0026#39;node_permissionWas_s\u0026#39;, \u0026#34;\u0026#34;), TeamName=columnifexists(\u0026#39;node_teamName_s\u0026#39;, \u0026#34;\u0026#34;), Reason=columnifexists(\u0026#39;node_reason_s\u0026#39;, \u0026#34;\u0026#34;), BlockedUser=columnifexists(\u0026#39;node_blockedUserName_s\u0026#39;, \u0026#34;\u0026#34;), CanCreateRepositories=columnifexists(\u0026#39;canCreateRepositories_b\u0026#39;, \u0026#34;\u0026#34;) 作成例）GitHubRepo関数\nGitHubRepoLogs_CL | project TimeGenerated = columnifexists(\u0026#39;DateTime_t\u0026#39;, \u0026#34;\u0026#34;), Organization=columnifexists(\u0026#39;Organization_s\u0026#39;, \u0026#34;\u0026#34;), Repository=columnifexists(\u0026#39;Repository_s\u0026#39;,\u0026#34;\u0026#34;), Action=columnifexists(\u0026#39;LogType_s\u0026#39;,\u0026#34;\u0026#34;), Actor=coalesce(login_s, owner_login_s), ActorType=coalesce(owner_type_s, type_s), IsPrivate=columnifexists(\u0026#39;private_b\u0026#39;,\u0026#34;\u0026#34;), ForksUrl=columnifexists(\u0026#39;forks_url_s\u0026#39;,\u0026#34;\u0026#34;), PushedAt=columnifexists(\u0026#39;pushed_at_t\u0026#39;,\u0026#34;\u0026#34;), IsDisabled=columnifexists(\u0026#39;disabled_b\u0026#39;,\u0026#34;\u0026#34;), AdminPermissions=columnifexists(\u0026#39;permissions_admin_b\u0026#39;,\u0026#34;\u0026#34;), PushPermissions=columnifexists(\u0026#39;permissions_push_b\u0026#39;,\u0026#34;\u0026#34;), PullPermissions=columnifexists(\u0026#39;permissions_pull_b\u0026#39;,\u0026#34;\u0026#34;), ForkCount=columnifexists(\u0026#39;forks_count_d\u0026#39;,\u0026#34;\u0026#34;), Count=columnifexists(\u0026#39;count_d,\u0026#39;,\u0026#34;\u0026#34;), UniqueUsersCount=columnifexists(\u0026#39;uniques_d\u0026#39;,\u0026#34;\u0026#34;), DismmisedAt=columnifexists(\u0026#39;dismissedAt_t\u0026#39;,\u0026#34;\u0026#34;), Reason=columnifexists(\u0026#39;dismissReason_s\u0026#39;,\u0026#34;\u0026#34;), vulnerableManifestFilename = columnifexists(\u0026#39;vulnerableManifestFilename_s\u0026#39;,\u0026#34;\u0026#34;), Description=columnifexists(\u0026#39;securityAdvisory_description_s\u0026#39;,\u0026#34;\u0026#34;), Link=columnifexists(\u0026#39;securityAdvisory_permalink_s\u0026#39;,\u0026#34;\u0026#34;), PublishedAt=columnifexists(\u0026#39;securityAdvisory_publishedAt_t \u0026#39;,\u0026#34;\u0026#34;), Severity=columnifexists(\u0026#39;securityAdvisory_severity_s\u0026#39;,\u0026#34;\u0026#34;), Summary=columnifexists(\u0026#39;securityAdvisory_summary_s\u0026#39;,\u0026#34;\u0026#34;) 次の画像を参考に Microsoft Sentinel のログ画面から 2 つのパーサ関数を登録しましょう。なお、「従来のカテゴリ」の値については任意の文字列で大丈夫です。\n(2) 分析ルールの追加 # 次に GitHub 固有の脅威を検出できるように分析ルールをテンプレートから追加していきしょう。Microsoft Sentinel では GitHub 固有の脅威に対する分析ルールのテンプレートが用意されていますので、今回はこちらを活用して分析ルールの追加をしてきたいと思います。出力例のように検索キーワードに「github」と入力することで目的のテンプレートを見つけることができます。\nなお、2021 年 11 月時点で用意されているテンプレートは次の 6 種類となります。\nNo. テンプレート名 説明 1 GitHub Two Factor Auth Disable 二要素認証の無効化イベントを検知するルール、デフォルトでは 1 日ごとにクエリを実行 2 GitHub Activites from a New Country 新しい国からのアクティビティを検知するルール、デフォルトでは過去 7 日分を学習データに利用して 1 日ごとにクエリを実行 3 Brute Force Attack against GitHub Account GitHub アカウントへのブルートフォース攻撃を検知するルール、デフォルトでは 1 日ごとにクエリを実行 4 GitHub Signin Burst from Multiple Locations 複数の異なるロケーションからのサインインバーストを検知するルール、デフォルトでは 1 時間ごとにクエリを実行 5 TI map IP entity to GitHub_CL TI インジケータに登録した IP アドレスからのアクセスを検知するルール、デフォルトでは 1 時間ごとにクエリを実行 6 GitHub Security Vulnerability in Repository リポジトリ内にセキュリティ脆弱性が含まれていることを検知するルール、デフォルトでは 1 時間ごとにクエリを実行 それではテンプレートを用いた分析ルールの追加をしていきましょう。まず追加したいルールを選択し、「ルールの作成」ボタンをクリックします。\nルールの追加画面ではウィザードにしたがってルールの作成を行っていきます。各パラメータにはテンプレートによってデフォルト値が入力されていますのでとくに値を変更する要件がなければそのまま作成していきましょう。\n分析ルールが追加されたテンプレートについては次の出力例のように「使用中」アイコンが付与されます。先ほどと同様の手順を繰り返し、他のテンプレートに対しても分析ルールの追加をしていきましょう。出力例のように 6 種類すべてに使用中アイコンが付与されれば脅威を自動検知するルールの設定も終わりです。お疲れ様でした。\n補足、もう一歩踏み込んだ脅威分析を行うには # Microsoft Sentinel では今回紹介した標準の分析ルールを利用する以外に、カスタム分析ルールを自作してより高度なイベントの検知をすることが可能です。サンプルとして次のようなカスタム分析ルールが Microsoft 技術ブログでも紹介されていますのでぜひ参考にしてみてください。\nリポジトリに対する異常数のクローン操作 リポジトリの一括削除 リポジトリをプライベートからパブリックに変更 リポジトリに対する部外者からのフォーク操作 GitHub Organization へのユーザ招待およびユーザ追加 ユーザへのアクセス許可の追加付与　など 終わりに # 今回は GitHub Enterprise Cloud の各種ログを SIEM マネージドサービスである Microsoft Sentinel (旧称 Azure Sentinel) に集約し、サイバー攻撃の兆候を検知できるようにする方法のご紹介でしたがいかがだったでしょうか？\nGitHub Enterprise Cloud に限らずさまざまな監査ログを収集し、長期保存されている方は多いと思います。ただ、せっかく取得するのであればただ集めて長期保存しておくだけではなく、Microsoft Sentinel を活用してもう一歩進んだセキュアな環境を実現してみる、というのも \u0026ldquo;有り\u0026rdquo; なのではないでしょうか。\n以上、「Microsoft Sentinel を使って GitHub Enterprise Cloud のセキュリティを強化しよう (Azure Logic Apps 編)」でした。\nMicrosoft Azure は，Microsoft Corporation の商標または登録商標です。 GitHub は、GitHub Inc. の商標または登録商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。 SIEM(Security Information and Event Management) は、さまざまなログを一元的に集約、相関分析をしてサイバー攻撃などの異常を自動的に検出するソリューションです。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"December 4, 2021","permalink":"/posts/2021/11/azure-sentinel-logicapps-data-connector-for-ghec/","section":"記事一覧","summary":"みなさん、こんにちは。今回は GitHub Enterprise Cloud(GHEC) の各種ログを SIEM1 マネージドサービスである Microsoft Sentinel (旧称 Azure Sentinel) に集約し、サイバー攻撃の兆候を検知できるようにする方法をご紹介していきたいと思います。","title":"Microsoft Sentinel を使って GitHub Enterprise Cloud のセキュリティを強化しよう (Azure Logic Apps コネクタ編)"},{"content":"","date":"December 4, 2021","permalink":"/tags/siem/","section":"タグ一覧","summary":"","title":"SIEM"},{"content":"","date":"December 2, 2021","permalink":"/tags/aws-cloudformation/","section":"タグ一覧","summary":"","title":"AWS CloudFormation"},{"content":"","date":"December 2, 2021","permalink":"/tags/aws-codepipeline/","section":"タグ一覧","summary":"","title":"AWS CodePipeline"},{"content":"","date":"December 2, 2021","permalink":"/tags/taskcat/","section":"タグ一覧","summary":"","title":"TaskCat"},{"content":"みなさん、こんにちは。今回は TaskCat というオープンソースを利用した AWS CloudFormation (CFn) テンプレートの自動テストについてのお話です。\nCFn テンプレートを扱っていると構文エラーチェックはパスしたものの、いざ動かしてみたらスタックの作成でエラーになってしまうといった経験をすることがあるかと思います。TaskCat は多くの方にとってあまり馴染みのないツールだと思いますが、実際に使ってみるととても手軽に CFn テンプレートの自動テストを実現できます。\n今回は Linux 上に開発環境を作るところからはじめて、簡素なサンプルを用いたテストの実行、テスト自動化を組み込んだシンプルな CI/CD パイプラインの構築まで紹介していきたいと思います。これから CFn テンプレート開発されている方で自動テストをやりたいと考えている方は参考にしてみてはいかがでしょうか。\nTaskCat とは # TaskCat とは、AWS CloudFormation (CFn) テンプレートの自動テストを行う Python 製のテストツールです。\nこのツールを利用することで、指定した各リージョンに CFn テンプレートから環境を一時的にデプロイ、各リージョンでのデプロイ可否結果のレポート生成、テストで一時的に作成した環境を削除、といった一連の流れを自動化できます。\nなお、AWS TackCat はローカルでテストを実行する際に Docker が必要となるため、Docker をサポートしていない AWS CloudShell では利用することができないのでご注意ください。\nTaskCat を使ってみよう # 冒頭ですでに述べたとおり、今回は Linux 上に開発環境を作るところからはじめて、簡素なサンプルを用いたテストの実行、テスト自動化を組み込んだシンプルな CI/CD パイプラインの構築まで紹介していきたいと思います。\nまずは開発環境の設定から # それでは Linux 上に開発環境を作っていきたいと思います。まず TaskCat をインストールする事前準備として Python の仮想環境を作成していきましょう。なお、今回の例で使用している Linux ディストリビューションは Amazon Linux 2 です。\n実行例）\n$ sudo yum install -y python3 $ python3 --version Python 3.7.10 $ python3 -m venv venv37 $ . venv37/bin/activate 次に、TaskCat をインストールします。\n実行例）\n$ python3 -m pip install taskcat $ taskcat --version _ _ _ | |_ __ _ ___| | _____ __ _| |_ | __/ _` / __| |/ / __/ _` | __| | || (_| \\__ \\ \u0026lt; (_| (_| | |_ \\__\\__,_|___/_|\\_\\___\\__,_|\\__| version 0.9.25 0.9.25 TaskCat のテストに必要な docker サービスやこの後のステップで利用する git を追加で設定します。\n実行例）\n$ sudo yum install -y docker git $ sudo systemctl start docker 最後に、AWS CLI の設定をして開発環境の構築は完了です。\n実行例）\n$ aws configure 手動でテストを実行してみよう # では簡単なサンプルを用いて TaskCat を使ったテストを行っていきましょう。今回は、東京リージョン(ap-northeast-1)と大阪リージョン(ap-northeast-3)の 2 つのリージョンに対して、同じテンプレートを使ってスタックの作成ができるかを確認していきたいと思います。\nStep1. テスト対象のテンプレートを作成しよう # 今回は VPC を作るだけのとてもシンプルなテンプレートを用意しました。\n作成例）my-vpc.yaml\nAWSTemplateFormatVersion: \u0026#34;2010-09-09\u0026#34; Description: Sample CloudFormation Template Parameters: vpcIpv4CicdBlock: Type: String Default: 10.0.0.0/16 vpcNameTag: Type: String Resources: myVPC: Type: AWS::EC2::VPC Properties: CidrBlock: !Ref vpcIpv4CicdBlock EnableDnsSupport: true EnableDnsHostnames: true Tags: - Key: Name Value: !Ref vpcNameTag Outputs: myVpcId: Description: VPC ID Value: !Ref myVPC Export: Name: myVpcId Step2. TaskCat のテスト定義ファイルを作成しよう # 次に TaskCat のテスト定義ファイル .taskcat.yml を作成します。今回の例では東京リージョン(ap-northeast-1)と大阪リージョン(ap-northeast-3)でテストを実施するように定義しています。\n作成例）.taskcat.yml\nproject: name: sample-taskcat-project regions: - ap-northeast-1 - ap-northeast-3 tests: test-my-vpc: parameters: vpcIpv4CicdBlock: 10.255.0.0/16 vpcNameTag: test-vpc template: my-vpc.yml Step3. テストを実行してみよう # では次のコマンドでテストを実行していきましょう。テストを実行すると、スタック作成が東京リージョンと大阪リージョンに対して並列で実行され、各リージョンでのスタック作成の成否結果の収集、スタック削除まで自動的に行われます。\n実行例）\n$ taskcat test run _ _ _ | |_ __ _ ___| | _____ __ _| |_ | __/ _` / __| |/ / __/ _` | __| | || (_| \\__ \\ \u0026lt; (_| (_| | |_ \\__\\__,_|___/_|\\_\\___\\__,_|\\__| version 0.9.25 [INFO ] : Linting passed for file: /root/aws-taskcat-sample/my-vpc.yaml [S3: -\u0026gt; ] s3://tcat-sample-taskcat-project-XXXXXXX/sample-taskcat-project/my-vpc.yaml [INFO ] : ┏ stack Ⓜ tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f [INFO ] : ┣ region: ap-northeast-1 [INFO ] : ┗ status: CREATE_COMPLETE [INFO ] : ┏ stack Ⓜ tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f [INFO ] : ┣ region: ap-northeast-3 [INFO ] : ┗ status: CREATE_COMPLETE [INFO ] : Reporting on arn:aws:cloudformation:ap-northeast-1:\u0026lt;AWSアカウント名\u0026gt;:stack/tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f/XXXXXXX [INFO ] : Reporting on arn:aws:cloudformation:ap-northeast-3:\u0026lt;AWSアカウント名\u0026gt;:stack/tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f/XXXXXXX [INFO ] : Deleting stack: arn:aws:cloudformation:ap-northeast-3:\u0026lt;AWSアカウント名\u0026gt;:stack/tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f/XXXXXXX [INFO ] : Deleting stack: arn:aws:cloudformation:ap-northeast-1:\u0026lt;AWSアカウント名\u0026gt;:stack/tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f/XXXXXXX [INFO ] : ┏ stack Ⓜ tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f [INFO ] : ┣ region: ap-northeast-1 [INFO ] : ┗ status: DELETE_COMPLETE [INFO ] : ┏ stack Ⓜ tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f [INFO ] : ┣ region: ap-northeast-3 [INFO ] : ┗ status: DELETE_COMPLETE 実行後は taskcat_outputs ディレクトリにリージョンごとの実行ログが出力されますので、こちらからスタック作成の成否結果を確認できます。\n実行例）\n$ tree -a taskcat_outputs/ taskcat_outputs/ ├── index.html ├── tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f-ap-northeast-1-cfnlogs.txt └── tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f-ap-northeast-3-cfnlogs.txt 0 directories, 3 files $ cat taskcat_outputs/*-ap-northeast-3-*.txt ----------------------------------------------------------------------------- Region: ap-northeast-3 StackName: tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f ***************************************************************************** ResourceStatusReason: Stack launch was successful ***************************************************************************** : 以上、ローカル環境で CFn テンプレートのテストを実行する方法のご紹介でした。\nCI/CD パイプラインに組み込んでみよう # 次はもう一歩進んで、ソースコードの更新をトリガーに自動でテストを実行する CI/CD パイプラインを構築し、実際に動かすところまで行ってみたいと思います。なお、今回は AWS リソース作成を簡略化するため AWS クイックスタートを利用して CI/CD パイプラインを構築していきたいと思います。\nなお、今回作成するパイプラインでは開発ブランチ（例では develop ブランチ）に対して更新が入ると、それをトリガーに AWS TackCat を活用した自動テストを実行し、テストに成功したら特定ブランチ(例では main ブランチ)へマージするという一連の流れを自動化しています。\nStep1. GitHub にリポジトリを作ろう # まずは枠だけ作っておきましょう。中身は後続のステップで入れます。\nStep2. GitHub でアクセストークンを作ろう # GitHub \u0026gt; Setting \u0026gt; Developer settings \u0026gt; Personal access tokens \u0026gt; Generate new tokens から、指定の通り「repo」と「admin:repo_hook」のスコープを選択したトークンを作ります。\n作成に成功したら次のようにトークンが表示されます。スタック作成時に利用しますのでコピーしておきましょう。なお、トークン情報が漏れると大変なことになりますので絶対に漏らさないように注意しましょう。\nStep3. クイックスタートを起動します # ではクイックスタートを活用して環境を構築していきましょう。まずはクイックスタートサイトの「クイックスタートを起動します」をクリックします。\nリンクをクリックすると自動的にスタックの作成画面へ遷移します。作成先のリージョンがデフォルトだとオレゴンなので適宜変更して「次へ」をクリックします。\n次にパラメータを入力していきましょう。\n必要に応じてタグなどの設定を実施し、確認画面で入力ミスがないかを確認したら「スタックの作成」を実行します。\nあとはステータスが「CREATE_COMPLETE」になるまで待つだけです。\n以上で、CI/CD パイプラインに必要な AWS リソースの構築まで完了しました。\nStep4. CI/CD パイプラインの動作確認 # ではソースコードを GitHub に登録して CI/CD パイプラインが動作することを確認していきましょう。登録するソースコードは先ほど作成したこちらのファイルです。\n実行例）\n$ tree -a . ├── my-vpc.yaml └── .taskcat.yml 0 directories, 2 files スタック作成時に指定した GitHub の監視対象ブランチ(例では develop ブランチ)へソースコードをプッシュします。\n実行例）\n$ git init . $ git remote add origin \u0026lt;GitHub上のリポジトリ\u0026gt; $ git pull origin main $ git branch develop $ git checkout develop $ git add . $ git commit -m \u0026#34;initial commit\u0026#34; $ git push origin develop さてここからは再び AWS マネジメントコンソールへ移ります。先ほどのプッシュをトリガーにパイプラインが起動していることを確認するため、まずは CodePipeline 画面へ行きましょう。次のように CI/CD パイプラインが動作していることが確認できるかと思います。\nここからドリルダウンでビルドログなどを見ることができます。Build ステージの CodeBuild アクションボックスの「詳細」ボタンをクリックし、ビルドログを確認しましょう。出力例では指定したリージョンでのテストに成功していることがわかります。\n以上、TaskCat によるテストを組み込んだ CI/CD パイプラインの作成のご紹介でした。\n終わりに # TaskCat はいかがだったでしょうか？\n今回はクイックスタートを使って環境を構築しましたが、すでにパイプラインを構築されている方はリポジトリに .taskcat.yml を追加して、pip install tackcat と taskcat test run をステップに追加するだけで簡単に CFn テンプレートの自動テストを組み込むことができます。気になった方はぜひ触ってみていただければと思います。\n以上、AWS CloudFormation テンプレートの自動テストを実現する「TaskCat」のご紹介でした。\nAWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 GitHub は、GitHub Inc. の商標または登録商標です。 Linux は、Linus Torvalds 氏 の日本およびその他の国における登録商標または商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。 ","date":"December 2, 2021","permalink":"/posts/2021/10/aws-taskcat/","section":"記事一覧","summary":"みなさん、こんにちは。今回は TaskCat というオープンソースを利用した AWS CloudFormation (CFn) テンプレートの自動テストについてのお話です。","title":"TaskCatを使ってCloudFormationテンプレートの自動テストをしよう"},{"content":"","date":"December 1, 2021","permalink":"/tags/aws-chalice/","section":"タグ一覧","summary":"","title":"AWS Chalice"},{"content":"みなさん、こんにちは。今回は「AWS Chalice」を活用したサーバレスアプリケーション開発についてのお話です。AWS Summit Online Japan 2021 の日立製作所の講演で軽く触れたこともあり、どこかで紹介したいと思っておりました。\nさて、AWS Chalice は多くの方にとってあまり馴染みのないツールだと思いますが、実際に使ってみるととても手軽に Amazon API Gateway と AWS Lambda を使用するサーバレスアプリケーションを作成してデプロイできます。もちろん、どんなツールにも得手不得手はあるので「すべてのプロジェクトで AWS Chalice の活用が最適か？」と言われればもちろん「No！」なのですが、ちょっと試しに動く Web API をサッと手軽に作りたい、といったケースにはとても良いソリューションだと思います。\n今回は AWS CloudShell 上に開発環境を作るところからはじめて、簡素なサンプルを用いた一連の開発の流れ、テスト自動化を組み込んだシンプルな CI/CD パイプラインの構築まで紹介していきたいと思います。これから実際に動くサーバレスアプリケーションを手軽に作りたいと思われている方は参考にしてみてはいかがでしょうか。\nAWS Chalice とは # AWS Chalice とは、Amazon AWS Gateway や AWS Lambda を用いたサーバレスアプリケーションを、お手軽に開発できるようにする Python 製のサーバレスアプリケーションフレームワークです。具体的には、Web API をシンプルで直感的なコードで実装できるようにする機能や、作成したコードからアプリケーションの作成やデプロイを実行するコマンドラインインタフェース(CLI)といった開発者にやさしい機能を提供してくれます。\n普段から Python に触れている方であれば似たような機能として Flask や Bottle をイメージされるかと思いますが、これらにサーバレス環境へデプロイする機能が追加で付与されたものが AWS Chalice、とイメージしていただくと良いのかなと思います。\nAWS Chalice を使ってみよう # 冒頭ですでに述べたとおり、今回は AWS CloudShell 上に開発環境を作るところからはじめて、簡素なサンプルを用いた一連の開発の流れ、テスト自動化を組み込んだシンプルな CI/CD パイプラインの構築まで紹介していきたいと思います。\nまずは開発環境の構築から # 今回は AWS CloudShell 上に開発環境を作っていきたいと思います。 それではまず AWS Chalice をインストールする事前準備として Python の仮想環境を作成していきましょう。\n実行例）\n$ sudo yum install -y python3 $ python3 --version Python 3.7.10 $ python3 -m venv venv37 $ . venv37/bin/activate 次に、AWS Chalice をインストールします。\n実行例）\n$ python3 -m pip install chalice $ chalice --version chalice 1.26.0, python 3.7.10, linux 4.14.243-185.433.amzn2.x86_64 最後に、AWS CLI の設定をして開発環境の構築は完了です。\n実行例）\n$ aws configure 簡単なアプリケーションを動かしてみよう # ここでは AWS Chalice を使ったアプリケーションの作成からデプロイまでの流れを、次のような簡単なサンプルを用いて紹介していきたいと思います。\nStep1. 新規プロジェクトの作成 # まずは chalice new-project コマンドを実行して新しいプロジェクトを作成します。出力例のようにプロジェクトを新しく作るとデフォルトでサンプルプログラムも生成されます。\n実行例）\n$ chalice new-project \u0026lt;任意のプロジェクト名\u0026gt; $ cd \u0026lt;任意のプロジェクト名\u0026gt; $ tree -a . ├── app.py # APIの実装を行うファイル ├── .chalice │ └── config.json # Chaliceの設定を行うファイル ├── .gitignore └── requirements.txt # 利用するライブラリの定義を行うファイル 1 directory, 4 file Step2. ソースコードの編集 # 今回はデフォルトで作られたソースコードにちょっとだけ手を加えました。修正後のファイルの中身は次の通りです。実際の例を見ていただけるとわかる通り、AWS Chalice を用いた実装は Python を普段使わないという方でも直感的でわかりやすい構文になっているのではないでしょうか。\n作成例）app.py\nfrom chalice import Chalice app = Chalice(app_name=\u0026#39;\u0026lt;任意のプロジェクト名\u0026gt;\u0026#39;) app.log.setLevel(logging.INFO) @app.route(\u0026#39;/hello\u0026#39;) def hello(): app.log.debug(\u0026#34;Invoking from function hello\u0026#34;) return {\u0026#39;hello\u0026#39;: \u0026#39;world\u0026#39;} @app.route(\u0026#39;/hello\u0026#39;, methods=[\u0026#39;POST\u0026#39;], content_types=[\u0026#39;application/json\u0026#39;], cors=True) def hello_post(): app.log.debug(\u0026#34;Invoking from function hello_post\u0026#34;) request = app.current_request return {\u0026#39;result\u0026#39;: request.json_body[\u0026#39;payload\u0026#39;]} 作成例）requirements.txt\nchalice 作成例）.chalice/config.json\n{ \u0026#34;version\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;app_name\u0026#34;: \u0026#34;\u0026lt;任意のプロジェクト名\u0026gt;\u0026#34;, \u0026#34;stages\u0026#34;: { \u0026#34;dev\u0026#34;: { \u0026#34;api_gateway_stage\u0026#34;: \u0026#34;api\u0026#34; } } } Step3. ローカル環境で動作確認 # AWS 上へデプロイする前にローカル環境で軽く動作をしたい場合は、chalice local コマンドを実行します。 実行例のように期待通りのレスポンスが返ってくるようであれば、いよいよ AWS 上へデプロイをしていきましょう。\n実行例）Tarminal#1から実行\n$ chalice local Serving on http://127.0.0.1:8000 実行例）Tarminal#2から実行\n$ curl http://127.0.0.1:8000/hello {\u0026#34;hello\u0026#34;:\u0026#34;world\u0026#34;} $ curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;payload\u0026#34;:\u0026#34;hello, world\u0026#34;}\u0026#39; http://127.0.0.1:8000/hello {\u0026#34;result\u0026#34;:\u0026#34;hello, world\u0026#34;} Step4. AWS 上へデプロイ # AWS 上へアプリケーションのデプロイをする場合は、chalice deploy コマンドを実行します。実行例ではメッセージから新たに Lambda 関数、API Gateway と IAM ロールが作られていることがわかります。\n実行例）\n$ chalice deploy --stage dev Creating deployment package. Creating IAM role: \u0026lt;任意のプロジェクト名\u0026gt;-dev Creating lambda function: \u0026lt;任意のプロジェクト名\u0026gt;-dev Creating Rest API Resources deployed: - Lambda ARN: arn:aws:lambda:ap-northeast-1:\u0026lt;AWSアカウント名\u0026gt;:function:\u0026lt;任意のプロジェクト名\u0026gt;-dev - Rest API URL: https://\u0026lt;文字列\u0026gt;.execute-api.ap-northeast-1.amazonaws.com/api/ デプロイ完了後は期待するレスポンスが返ってくるか確認しましょう。\n実行例）\n$ curl https://\u0026lt;文字列\u0026gt;.execute-api.ap-northeast-1.amazonaws.com/api/hello {\u0026#34;hello\u0026#34;:\u0026#34;world\u0026#34;} $ curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;payload\u0026#34;:\u0026#34;hello, world\u0026#34;}\u0026#39; https://\u0026lt;文字列\u0026gt;.execute-api.ap-northeast-1.amazonaws.com/api/hello {\u0026#34;result\u0026#34;:\u0026#34;hello, world\u0026#34;} 以上、AWS Chalice を使ったアプリケーションの作成からデプロイまでの一連の流れでした。\nStepEX. デプロイしたアプリケーションの削除 # 不要になったアプリケーションは chalice delete コマンドを使って削除しておきましょう。\n実行例）\n$ chalice delete Deleting Rest API: \u0026lt;文字列\u0026gt; Deleting function: arn:aws:lambda:ap-northeast-1:\u0026lt;AWSアカウント名\u0026gt;:function:\u0026lt;任意のプロジェクト名\u0026gt;-dev Deleting IAM role: \u0026lt;任意のプロジェクト名\u0026gt;-dev ユニットテストを自動化しよう # 近頃はユニットテストの自動化は一般的に行われているかと思います。AWS Chalice でも Python の一般的なテストツール Pytest を使ってユニットテストを実装できます。例として、今回は次のサンプルに対するテストコードを実装してみます。\n作成例）app.py(テスト対象のプログラム)\nfrom chalice import Chalice app = Chalice(app_name=\u0026#39;\u0026lt;任意のプロジェクト名\u0026gt;\u0026#39;) app.log.setLevel(logging.INFO) @app.route(\u0026#39;/hello\u0026#39;) def hello(): app.log.debug(\u0026#34;Invoking from function hello\u0026#34;) return {\u0026#39;hello\u0026#39;: \u0026#39;world\u0026#39;} @app.route(\u0026#39;/hello\u0026#39;, methods=[\u0026#39;POST\u0026#39;], content_types=[\u0026#39;application/json\u0026#39;], cors=True) def hello_post(): app.log.debug(\u0026#34;Invoking from function hello_post\u0026#34;) request = app.current_request return {\u0026#39;result\u0026#39;: request.json_body[\u0026#39;payload\u0026#39;]} Step1. テストコードの実装 # では、まず必要なファイルを用意していきましょう。中身は後で入れるとしてここでは空ファイルだけ作ります。\n実行例）\n$ touch test_requirements.txt # テストプログラムの依存ライブラリ定義 $ mkdir tests $ touch tests/__init__.py $ touch tests/conftest.py # テストプログラム間で共通の処理を実装 $ touch tests/test_app.py # テストプログラムを実装 次に各ファイルを編集していきましょう。今回は HTTP レスポンスのステータスコードとボディの中身を確認するだけの簡単なテストプログラムを用意しました。\n作成例）test_requirements.txt（依存ライブラリの定義）\npytest-chalice pytest-cov 作成例）tests/init.py\n(EOF) 作成例）tests/conftest.py（共通処理の実装）\nimport pytest from app import app as chalice_app @pytest.fixture def app(): return chalice_app 作成例）tests/test_app.py（テストの実装）\nfrom http import HTTPStatus import json def test_hello_get(client): response = client.get(\u0026#39;/hello\u0026#39;) assert response.status_code == HTTPStatus.OK assert response.json == {\u0026#39;hello\u0026#39;: \u0026#39;world\u0026#39;} def test_hello_post(client): headers = {\u0026#39;Content-type\u0026#39;:\u0026#39;application/json\u0026#39;} payload = {\u0026#39;payload\u0026#39;:\u0026#39;hello, world\u0026#39;} response = client.post(\u0026#39;/hello\u0026#39;, headers=headers, body=json.dumps(payload)) assert response.status_code == HTTPStatus.OK assert response.json == {\u0026#39;result\u0026#39;: \u0026#39;hello, world\u0026#39;} def test_hello_put(client): response = client.put(\u0026#39;/hello\u0026#39;) assert response.status_code == HTTPStatus.METHOD_NOT_ALLOWED assert response.json == {\u0026#34;Code\u0026#34;:\u0026#34;MethodNotAllowedError\u0026#34;,\u0026#34;Message\u0026#34;:\u0026#34;Unsupported method: PUT\u0026#34;} Step2. ユニットテストの実行 # ではテストプログラムが用意できたので pytest コマンドを使ってユニットテストを実行しましょう。実行例では、出力メッセージからユニットテスト 3 件が実行され、3 件とも成功 (PASSED) して、C1 カバレッジが 100% 網羅されていることがわかります。\n実行例）\n$ python3 -m pip install -r test_requirements.txt $ pytest -v --cov --cov-branch =========================== test session starts ============================ platform linux -- Python 3.7.10, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /home/cloudshell-user/venv37/bin/python3 cachedir: .pytest_cache rootdir: /home/cloudshell-user/\u0026lt;任意のプロジェクト名\u0026gt; plugins: cov-3.0.0, chalice-0.0.5 collected 3 items tests/test_app.py::test_hello_get PASSED [ 33%] tests/test_app.py::test_hello_post PASSED [ 66%] tests/test_app.py::test_hello_put PASSED [100%] ---------- coverage: platform linux, python 3.7.10-final-0 ----------- Name Stmts Miss Branch BrPart Cover ----------------------------------------------------- app.py 11 0 0 0 100% tests/__init__.py 0 0 0 0 100% tests/conftest.py 4 0 0 0 100% tests/test_app.py 16 0 0 0 100% ----------------------------------------------------- TOTAL 31 0 0 0 100% ============================ 3 passed in 0.05s ============================= 以上、ちょっとしたユニットテストの自動化のご紹介でした。\nCI/CD パイプラインを構築しよう # 次はもう一歩進んで、ソースコードの更新をトリガーに自動でテストを実行してデプロイまで実施する CI/CD パイプラインを構築し、実際に動かすところまで行ってみたいと思います。\nStep1. CI/CD パイプラインの作成 # AWS Chalice には CI/CD パイプライン用の CloudFormation(CFn) テンプレートを自動生成してくれるとても便利な chalice generate-pipeline コマンドがあります。今回の例では、先ほど作ったユニットテストもパイプラインの中で実行するようにしますので、buildspec.yml を分離する -b オプションも付与して実行します。\n実行例）\n$ chalice generate-pipeline -b buildspec.yml \u0026lt;テンプレートファイル名\u0026gt; 作成された CFn テンプレートは中身が大きいのでここには貼り付けませんが、おおよそ次のような AWS リソース作成が定義されています。\nAWS CodeCommit リポジトリ AWS CodeBuild プロジェクト AWS CodePipeline パイプライン Amazon Simple Storage Service(S3) バケット AWS IAM ロールおよびポリシー なお、今回はユニットテストの結果をレポート出力できるように作成されたテンプレートファイルを編集して CodeBuild に割り当てる権限を追加しました。\n作成例）CFnテンプレート（編集後）\n\u0026#34;CodeBuildPolicy\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;AWS::IAM::Policy\u0026#34;, \u0026#34;Properties\u0026#34;: { \u0026#34;PolicyName\u0026#34;: \u0026#34;CodeBuildPolicy\u0026#34;, \u0026#34;PolicyDocument\u0026#34;: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { + \u0026#34;Action\u0026#34;: [ + \u0026#34;codebuild:CreateReportGroup\u0026#34;, + \u0026#34;codebuild:CreateReport\u0026#34;, + \u0026#34;codebuild:UpdateReport\u0026#34;, + \u0026#34;codebuild:BatchPutTestCases\u0026#34;, + \u0026#34;codebuild:BatchPutCodeCoverages\u0026#34; + ], + \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, + \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; + }, { \u0026#34;Action\u0026#34;: [ では CFn テンプレートからリソースをデプロイしましょう。実行例のようにスタック作成の成功と出力されたら完了です。\n実行例）\n$ aws cloudformation deploy --stack-name \u0026lt;スタック名\u0026gt; --template-file \u0026lt;テンプレートファイル名\u0026gt; --capabilities CAPABILITY_IAM : Successfully created/updated stack - \u0026lt;スタック名\u0026gt; Step2. パイプラインジョブ定義の編集 # 次に chalice generate-pipeline で生成されたパイプラインジョブの定義を編集して、ユニットテストも自動で実行するようにしていきましょう。編集前のファイル内容はこちらです。\n作成例）buildspec.yml（編集前）\nartifacts: files: - transformed.yaml type: zip phases: install: commands: - sudo pip install --upgrade awscli - aws --version - sudo pip install \u0026#39;chalice\u0026gt;=1.26.0,\u0026lt;1.27.0\u0026#39; - sudo pip install -r requirements.txt - chalice package /tmp/packaged - aws cloudformation package --template-file /tmp/packaged/sam.json --s3-bucket ${APP_S3_BUCKET} --output-template-file transformed.yaml version: \u0026#39;0.1\u0026#39; 編集後のファイルは次のようにしてみました。記載順序の入れ替えやビルドフェーズの分割も併せて実施していてわかりにくくなっていますが、要約すると test_requirements.txt の読み込みと pytest コマンドの実行をステップとして追加しています。\n作成例）buildspec.yml（編集後）\nversion: 0.2 phases: install: commands: - python --version pre_build: commands: - sudo pip install --upgrade awscli pip - aws --version - sudo pip install -r requirements.txt -r test_requirements.txt build: commands: - pytest -v -junit-xml=test-result.xml --cov --cov-branch --cov-report=xml --cov-report=term post_build: commands: - chalice package /tmp/packaged - aws cloudformation package --template-file /tmp/packaged/sam.json --s3-bucket ${APP_S3_BUCKET} --output-template-file transformed.yaml reports: pytest_reports: files: - test-result.xml file-format: JUNITXML cobertura_reports: files: - coverage.xml file-format: COBERTURAXML artifacts: files: - transformed.yaml Step3. CI/CD パイプラインの動作確認 # 環境もパイプラインジョブの定義も整いましたので、あとは作成された CodeCommit リポジトリへソースコードを登録して、それをトリガーに自動でパイプラインが実行されるところまで見ていきましょう。\nまずはローカルに git リポジトリを作成して、ソースコードのコミットまでしていきましょう。今回の .gitignore ファイルは GitHub さんが公開する Python 向けテンプレートを使ってます。\n実行例）\n$ git init . $ curl https://raw.githubusercontent.com/github/gitignore/master/Python.gitignore \u0026gt; .gitignore $ git add . $ git commit -m \u0026#34;Initial commit\u0026#34; 次に、ソースコードを格納する AWS CodeCommit リポジトリの URL を確認します。\n実行例）\n$ aws cloudformation describe-stacks --stack-name \u0026lt;スタック名\u0026gt; --query \u0026#39;Stacks[0].Outputs\u0026#39; : - OutputKey: SourceRepoURL OutputValue: https://git-codecommit.ap-northeast-1.amazonaws.com/v1/repos/\u0026lt;プロジェクト名\u0026gt; : 先ほど調べたリポジトリ情報を元に、リモートリポジトリを追加します。\n実行例）\n$ git remote add codecommit https://git-codecommit.ap-northeast-1.amazonaws.com/v1/repos/\u0026lt;プロジェクト名\u0026gt; CodeCommit へ Push できるように認証情報ヘルパーを設定します。\n実行例）\n$ git config --global credential.helper \u0026#39;!aws codecommit credential-helper $@\u0026#39; $ git config --global credential.UseHttpPath true ではソースコードをリモートレポジトリへ push してみましょう。\n実行例）\n$ git push codecommit master さてここからは AWS マネジメントコンソールに移ります。先ほどの git push をトリガーにパイプラインが起動していることを確認するためまずは CodePipeline 画面へ。次のように CI/CD パイプラインが動作していることが確認できるかと思います。\nでは次に、ビルド処理に追加したユニットテストが動いているかを確認しましょう。Build ステージの CodeBuild アクションボックスの「成功しました」メッセージのすぐ下にある「詳細」ボタンをクリックして CodeBuild 画面へ進みましょう。次のようにビルドログからユニットテストが実行されていることが確認できるかと思います。\nまた、テストとカバレッジのレポートについてはレポートグループの方にも登録されていることが確認できるかと思います。\nでは CodePipeline 画面へ戻りましょう。Beta ステージで実施しているアプリケーションのデプロイにも成功していることがわかりますね。ExcecuteChangeSet アクションボックスの「詳細」ボタンをクリックし、CloudFormation 画面へ進みましょう。\nCloudFormation 画面では出力の一覧から EndpointURL 値を確認しましょう。\n最後に、期待するレスポンスが返ってくるか生成された EndpointURL に向けて HTTP リクエストを発行して確認しましょう。実行例では期待するレスポンスが返ってきてますね。\n実行例）\n$ curl https://\u0026lt;文字列\u0026gt;.execute-api.ap-northeast-1.amazonaws.com/api/hello {\u0026#34;hello\u0026#34;:\u0026#34;world\u0026#34;} $ curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;payload\u0026#34;:\u0026#34;hello, world\u0026#34;}\u0026#39; https://\u0026lt;文字列\u0026gt;.execute-api.ap-northeast-1.amazonaws.com/api/hello {\u0026#34;result\u0026#34;:\u0026#34;hello, world\u0026#34;} 本来はもっとパイプラインに条件分岐を持たせたりすると思いますが、ひとまずこれでソースコードのプッシュを契機にテストを走らせてからデプロイを行う、という最低限の流れを自動化することができました。\n以上、ちょっとした CI/CD パイプラインの作成でした。\nちゃんとしたアプリケーションを開発するには # 公式ドキュメントを活用しよう # ちゃんとしたアプリケーションにするにはもちろん色々と機能を実装しないといけませんよね。こんなときにいつも私がお世話になっているのが公式ドキュメントです。トピックごとに簡単なサンプルを交えた実装方法について記載がありますのでとても参考になるかと思います。\nサービス別資料を参考にしよう # もちろん AWS さんのサービス別資料もとても参考になります。こちらも目を通していただけると良いかと思います。\n余談ですが、、、 # 本記事執筆のきっかけとなった AWS Summit Online Japan 2021 で行った日立の講演についてもご紹介させてください。\nこの講演では、IoT やサーバレス技術を活用した異なるタイプ(データ分析系とアプリケーション開発系)の事例を 1 件ずつお話させていただきました。AWS Chalice については、2 つ目のアプリケーション開発系の事例にて本当に軽くですが触れておりますのでよろしければご参照いただけると幸いです。\nこの講演を通じて「ほほう、日立ってこんなこともしてたんだ」と多少なりとも良いイメージを抱いてもらえるきっかけになってくれると嬉しいなと思ってます σ(,,´∀ ｀,,)\n終わりに # AWS Chalice はいかがだったでしょうか？\nAWS には、AWS SAM (Serverless Application Model)といった別のサーバレスアプリケーション開発用フレームワークがありますが、こちらと比較すると機能がかなり限定されることもあって、今回ご紹介した AWS Chalice は多くの方にとって馴染みもなければ、なかなかに触れる機会も少ないのかもしれません。\nただ、実際に触ってみていただけばわかる通り、とても簡単・手軽にサーバレスアプリケーションを作ることができますので、迅速性を求められる PoC(概念実証)や PoV(価値実証)といった場面でとくに有用なのでは、と考えています。気になった方はぜひ触ってみて、そのお手軽さを実際に体験していただければと思います。\n以上、AWS 上でサーバレスアプリケーションを手軽に開発できるようにする「AWS Chalice」のご紹介でした。\nAWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。 ","date":"December 1, 2021","permalink":"/posts/2021/10/aws-chalice/","section":"記事一覧","summary":"みなさん、こんにちは。今回は「AWS Chalice」を活用したサーバレスアプリケーション開発についてのお話です。AWS Summit Online Japan 2021 の日立製作所の講演で軽く触れたこともあり、どこかで紹介したいと思っておりました。","title":"AWS Chaliceを使ってサーバレスアプリケーションを開発しよう"},{"content":"","date":"December 1, 2021","permalink":"/tags/aws-lambda/","section":"タグ一覧","summary":"","title":"AWS Lambda"},{"content":"","date":"November 25, 2021","permalink":"/tags/azure-functions/","section":"タグ一覧","summary":"","title":"Azure Functions"},{"content":"みなさん、こんにちは。今回は GitHub Enterprise Cloud(GHEC) の各種ログを SIEM1 マネージドサービスである Microsoft Sentinel (旧称 Azure Sentinel) に集約し、サイバー攻撃の兆候を検知できるようにする方法をご紹介していきたいと思います。\nMicrosoft Sentinel と GHEC との連携方法はいくつかあるのですが、この記事では Microsoft Sentinel コミュニティで開発している Azure Logic Apps(ロジックアプリ)、Azure Functions(関数アプリ)の 2 種類のカスタムデータコネクタの内、「Azure Functions コネクタ」を使った方法をご紹介していきたいと思います。\nこれから GHEC の利用を検討している方や、GHEC は利用しているけれど SIEM システムの導入まではしていないという方は、セキュリティ強化策のひとつとして参考にしてみてはいかがでしょうか。\n構築するシステムについて # 今回は Azure Functions(関数アプリ)を定期的に起動し、GHEC から監査ログなどを取得して Microsoft Sentinel ワークスペースへ格納、格納されたログに対して Microsoft Sentinel が自動的に相関分析をかけていく、といった流れで処理を行うシステムを構築していきたいと思います。\nAzure Sentinel とのコネクタについては、今回は Microsoft Sentinel コミュニティで公開されている次のカスタムデータコネクタを利用していきたいと思います。\n本カスタムコネクタで取得した各種ログデータについては、Log Analytics ワークスペースの次のカスタムテーブルへ格納されるようになります。\nテーブル名 説明 GitHub_CL 監査ログのデータを格納するテーブル GitHubRepoLogs_CL 各リポジトリに対するフォーク、クローン、コミットなどの操作ログやリポジトリに対するセキュリティ脆弱性診断ログのデータを格納するテーブル それでは構築していきましょう # 今回は GitHub Enterprise Cloud → Microsoft Sentinel ワークスペース → カスタムコネクタ → Microsoft Sentinel の順で設定していきましょう。\nStep1. GitHub Enterprise Cloud を設定しよう # ここではログを収集する先の GitHub Organization の作成と、Azure Functions から GitHub API へアクセスする際に利用するアクセストークンの作成を実施していきましょう。\n(1) Organization(Enterprise) の作成 # 監査ログなどを取得する対象の Organization を作成しましょう。今回は GitHub Enterprise Cloud とするため課金プランは「Enterprise」を選択します。\n(2) アクセストークンの作成 # 次に Azure Logic Apps から GitHub API へアクセスする際に利用するアクセストークンを作成します。Organization の Owner 権限を持つユーザで、Settings \u0026gt; Developer settings \u0026gt; Personal access tokens \u0026gt; Generate new tokens から作成しましょう。\n付与するスコープについては Microsoft Sentinel コミュニティでは言及されておりませんが、筆者の環境では必要最低限のスコープとして次の 5 つの権限を付与することで動作を確認することができました。\npublic_repo admin:org read:user user:email admin:enterprise 作成後はデータコネクタをデプロイする際に利用するためトークン情報をコピーしておきます。 トークン情報が漏れると大変なことになりますので絶対に漏らさないように注意しましょう。 Step2. Microsoft Sentinel ワークスペースを設定しよう # 次に Microsoft Sentinel ワークスペースの設定です。ここでは Log Analytics ワークスペースの作成および Log Analytics ワークスペースへの Microsoft Sentinel 機能の追加を実施していきましょう。\n(1) ワークスペースの作成 # Azure ポータルなどから Log Analytics ワークスペースの作成および作成したワークスペースへの Microsoft Sentinel 機能の追加を次の例のように実施していきましょう。\n(2) ワークスペース情報の取得 # Microsoft Sentinel ワークスペースの作成後は、データコネクタをデプロイする際に利用するため Log Analytics ワークスペース画面のエージェント管理などから「ワークスペース ID」と「主キー」をコピーしておきます。\nStep3. カスタムコネクタを設定しよう # ここでは GitHub 用カスタムデータコネクタを ARM テンプレートからデプロイして、対象 GitHub Organization からログデータを取得できるようにデータコネクタの諸設定まで実施していきましょう。\n(1) ARM テンプレートのデプロイ # それでは GitHub 用カスタムデータコネクタをデプロイしていきましょう。データコネクタの「Readme」に記載されている Deploy to Azureボタンをクリックします。\nAzure のカスタムテンプレートのデプロイ画面へ遷移したらパラメータ値を入力して作成をしましょう。\nパラメータ 説明 Personal Access Token GitHub のアクセストークンを指定する Workspace Id Microsoft Sentinel ワークスペースのワークスペース ID を指定する Workspace Key Microsoft Sentinel ワークスペースの主キーを指定する Function Schedule Azure Functions の起動スケジュールを「\u0026ldquo;秒\u0026rdquo; \u0026ldquo;分\u0026rdquo; \u0026ldquo;時\u0026rdquo; \u0026ldquo;日付\u0026rdquo; \u0026ldquo;月\u0026rdquo; \u0026ldquo;曜日\u0026rdquo;」で指定する(デフォルト 10 分間隔) デプロイにかかる時間は環境により誤差はあると思いますがおおよそ 3 分でした。デプロイが終わると Azure Functions 以外にもストレージアカウントや Azure Key Vault などのリソースも作成されていることがわかります。\n(2) 設定ファイルの作成 # データコネクタの 2 種類の設定ファイル ORGS.json と lastrun-Audit.json を作成します。各ファイルの概要およびフォーマットは次の通りです。\nファイル名 説明 ORGS.json ログ取得対象の GitHub Organization を定義するファイル lastrun-Audit.json 最終実行時刻を管理するファイル(新しいレコードのみを収集するために利用される) 作成例）ORGS.json\n[ { \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your 1st organization account name\u0026gt;\u0026#34; }, { \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your 2nd organization account name\u0026gt;\u0026#34; }, { \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your Nth organization account name\u0026gt;\u0026#34; } ] 作成例）lastrun-Audit.json\n[ { \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your 1st organization account name\u0026gt;\u0026#34; \u0026#34;lastContext\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;lastRun\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your 2nd organization account name\u0026gt;\u0026#34; \u0026#34;lastContext\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;lastRun\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your Nth organization account name\u0026gt;\u0026#34; \u0026#34;lastContext\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;lastRun\u0026#34;: \u0026#34;\u0026#34; } ] 各ファイルを編集し、org パラメータ値をログ取得対象の GitHub Organization アカウント名に置きかえます。フォーマットのように、複数の GitHub Organization を指定することもできますので、実際の GitHub Organization の数にあわせて増減させてください。\nなお、lastrun-Audit.json ファイルには org 以外のパラメータもありますが、これらはカスタムコネクタが動作すると自動的に値が更新されるため、ユーザ側で意識する必要はありません。\n(3) 設定ファイルの配置 # 2 種類のファイルを作成したら、ARM テンプレートから作成されたストレージアカウントの github-repo-logs コンテナにアップロードしましょう。\nここまででひととおりの設定は終わりましたので、あとはスケジュールに沿ってログの取得が行われるようになっていることを確認していきましょう。\n(4) 動作確認 # まずは Azure Functions から見ていきましょう。Azure Functions では、関数が正しくスケジュール実行されていること、実行された関数が正常終了していることを確認します。出力例のように実行回数と成功回数がカウントされていれば OK です。\n次に Log Analytics ワークスペースを確認していきましょう。ここではカスタムログに GitHub Enterprise Cloud から取得したログが格納されていることを確認します。出力例のように「GitHub_CL」と「GitHubRepoLogs_CL」の 2 種類のカスタムテーブルが作成されていれば OK です。\n最後に Microsoft Sentinel を確認してみましょう。出力例のように「GitHub_CL」および「GitHubRepoLogs_CL」からのイベントが記録されていれば OK です。\n以上でカスタムコネクタのデプロイは終わりです。お疲れ様でした。\n複数の異なるロケーションからのサインインバースト (※1) 新しい国からのアクティビティ TI インジケータに登録した IP アドレスからのアクセス (※2) 二要素認証の無効化 リポジトリ内にセキュリティ脆弱性 ※1: Azure Active Directory(Azure AD) とのシングルサインオン(SSO)設定、Azure AD ログの収集が必要\n※2: Microsoft Sentinel の脅威インテリジェンス設定にて TI インジケータの事前登録が必要\n(1) データの解析および正規化 # まずは GitHub_CL テーブルおよび GitHubRepoLogs_CL テーブルに格納されているデータを、Microsoft Sentinel で分析しやすいように加工をしていきましょう。なお、Microsoft Sentinel コミュニティにてデータ加工用のパーサ関数(GitHubAudit 関数、GitHubRepo 関数)が公開されておりますので、今回はこちらを利用していきましょう。\n作成例）GitHubAudit関数\nGitHub_CL | project TimeGenerated=node_createdAt_t, Organization=columnifexists(\u0026#39;node_organizationName_s\u0026#39;, \u0026#34;\u0026#34;), Action=node_action_s, OperationType=node_operationType_s, Repository=columnifexists(\u0026#39;node_repositoryName_s\u0026#39;,\u0026#34;\u0026#34;), Actor=columnifexists(\u0026#39;node_actorLogin_s\u0026#39;, \u0026#34;\u0026#34;), IPaddress=columnifexists(\u0026#39;node_actorIp_s\u0026#39;, \u0026#34;\u0026#34;), City=columnifexists(\u0026#39;node_actorLocation_city_s\u0026#39;, \u0026#34;\u0026#34;), Country=columnifexists(\u0026#39;node_actorLocation_country_s\u0026#39;, \u0026#34;\u0026#34;), ImpactedUser=columnifexists(\u0026#39;node_userLogin_s\u0026#39;, \u0026#34;\u0026#34;), ImpactedUserEmail=columnifexists(\u0026#39;node_user_email_s\u0026#39;, \u0026#34;\u0026#34;), InvitedUserPermission=columnifexists(\u0026#39;node_permission_s\u0026#39;, \u0026#34;\u0026#34;), Visibility=columnifexists(\u0026#39;node_visibility_s\u0026#39;, \u0026#34;\u0026#34;), OauthApplication=columnifexists(\u0026#39;node_oauthApplicationName_s\u0026#39;, \u0026#34;\u0026#34;), OauthApplicationUrl=columnifexists(\u0026#39;node_applicationUrl_s\u0026#39;, \u0026#34;\u0026#34;), OauthApplicationState=columnifexists(\u0026#39;node_state_s\u0026#39;, \u0026#34;\u0026#34;), UserCanInviteCollaborators=columnifexists(\u0026#39;node_canInviteOutsideCollaboratorsToRepositories_b\u0026#39;, \u0026#34;\u0026#34;), MembershipType=columnifexists(\u0026#39;node_membershipTypes_s\u0026#39;, \u0026#34;\u0026#34;), CurrentPermission=columnifexists(\u0026#39;node_permission_s\u0026#39;, \u0026#34;\u0026#34;), PreviousPermission=columnifexists(\u0026#39;node_permissionWas_s\u0026#39;, \u0026#34;\u0026#34;), TeamName=columnifexists(\u0026#39;node_teamName_s\u0026#39;, \u0026#34;\u0026#34;), Reason=columnifexists(\u0026#39;node_reason_s\u0026#39;, \u0026#34;\u0026#34;), BlockedUser=columnifexists(\u0026#39;node_blockedUserName_s\u0026#39;, \u0026#34;\u0026#34;), CanCreateRepositories=columnifexists(\u0026#39;canCreateRepositories_b\u0026#39;, \u0026#34;\u0026#34;) 作成例）GitHubRepo関数\nGitHubRepoLogs_CL | project TimeGenerated = columnifexists(\u0026#39;DateTime_t\u0026#39;, \u0026#34;\u0026#34;), Organization=columnifexists(\u0026#39;Organization_s\u0026#39;, \u0026#34;\u0026#34;), Repository=columnifexists(\u0026#39;Repository_s\u0026#39;,\u0026#34;\u0026#34;), Action=columnifexists(\u0026#39;LogType_s\u0026#39;,\u0026#34;\u0026#34;), Actor=coalesce(login_s, owner_login_s), ActorType=coalesce(owner_type_s, type_s), IsPrivate=columnifexists(\u0026#39;private_b\u0026#39;,\u0026#34;\u0026#34;), ForksUrl=columnifexists(\u0026#39;forks_url_s\u0026#39;,\u0026#34;\u0026#34;), PushedAt=columnifexists(\u0026#39;pushed_at_t\u0026#39;,\u0026#34;\u0026#34;), IsDisabled=columnifexists(\u0026#39;disabled_b\u0026#39;,\u0026#34;\u0026#34;), AdminPermissions=columnifexists(\u0026#39;permissions_admin_b\u0026#39;,\u0026#34;\u0026#34;), PushPermissions=columnifexists(\u0026#39;permissions_push_b\u0026#39;,\u0026#34;\u0026#34;), PullPermissions=columnifexists(\u0026#39;permissions_pull_b\u0026#39;,\u0026#34;\u0026#34;), ForkCount=columnifexists(\u0026#39;forks_count_d\u0026#39;,\u0026#34;\u0026#34;), Count=columnifexists(\u0026#39;count_d,\u0026#39;,\u0026#34;\u0026#34;), UniqueUsersCount=columnifexists(\u0026#39;uniques_d\u0026#39;,\u0026#34;\u0026#34;), DismmisedAt=columnifexists(\u0026#39;dismissedAt_t\u0026#39;,\u0026#34;\u0026#34;), Reason=columnifexists(\u0026#39;dismissReason_s\u0026#39;,\u0026#34;\u0026#34;), vulnerableManifestFilename = columnifexists(\u0026#39;vulnerableManifestFilename_s\u0026#39;,\u0026#34;\u0026#34;), Description=columnifexists(\u0026#39;securityAdvisory_description_s\u0026#39;,\u0026#34;\u0026#34;), Link=columnifexists(\u0026#39;securityAdvisory_permalink_s\u0026#39;,\u0026#34;\u0026#34;), PublishedAt=columnifexists(\u0026#39;securityAdvisory_publishedAt_t \u0026#39;,\u0026#34;\u0026#34;), Severity=columnifexists(\u0026#39;securityAdvisory_severity_s\u0026#39;,\u0026#34;\u0026#34;), Summary=columnifexists(\u0026#39;securityAdvisory_summary_s\u0026#39;,\u0026#34;\u0026#34;) 次の画像を参考に Microsoft Sentinel のログ画面から 2 つのパーサ関数を登録しましょう。なお、「従来のカテゴリ」の値については任意の文字列で大丈夫です。\n(2) 分析ルールの追加 # 次に GitHub 固有の脅威を検出できるように分析ルールをテンプレートから追加していきしょう。Microsoft Sentinel では GitHub 固有の脅威に対する分析ルールのテンプレートが用意されていますので、今回はこちらを活用して分析ルールの追加をしてきたいと思います。出力例のように検索キーワードに「github」と入力することで目的のテンプレートを見つけることができます。\nなお、2021 年 11 月時点で用意されているテンプレートは次の 6 種類となります。\n| No. | テンプレート名 | 説明 | | 3 | Brute Force Attack against GitHub Account | GitHub アカウントへのブルートフォース攻撃を検知するルール、デフォルトでは 1 日ごとにクエリを実行 | | 5 | TI map IP entity to GitHub_CL | TI インジケータに登録した IP アドレスからのアクセスを検知するルール、デフォルトでは 1 時間ごとにクエリを実行 | | 6 | GitHub Security Vulnerability in Repository | リポジトリ内にセキュリティ脆弱性が含まれていることを検知するルール、デフォルトでは 1 時間ごとにクエリを実行 |\nそれではテンプレートを用いた分析ルールの追加をしていきましょう。まず追加したいルールを選択し、「ルールの作成」ボタンをクリックします。\nルールの追加画面ではウィザードにしたがってルールの作成を行っていきます。各パラメータにはテンプレートによってデフォルト値が入力されていますのでとくに値を変更する要件がなければそのまま作成していきましょう。\n分析ルールが追加されたテンプレートについては次の出力例のように「使用中」アイコンが付与されます。先ほどと同様の手順を繰り返し、他のテンプレートに対しても分析ルールの追加をしていきましょう。出力例のように 6 種類すべてに使用中アイコンが付与されれば脅威を自動検知するルールの設定も終わりです。お疲れ様でした。\n補足、もう一歩踏み込んだ脅威分析を行うには # Microsoft Sentinel では今回紹介した標準の分析ルールを利用する以外に、カスタム分析ルールを自作してより高度なイベントの検知をすることが可能です。サンプルとして次のようなカスタム分析ルールが Microsoft 技術ブログでも紹介されていますのでぜひ参考にしてみてください。\nリポジトリに対する異常数のクローン操作 リポジトリの一括削除 リポジトリをプライベートからパブリックに変更 リポジトリに対する部外者からのフォーク操作 GitHub Organization へのユーザ招待およびユーザ追加 ユーザへのアクセス許可の追加付与　など 終わりに # 今回は GitHub Enterprise Cloud の各種ログを SIEM マネージドサービスである Microsoft Sentinel (旧称 Azure Sentinel) に集約し、サイバー攻撃の兆候を検知できるようにする方法のご紹介でしたがいかがだったでしょうか？\nGitHub Enterprise Cloud に限らずさまざまな監査ログを収集し、長期保存されている方は多いと思います。ただ、せっかく取得するのであればただ集めて長期保存しておくだけではなく、Microsoft Sentinel を活用してもう一歩進んだセキュアな環境を実現してみる、というのも \u0026ldquo;有り\u0026rdquo; なのではないでしょうか。\n以上、「Microsoft Sentinel を使って GitHub Enterprise Cloud のセキュリティを強化しよう (Azure Functions コネクタ編)」でした。\nMicrosoft Azure は，Microsoft Corporation の商標または登録商標です。 GitHub は、GitHub Inc. の商標または登録商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。 SIEM(Security Information and Event Management) は、さまざまなログを一元的に集約、相関分析をしてサイバー攻撃などの異常を自動的に検出するソリューションです。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"November 25, 2021","permalink":"/posts/2021/11/azure-sentinel-functions-data-connector-for-ghec/","section":"記事一覧","summary":"みなさん、こんにちは。今回は GitHub Enterprise Cloud(GHEC) の各種ログを SIEM1 マネージドサービスである Microsoft Sentinel (旧称 Azure Sentinel) に集約し、サイバー攻撃の兆候を検知できるようにする方法をご紹介していきたいと思います。","title":"Microsoft Sentinel を使って GitHub Enterprise Cloud のセキュリティを強化しよう (Azure Functions コネクタ編)"},{"content":"今更ですが Microsoft Build 2021 にて AKS(Azure Kubernetes Service) on Azure Stack HCI の一般提供が発表されておりました。\nこの機能によって Kubernetes のコントロールプレーンは Azure マネージドで提供されつつも、ワークロードはオンプレの Azure Stack HCI 上で動かせるようになります。\nそのため、PoC などは Azure 上の AKS でフットワーク軽くやって、でも本番はオンプレの AKS on Azure Stack HCI でカッチリ、っていうシナリオも描けるようになってきた感じでしょうか。Azure Stack HCI が世に出てから2年越し(?)でやっと単なるサーバ製品じゃなく、Azure を冠した意味/メリットがでてきてくれそうですね。\n","date":"July 1, 2021","permalink":"/posts/2021/07/azure-aks-on-hci-entered-ga/","section":"記事一覧","summary":"今更ですが Microsoft Build 2021 にて AKS(Azure Kubernetes Service) on Azure Stack HCI の一般提供が発表されておりました。","title":"AKS on Azure Stack HCIの一般提供(GA)が開始しました"},{"content":"","date":"July 1, 2021","permalink":"/tags/azure-kubernetes-serviceaks/","section":"タグ一覧","summary":"","title":"Azure Kubernetes Service(AKS)"},{"content":"","date":"July 1, 2021","permalink":"/tags/azure-stack-hci/","section":"タグ一覧","summary":"","title":"Azure Stack HCI"},{"content":"Microsoft Certified Azure Data Scientist Associate (DP-100) を 2021/5/28 に受験していたのでその時のメモです。ほんとは違う試験を受けに行く予定だったので情報少ないです。すんません。\n受験メモ # 受験者情報 # AWS、Azure、Google Cloudの三大クラウド上級資格持ち、とくにAWS資格は全種コンプしているので一見するとけっこう良さげな(クラウド歴1年の)新米クラウドエンジニアです。\nNo. 項目 取得数 取得済み資格/合格済み試験 1 AWS資格 12 CLF、SAA、SOA、DVA、SAP、DOP、\nSCS、ANS、AXS、DBS、DAS、MLS 2 Azure資格 4 AZ-900、AZ-104、AZ-303/304、AZ-400 3 Google Cloud資格 3 ACE、PCA、PDE 結果 # 781 点 (合格) 試験対策例 # 合格者のブログを漁って勉強の計画などを立てる (うっかりで 0 時間) 試験のアウトラインを読んで不安な箇所がないかを確認する (うっかりで 0 時間) 自信のない部分を Microsoft Learn などでお勉強する (うっかりで 0 時間) Udemy などで評価の高い問題集をやる (うっかりで 0 時間) 感想 # Azure Machine Learning の知識が無くても答えられる設問が多かった印象でした。たとえば、Python のコードが出てきて実装コードの穴あきを埋めよとか、トレーニングパイプライン/推論パイプラインの穴あきを埋めよとか、xx のケースだとモデルデプロイ先のアーキテクチャとして最適なものはどれか？みたいな。いやほんと、Azure 以外でも通用するような一般知識を問われるケースが多かったです。 アドバイスなど # DP-100 は、Azure Machine Learning に特化した試験なのではっきり言って範囲は広くないです。ワークスペースをどうやってセットアップするか、セットアップ後はどうやってモデルのトレーニングを始めるのか、モデルの精度がイマイチだった場合にどういう手段が取れるのか、トレーニングが終わったモデルを実際にアプリから呼び出して使う際はどうするか、とかいった一連の流れを問われます。ここら辺の分野がまったくの素人ですって方は、時間効率は悪いですが MS Learn でもひととおり学べるので目を通しておくと良いでしょう。 もし仮に勉強する時間が取れていたら、Udemy で評価の高い英語の問題集を購入してやっていたかと思います。Solutions Architect とかの人気試験ならまだしも、この手の試験は日本語で質の良い問題集はまずないと思った方がいいですし。 最後に # 試験を申し込むときは似たような名前の試験があるので最新の注意を払いましょう！\n試験を申し込むときは似たような名前の試験があるので最新の注意を払いましょう！\n重要なことなので 2 回書きました。本当は私も Power BI の試験 (D A -100) を申し込んだつもりだったんですよね、、、試験を開始して 5 問くらいやったあたりで「はわわわ、こいつはおかしい！Azure Machine Learning に関する問題しか出てこないんですけどーーーー！」と内心めちゃくちゃ焦ってました。でも、なんとか合格できたのでラッキーでしたね^^;\nということで、Microsoft Certified Azure Data Scientist Associate (DP-100) を受験したときのお話でした。\n","date":"June 5, 2021","permalink":"/posts/2021/06/azure-dp-100-exam/","section":"記事一覧","summary":"Microsoft Certified Azure Data Scientist Associate (DP-100) を 2021/5/28 に受験していたのでその時のメモです。ほんとは違う試験を受けに行く予定だったので情報少ないです。すんません。","title":"Microsoft Certified Azure Data Scientist Associate (DP-100) 受験メモ"},{"content":"Microsoft Certified Azure Developer Associate (AZ-204) を 2021/5/28 に受験してきました。その時のメモです。\n受験メモ # 受験者情報 # AWS、Azure、Google Cloudの三大クラウド上級資格持ち、とくにAWS資格は全種コンプしているので一見するとけっこう良さげな(クラウド歴1年の)新米クラウドエンジニアです。\nNo. 項目 取得数 取得済み資格/合格済み試験 1 AWS資格 12 CLF、SAA、SOA、DVA、SAP、DOP、\nSCS、ANS、AXS、DBS、DAS、MLS 2 Azure資格 4 AZ-900、AZ-104、AZ-303/304、AZ-400 3 Google Cloud資格 3 ACE、PCA、PDE 結果 # 715点 (合格)\n試験対策例 # No. 対策項目 実施時間(例) 1 試験のアウトラインを読む 約0.5時間 アドバイスなど # 上級2種持っているし、大丈夫だろうとタカをくくって受けに行くと痛い目を見る可能性が高いと思います。Azureの上級資格とはまったく違う知識、べスプラを問うような考えれば答えが出る問題ではなく、単純な知識問題、知っているか知らないかを要求される印象でした。まずはガイドラインを見て不安要素がないかを確認しましょう。\nあと、試験の最初にC#かVisual Basicを選べと2択を迫られるので「おっと、この後全部C#か、、、」と思ったら別の言語も普通に出てきて、正直どっちを選んだとしても大して差はなかったのかもしれませんね。あれはなんだったんでしょうか、、、^^;\n終わりに # うっかりノーガードで行ってしまったため、AWSだと余裕で答えられるメッセージングサービスの違いや使い分けが、Azureのサービスに置き換えると何ていうサービスになるのかが答えられない、という歯がゆい状態でした。\nMS Learnの学習コンテンツやUdemyなどで評価の良さそうな問題集を活用して、ここら辺を余裕で答えられるようになっていればもう少し余裕を持った点数になったのかもなと思ってます。とはいえ、省エネで合格できてラッキーでした。\n","date":"June 5, 2021","permalink":"/posts/2021/06/azure-az-204-exam/","section":"記事一覧","summary":"Microsoft Certified Azure Developer Associate (AZ-204) を 2021/5/28 に受験してきました。その時のメモです。","title":"Microsoft Certified Azure Developer Associate (AZ-204) 受験メモ"},{"content":"Microsoft Certified Azure Security Engineer Associate (AZ-500) を 2021/5/28 に受験してきました。その時のメモです。\n受験メモ # 受験者情報 # AWS、Azure、Google Cloudの三大クラウド上級資格持ち、とくにAWS資格は全種コンプしているので一見するとけっこう良さげな(クラウド歴1年の)新米クラウドエンジニアです。\nNo. 項目 取得数 取得済み資格/合格済み試験 1 AWS資格 12 CLF、SAA、SOA、DVA、SAP、DOP、\nSCS、ANS、AXS、DBS、DAS、MLS 2 Azure資格 4 AZ-900、AZ-104、AZ-303/304、AZ-400 3 Google Cloud資格 3 ACE、PCA、PDE 結果 # 647点 (不合格)\n試験対策例 # No. 対策項目 実施時間(例) 1 試験のアウトラインを読む 約0.5時間 アドバイスなど # 上級2種持っているし、大丈夫だろうとタカをくくって受けに行ったら痛い目を見たクチです。Azureの上級資格とはまったく違う知識、たとえば、ポリシー定義の穴埋めをさせるなどの細かいところを聞かれます。知っていれば楽勝でしょうが、知らないとパラメータの名前から推測＝勘となってしまう問題が多かったです。\n不合格しといて言うのもあれですが、出題パターンは限られていそうなので問題集を何か買ってやりこなせば余裕で突破できそうな気がします。\n終わりに # 最近は試験準備をどこまで手抜きできるかのチキンレース状態で、今回もうっかりノーガードで行ってしまい、とうとう痛い目を見てしまった感じですね^^; 次回はあまり手を抜きすぎず、Udemyの問題集か何かしら対策をしてからリベンジをしに行きたいと思います。\n","date":"June 5, 2021","permalink":"/posts/2021/06/azure-az-500-exam/","section":"記事一覧","summary":"Microsoft Certified Azure Security Engineer Associate (AZ-500) を 2021/5/28 に受験してきました。その時のメモです。","title":"Microsoft Certified Azure Security Engineer Associate (AZ-500) 受験メモ"},{"content":"","date":"June 5, 2021","permalink":"/tags/%E8%B3%87%E6%A0%BC/","section":"タグ一覧","summary":"","title":"資格"},{"content":"","date":"May 29, 2021","permalink":"/tags/aws-saas-boost/","section":"タグ一覧","summary":"","title":"AWS SaaS Boost"},{"content":"みなさん、こんにちは。ちょっと前に AWS さんのブログを漁ってたら「AWS SaaS Boost がオープンソースとしてリリースされました」っていう記事が目に入ってきたんですよね。\n普通は AWS SaaS Boost をサービスとして利用すればいいだけですし、へー、コードが公開されたんだ、でスルーしようとも思ったんですが、なんとなく気になったんです。もしかするとリポジトリをフォークして魔改造して使うときが来るかもしれない。いや、来ないだろうけど、、、\nということで、そんな時に備えるためちゃちゃっと改造して、改造したものを動かしてみるところまで試せたらいいなーと思って筆をとってみた次第です。\nそもそも \u0026ldquo;AWS SaaS Boost\u0026rdquo; って何者よ？ # AWS SaaS Boost というのは、自分の持っているアプリケーションを SaaS 化したいなー！というときに強い味方となってくれるサービスです。\nAWS さんのブログに書いてある内容をそのまま載せちゃいますが、要は、こちらのサービスが SaaS 化する際に作りこまないといけない機能を補完してくれる ってことなんですね。\n既存のアプリケーションを SaaS Boost にインストールするだけで、テナント管理、デプロイ、テナントごとの分析、ビリング（請求）、メータリングがすべてセットアップされ、すぐに使用できるようになります。これにより、ソリューションを再構築するコストをかけることなく、より迅速に、製品を SaaS モデルで市場に提供することが可能になります。\nということでユーザは、よりアプリのコア機能開発に時間を割くことができるようになる ってことなので、使うメリットは十分にあるんじゃなかろうかと思います。\nまずはオリジナルのまま環境を作ってみる # ん？改造するんじゃないの？と思った方は正解！でも、改造する前に絶対やっておくべきプロセスです。とくに若いオープンソースにあるあるだと思いますけど、「手順通りにやったけど動かないぜーー！」ということがあります。ということで、改造云々の前に導入するにあたって追加の前提条件や手順がないのかを確認しておきたいと思います。\nということで、こちらのドキュメントに基本沿って作っていきたいと思います。ちなみに、この記事の Step とドキュメント上の Step の数字は一致するように書いてます。題名は若干違いますが、、、\nStep0: 環境の準備 # 今回は Amazon Linux 2 の EC2 インスタンスを用意しました。メモリは 4GB 以上必要なようです。\nStep1: 前提パッケージの導入 # 公式ドキュメントにリスト掲載されているパッケージをインストールしていきましょう。\nJava 11 Amazon Corretto 11 Apache Maven AWS Command Line Interface version 2 Git Node 14.15 (LTS) Yarn 公式ドキュメントにはコマンドラインまでは書かれてませんけど Amazon Linux 2 だと下の例のような感じです。補足ですが、筆者の環境だと OpenJDK 8 がデフォルトの java のバージョンになってたので、必要に応じてコマンド例のように java バージョンを切り替えて使ってください。永続化はしてないので注意。\n実行例）Amazon Linux 2 の場合\n$ sudo yum update -y $ sudo yum install -y java-11-amazon-corretto maven $ sudo alternatives --config java $ export JAVA_HOME=/usr/lib/jvm/java-11-amazon-corretto.x86_64/ $ curl --silent --location https://rpm.nodesource.com/setup_14.x | sudo bash - $ sudo yum install -y nodejs $ curl --silent --location https://dl.yarnpkg.com/rpm/yarn.repo | sudo tee /etc/yum.repos.d/yarn.repo $ sudo yum install -y yarn AWS CLI v2 は最初から入っている環境であったため手順を省いてしまいましたが、もし自分で導入される際はこちらをご参照くださいね。\nStep2: ソースコードを取得 # AWS SaaS Boost のソースコードを GitHub からダウンロードします。 ここは公式ドキュメントにコマンドラインが書かれているのでそのまま実行すれば OK なんですが、今回はオリジナルからフォークしてきた個人のリポジトリを使うようにしましたよ、と。\n実行例）Amazon Linux 2 の場合\n$ git clone https://github.com/chacco38/aws-saas-boost.git ./aws-saas-boost Step3: アプリをビルド＆デプロイ # AWS CLI の設定をした上で対話型インストーラを用いて AWS SaaS Boost を導入していきましょう、、、って、このタイミングでシステム要件出してくるのか、まぁいいけどｗ\nThe system where you run the installation should have at least 4 GB of memory.\n気を取り直して、コマンドを実行していきましょう。こんな感じです。ちなみに、出力形式を「YAML」にすると install.sh がエラーになったので「JSON」に設定してます。\n実行例）Amazon Linux 2 の場合\n$ aws configure $ aws configure set output json $ cd aws-saas-boost $ sh -x install.sh install.sh スクリプトの処理が進むとインストーラ起動オプションの入力を迫られます。今回は「1」を選択。\n出力例）インストーラ起動オプション入力時\n=========================================================== Welcome to the AWS SaaS Boost Installer Setting version to v0 as it is missing from the git properties file. Installer Version: d64277c-dirty, Commit time: 2021-05-19T21:15:02+0000 Checking maven, yarn and AWS CLI... Environment Checks for maven, yarn, and AWS CLI PASSED. =========================================================== 1. New AWS SaaS Boost install. 2. Install Metrics and Analytics in to existing AWS SaaS Boost deployment. 3. Update Web Application for existing AWS SaaS Boost deployment. 4. Update existing AWS SaaS Boost deployment. 5. Delete existing AWS SaaS Boost deployment. 6. Exit installer. Please select an option to continue (1-6): 1 次に各パラメータの入力を求められるので適当に設定していき、、、\n出力例）パラメータ入力時\nDirectory path of saas-boost download (Press Enter for \u0026#39;/home/ssm-user/aws-saas-boost\u0026#39;) : ***** Enter name of the AWS SaaS Boost environment to deploy (Ex. dev, test, uat, prod, etc.): ***** Enter the email address for your AWS SaaS Boost administrator: ***** Enter the email address address again to confirm: ***** Would you like to setup a domain in Route 53 as a Hosted Zone for the AWS SaaS Boost environment (y or n)? ***** Would you like to install the metrics and analytics module of AWS SaaS Boost (y or n)? ***** If your application requires a FSX for Windows Filesystem, an Active Directory is required. Would you like to provision a Managed Active Directory to use with FSX for Windows Filesystem (y or n)? ***** 最後にパラメータの確認をしてから「continue」を意味する「y」を入力したら、あとは何もせずに完了を待つだけです。筆者の環境だと完了まで 15 分弱かかりました。\n出力例）入力パラメータ確認時\nWould you like to continue the installation with the following options? AWS SaaS Boost Environment Name: ***** Admin Email Address: ***** Route 53 Domain for AWS SaaS Boost environment: ***** Install Metrics and Analytics: ***** Amazon Quicksight user for setup of Metrics and Analytics: ***** Setup Active Directory for FSX for Windows: ***** Enter y to continue or n to cancel: y install.sh スクリプトが完了したらこちらの EC2 インスタンス上での操作はいったん終了のようです。ということで、管理者 Email アドレスに新しいメールが届いていると思うのでメールボックスを見に行きましょう。\nStep4: AWS SaaS Boost への初回ログイン # ここからは Web ブラウザを中心に作業を行っていきます。本ステップでは管理者へ届いたメール本文に書かれている URL へアクセスする、ログインする、そしてパスワード変える、それだけです。入れたことを確認したら終わりです。\n公式ドキュメントでは Step5 以降で AWS SaaS Boost アプリ上の設定をしていくわけですが、今回は導入までの流れを確認したかっただけなので以降の手順は割愛します。\n余談、導入成功までの過程で出たエラー # ということで、今回導入するにあたって出てきたエラー 2 件のご紹介をしたいと思います。\nCase1: 「Error with build of Java installer for SaaS Boost」というエラーメッセージを出力してスクリプト停止 # Step1 で必要なパッケージをインストールだけしてスクリプトを実行したらこんな感じでエラーになったんです。mvn コマンドがエラーになったように見えるんだけど mvn はなんもエラーはいてないぞ、と。\n実行例）\n$ sh -x install.sh (省略) + cd /home/ssm-user/aws-saas-boost/installer + echo \u0026#39;Build installer jar with maven\u0026#39; Build installer jar with maven + mvn + echo \u0026#39;Error with build of Java installer for SaaS Boost\u0026#39; Error with build of Java installer for SaaS Boost + exit 2 そこでスクリプトの中身を見てみたんです。すると、mvn の標準出力も標準エラー出力も/dev/null にリダイレクトしてましたよ、と。\n実行例）\n$ less install.sh (/mvn) (n) if ! mvn \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 ; then echo \u0026#34;Error with build of Java installer for SaaS Boost\u0026#34; exit 2 fi ということで、mvn コマンドが何を出力するのかを見たくて手動で実行しましたよ、と。 以下のようにちゃんとメッセージでてくれました。「javac: invalid target release: 11」か、、、Java のターゲットリリースが 11 じゃないと怒られてた^^;\n実行例）\n$ cd /home/ssm-user/aws-saas-boost/installer $ mvn (省略) [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.6.0:compile (default-compile) on project SaaSBoostInstall: Compilation failure [ERROR] javac: invalid target release: 11 [ERROR] Usage: javac \u0026lt;options\u0026gt; \u0026lt;source files\u0026gt; [ERROR] use -help for a list of possible options [ERROR] -\u0026gt; [Help 1] [ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch. [ERROR] Re-run Maven using the -X switch to enable full debug logging. [ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles: [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException 確認すると、たしかに OpenJDK のバージョン 8 が設定されておりました。凡ミス。\n実行例）\n$ java -version openjdk version \u0026#34;1.8.0_282\u0026#34; $ mvn -version Java version: 1.8.0_282, vendor: Red Hat, Inc. ということで、Corretto のバージョン 11 で動くように設定して、再度 mvn を実行すると成功しましたよ、と。\n実行例）\n$ sudo alternatives --config java $ java -version openjdk version \u0026#34;11.0.11\u0026#34; 2021-04-20 LTS $ export JAVA_HOME=/usr/lib/jvm/java-11-amazon-corretto.x86_64/ $ mvn -version Java version: 11.0.11, vendor: Amazon.com Inc. $ mvn ということで install.sh から実行してもちゃんとビルドは通りました。ちゃんちゃん。\nCase2: 「Could not execute \u0026lsquo;aws sts get-caller-identity\u0026rsquo;, please check AWS CLI configuration.」というエラーメッセージを出力してスクリプト停止 # でも、すぐに次のエラーはやってきんです。「aws sts get-caller-identity」の実行ができなかったぞ、と。\n実行例）\n$ sh -x install.sh ： + echo \u0026#39;Launch Java Installer for SaaS Boost\u0026#39;Launch Java Installer for SaaS Boost + java -Djava.util.logging.config.file=logging.properties -jar /home/ssm-user/aws-saas-boost/installer/target/SaaSBoostInstall-1.0.0-shaded.jar WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance. Setting version to v0 as it is missing from the git properties file. =========================================================== Welcome to the AWS SaaS Boost Installer Setting version to v0 as it is missing from the git properties file. Installer Version: d64277c-dirty, Commit time: 2021-05-19T21:15:02+0000 Checking maven, yarn and AWS CLI... Could not execute \u0026#39;aws sts get-caller-identity\u0026#39;, please check AWS CLI configuration. 実行できなかったってどういう意味なのかさっぱり分からないので、まずは手動で実行してみるとあっさり原因判明。YAML という知らない出力タイプが設定されているって言われてました、、、なるほど aws configure の設定値が悪いのか。\n実行例）\n$ aws sts get-caller-identity Unknown output type: yaml ということで出力フォーマットを JSON 形式にして実行すると、ちゃんとインストーラのメニュー表示まで進んでくれました。ちゃんちゃん。\n実行例）\n$ aws configure set output json $ sh install.sh ： =========================================================== Welcome to the AWS SaaS Boost Installer Setting version to v0 as it is missing from the git properties file. Installer Version: d64277c-dirty, Commit time: 2021-05-19T21:15:02+0000 Checking maven, yarn and AWS CLI... Environment Checks for maven, yarn, and AWS CLI PASSED. =========================================================== 1. New AWS SaaS Boost install. 2. Install Metrics and Analytics in to existing AWS SaaS Boost deployment. 3. Update Web Application for existing AWS SaaS Boost deployment. 4. Update existing AWS SaaS Boost deployment. 5. Delete existing AWS SaaS Boost deployment. 6. Exit installer. Please select an option to continue (1-6): ということでエラー集のご紹介でした。\nでは改造して使ってみよう # なんですが、もうオリジナルを導入しただけおなかいっぱいだな、、、ってことで、またどこか時間が取れるときにでも更新しようと思います^^;\n(まぁ後回しにした場合は大抵やらないんだけどね、、、)\n終わりに # ということで、今回は「AWS SaaS Boost がオープンソースとして公開されたのでさっそくいじってみよう（と思っただけ）」でした。\n、、、えっ！？タイトルが違う？まぁいいじゃないかｗ\n","date":"May 29, 2021","permalink":"/posts/2021/05/aws-saas-boost-installation-steps/","section":"記事一覧","summary":"みなさん、こんにちは。ちょっと前に AWS さんのブログを漁ってたら「AWS SaaS Boost がオープンソースとしてリリースされました」っていう記事が目に入ってきたんですよね。","title":"AWS SaaS Boost がオープンソースとして公開されたのでさっそくいじってみよう"},{"content":"開始から少し時間が経ってしまいましたが、2021年7月いっぱいまでの期間限定かつ自宅からのリモート受験限定で、2回目の再受験が無料になるキャンペーンが開催されています。\nもちろん、来るかもわからない2回目の試験に保険をかけるの？っていう方もいるとは思いますが、ひとまず「普段は会場派という方も一考の余地あり」かとは思います。\n","date":"May 28, 2021","permalink":"/posts/2021/05/aws-exam-free-retry-campaign/","section":"記事一覧","summary":"開始から少し時間が経ってしまいましたが、2021年7月いっぱいまでの期間限定かつ自宅からのリモート受験限定で、2回目の再受験が無料になるキャンペーンが開催されています。","title":"AWS 認定試験の1回目が不合格でも2回目の再受験が無料になるキャンペーンが開催中です"},{"content":"","date":"May 19, 2021","permalink":"/tags/azure-network-security-groupnsg/","section":"タグ一覧","summary":"","title":"Azure Network Security Group(NSG)"},{"content":"最近も出会ってしまった。こんな誤解。\nセキュリティグループ、正確に言えば Network Security Groups(NSG)。AWS のセキュリティグループと同じで、ネットワークインターフェースに割り当てて使う仮想ファイアウォール。こちらの NSG なんですが、ちょっと AWS と違うのは、NSG の割当先として NIC とサブネットの 2 つから選べる点。\nそして MS から推奨されているのは、運用管理をシンプルにするということで個別の NIC ごとに NSG を割り当てるではなく、サブネットに NSG を割り当てる、です。なので、ふつうに設計を進めていくと用途ごとにサブネットを分けて、サブネットに NSG を割り当てていくことになるんですよね、、、と、ここで誤解が生じるんです。\nそう、サブネットに NSG を割り当てるってそのまま聞くと動き的には、いわゆる AWS でいうネットワーク ACL のような感じになるんじゃなかろうか、と。ちなみに、AWS のネットワーク ACL っていうのは、サブネットへの入出力のところでチェックをかけるっていうやつですよね。なので、サブネット内通信はネットワーク ACL を制御できないので、サブネット内は何でも OK なノーガード状態になるわけです。\nでも Azure でサブネットに NSG を割り当てた時の正しい動きは、「サブネットに NSG を割り当てると自動的にサブネット配下の NIC すべてに当該 NSG が適用される」なので、サブネット内通信もしっかり NSG でガードされます。Azure 有識者だと当たり前なんですが言葉通りにサブネットへ NSG を割り当てるだと、サブネット内はザルの方を想像される方が多いようです。とくにネットワークに詳しいエンジニアさんの方が誤解される傾向にあるかもしれないです。\nもう頭の中ではサブネット内通信はザルと思っているから、サブネットに NSG を割り当てて実際に動かしてみると「あれ、通信できないんですけどーーーー！！」ということで慌てられるケースもあったりなかったり。ということで、Azure を利用する際は少しだけお気を付けを。\n","date":"May 19, 2021","permalink":"/posts/2021/05/azure-misanderstanding-about-security-group/","section":"記事一覧","summary":"最近も出会ってしまった。こんな誤解。","title":"たまに出会う Azure のセキュリティグループに関する誤解"},{"content":"Google Cloud認定資格を受けたことがある方はご存じの通り、専用の申し込みサイト(Google Cloud Webassessor)から試験の予約をするのですが、英語試験の予約と日本語試験の予約では異なるアカウント（Emailアドレスは同じ)が必要になります。\nなぜ異なるアカウントが必要なのか # 理由は「日本語試験用アカウントでログインすると英語の試験を選択できない仕様だから」です。もちろん逆もまた然り、です。\nこれを知らないと、英語版の申し込みサイトに飛んで（日本語専用とは知らずに日本語専用の）アカウントでログインすると、（英語試験の申し込みサイトではなくて日本語試験の申し込みサイトに飛んでしまってて）英語での試験項目が一覧に出てこなくて「なぜだ!?」となります。\nそして再び英語版の申し込みサイトへ戻って、再度同じアカウントでログインしなおして、再度同じ事象に陥る、、、のループを繰り返してムダな時間をしばらく過ごしてしまう方も、きっと少なからずいらっしゃることかと思います。少なくとも筆者はムダな時間を過ごしました^^;\nということで、過去に日本語試験を受けたことがある方で、これから英語試験を受けようかなって思われている方はご注意ください。\nここからは完全に雑談です # そういえば英語版のGoogle Cloud Webassessorアカウントを作成する際に、次のように「Eメールアドレスはログイン名に使うなよ！」という注意書きが出てきて、めちゃくちゃドキっとしたんですよね。\nなんでかというと、私、日本語版のアカウント名にEメールアドレスを使っていたから、なんです、、、\nまじかー、完全にやらかしたわー、と思いつつ日本版のアカウント作る時にも「Eメールアドレスはダメよ！」って注意書きが出てきたのにそれを見逃したんだなきっと、、、と自分を責めつつも、念のため日本語のアカウント作成画面へ確認しにいったんです。\nすると、あろうことか日本語サイトくんはこう言っているわけです。\nおいおいおいおい、さすがにこの翻訳はイケてなさすぎるでしょう、、、そらー、Eメールアドレスをアカウント名に使っちゃうわ、っていうね。はい、以上どうでもいい小ネタでしたｗ\n","date":"May 17, 2021","permalink":"/posts/2021/05/gcp-exam-account-required-for-each-language/","section":"記事一覧","summary":"Google Cloud認定資格を受けたことがある方はご存じの通り、専用の申し込みサイト(Google Cloud Webassessor)から試験の予約をするのですが、英語試験の予約と日本語試験の予約では異なるアカウント（Emailアドレスは同じ)が必要になります。","title":"Google Cloud資格試験は言語ごとに異なるアカウントが必要なのでご注意を"},{"content":"","date":"May 10, 2021","permalink":"/tags/azure-ad/","section":"タグ一覧","summary":"","title":"Azure AD"},{"content":"みなさん、こんにちは。今回は Azure Active Directory(Azure AD) でアプリケーションと SAML 連携する際に使う SSL 証明書に関するお話です。\n最近は Azure AD と SaaS アプリケーション、たとえば GitHub や JFrog Artifactory など、とを SAML 連携してシングルサインオン(SSO)を実現しています、といった事例もそこそこあるんかなと推測してます。\nただご存じの通り、Azure AD で作成した証明書って期限が 3 年なんですよね。ん？証明書の期限切れになったら何が起きるか、ですか？答えはシングルサインオンに失敗します。つまりサインインできません。もちろん救済処置を用意している SaaS アプリケーションもあるんですけどね、、、\nということで、まぁ 3 年って個人的には優しい方だと思うんですが、証明書のロールオーバー作業を定期的に実施する必要が出てくるわけです。面倒なのですがね^^;\nただ最近、「そもそも証明書の期限をもっと長くできないの？」って聞かれたので、世の中の流れに反するので正直あまりオススメはしたくないんですが、タイトルに書いた通り期限を長くする方法について記載していきたいと思います。\nで、どうすればいいの？ # 結論を言うと、 期限の長い証明書を Azure AD の外部で作って、それを Azure AD へインポートする です。具体的な手順としては、、、\nAzure AD 外部の適切なところで SAML 署名証明書を作成する。詳細は割愛。 対象アプリケーションの SSO 設定画面から「証明書のインポート」を選択する。 インポートする証明書ファイルを選択する。 インポートした証明書をアクティブ化する。 証明書がアクティブになっていることを確認する。 てな感じで有効期限の長ーーーーい証明書にすることができましたｗ\n終わりに # ということで、今回は Azure AD でアプリケーションのシングルサインオンをする際に使用する SSL 証明書の期限を 3 年よりも長くする方法のご紹介でした。\n世の中の流れ（SSL 証明書の有効期限は徐々に短縮されていく）に反するので、あまり大きな声でできますというのは忍ばれるのですが、、、「どうしても証明書のロールオーバーは手間がかかるんでやりたくないんです！」というわがままバディさんには一応こういう方法もあるんですよ、とだけ言っておきたいと思います。\nちなみに補足ですが、Azure AD と SaaS アプリケーションの SSO 設定については Microsoft 社の公式ドキュメントに詳細がありますので、こちらをご参照いただければと思います。\n","date":"May 10, 2021","permalink":"/posts/2021/05/azure-ad-with-long-lived-saml-certificate/","section":"記事一覧","summary":"みなさん、こんにちは。今回は Azure Active Directory(Azure AD) でアプリケーションと SAML 連携する際に使う SSL 証明書に関するお話です。","title":"Azure AD でアプリケーションと SAML 連携する際に証明書の期限を長くする方法"},{"content":"","date":"May 10, 2021","permalink":"/tags/single-sign-onsso/","section":"タグ一覧","summary":"","title":"Single Sign-On(SSO)"},{"content":"","date":"May 10, 2021","permalink":"/tags/ssl%E8%A8%BC%E6%98%8E%E6%9B%B8/","section":"タグ一覧","summary":"","title":"SSL証明書"},{"content":"すでに Professional Cloud Architect だけは持っていたのですが、ACE の受験バウチャーを無償でいただく機会があったので「使わないなんてもったいない！」ということで 2021/05/06 に他の試験のついでに受験したのでその時のメモです。\nAWS や Azure などをよく知っている人には多少は役に立つかもしれませんが、逆に、これからクラウドを一から勉強するという方にとってはあまり参考にならない情報かもしれませんのであしからず。\nまぁいっぱい情報転がっていると思うのでこんな記事は読まずに素直に他の記事を読みましょう。\n受験メモ # 受験者情報 # AWS、Azure、Google Cloudの三大クラウド上級資格持ち、とくにAWS資格は全種コンプしているので一見するとけっこう良さげな(クラウド歴1年の)新米クラウドエンジニアです。\nNo. 項目 取得数 取得済み資格/合格済み試験 1 AWS資格 12 CLF、SAA、SOA、DVA、SAP、DOP、\nSCS、ANS、AXS、DBS、DAS、MLS 2 Azure資格 4 AZ-900、AZ-104、AZ-303/304、AZ-400 3 Google Cloud資格 1 PCA 結果 # 合格 (点数不明) 準備期間 # 4 時間強くらい 内訳 # 前準備、パートナーキックスタートに登録して Qwiklabs クエスト 2 種をクリアして無料で受験バウチャーをゲット (実質 3h、実際はバウチャー送付待ちなどで数 days) 試験ガイドを読んで不安な箇所がないかを確認する (0.2h) 公式の模擬試験(無料)を実施し、解説を読みこむ (1h) 感想 # PCA を持っていたためか公式模擬試験を初見で 9 割くらい取れたのでそのまま GO で OK でした。とくに対策してません。参考にならんくてすんません。 試験時間は 2 時間ですが開始 40 分くらいの時点で試験会場を出てきたので、かなり時間には余裕があるかと思います。 個人的には AWS や Azure などのアソシ試験と比較して楽だったかな、と。AWS よりも出てくるサービスが少ないし、Azure のように細かい手順を問われたりもしないので。 アドバイスなど # まずは模擬試験(30 問)をやってどこが弱いかを把握し、不足を補うように勉強しましょう。 試験ガイドに載っている各サービスがどんなことできるのかは把握しておきましょう。深い知識は不要。 AWS や Azure に詳しい方ならあらかじめ以下を見ておくといいかもしれないです。 べスプラに沿った回答を求められるケースが多いが、こちらは AWS や Azure の知識がだいたいそのまま流用可能です。 終わりに # ということで Associate Cloud Engineer (ACE)を取ってきました、、、だがしかし！！\nこちらの資格は上位資格である Professional Cloud Architect (PCA)の前提条件というわけでもないですし、PCA も感覚的には AWS などと比較してそこまで難しくはないので、普通の人はアソシをすっ飛ばして最初からプロを目指しちゃってもいいのかなと思います^^;\nGoogle Cloud は、Google LLC の商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"May 7, 2021","permalink":"/posts/2021/05/gcp-associate-cloud-engineer-exam/","section":"記事一覧","summary":"すでに Professional Cloud Architect だけは持っていたのですが、ACE の受験バウチャーを無償でいただく機会があったので「使わないなんてもったいない！」ということで 2021/05/06 に他の試験のついでに受験したのでその時のメモです。","title":"Google Cloud Certified Associate Cloud Engineer 受験メモ"},{"content":"最近、待望の Google Cloud Certified Professional Data Engineer のパートナー認定資格キックスタートプログラムが開始されました。ってことで、さっそくバウチャーを取得して 2021/05/06 に受験してきたので、その時のメモです。\nちなみに、参考までに費やした時間なども記載してますが個人差が大きいと思うのであくまでご参考です。\n受験メモ # 受験者情報 # AWS、Azure、Google Cloudの三大クラウド上級資格持ち、とくにAWS資格は全種コンプしているので一見するとけっこう良さげな(クラウド歴1年の)新米クラウドエンジニアです。\nNo. 項目 取得数 取得済み資格/合格済み試験 1 AWS資格 12 CLF、SAA、SOA、DVA、SAP、DOP、\nSCS、ANS、AXS、DBS、DAS、MLS 2 Azure資格 4 AZ-900、AZ-104、AZ-303/304、AZ-400 3 Google Cloud資格 1 PCA 結果 # 合格 (点数不明) 準備期間 # 12 時間くらい 内訳 # 前準備、パートナーキックスタートに登録して Qwiklabs クエスト 2 種をクリアして無料で受験バウチャーをゲット (実質 4h、実際はバウチャー送付待ちなどで数 days) 合格者のブログを漁って勉強の計画などを立てる (1h) 試験ガイドを読んで不安な箇所がないかを確認する (0.2h) 公式の模擬試験(無料)を実施し、解説を読みこむ (1h) 高評価な問題集をやる (5-6h) 感想 # 模擬試験は初見で 6 割くらいだったので少し勉強すりゃ大丈夫かなと思い、評価のよさそうな問題集(英語)を 1 つ選んでやりました。 2 時間の試験でしたが 1 時間強くらいで見直し含めて終わりました。 日本語はしっかりしているから普通に受ける分には問題はないのだけど、英語で勉強した手前、AWS や Azure のように英語へ切り替えられない仕様なのがちょいとつらかった。 アドバイスなど # まずは模擬試験(30 問)をやってどこが弱いかを把握し、不足を補うように勉強しましょう。 以下のブログにて試験のテクニック的なこと書いてくれてて結構参考になった気がします。 今回利用した教材は次の Udemy のこちら、キャンペーンで 1500 円くらいでした。こちらの問題集から似たような問題がちょいちょい本番でも出てきたので点数の底上げに役立つかと思います。余談ですが筆者は ★4.5 を付けました。有用だけど日本語じゃないから－★0.5 です。 終わりに # ということで Professional Data Engineer を取ってきました、、、しかもタダで！！ あ、Udemy で教材買ったんだった、、、\nもし Google さんとパートナーの企業のエンジニアの方は、ぜひパートナー認定資格キックスタートプログラムを利用してタダでチャレンジしてみてはいかがでしょうか。\nGoogle Cloud は、Google LLC の商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。 ","date":"May 7, 2021","permalink":"/posts/2021/05/gcp-professional-data-engineer-exam/","section":"記事一覧","summary":"最近、待望の Google Cloud Certified Professional Data Engineer のパートナー認定資格キックスタートプログラムが開始されました。ってことで、さっそくバウチャーを取得して 2021/05/06 に受験してきたので、その時のメモです。","title":"Google Cloud Certified Professional Data Engineer 受験メモ"},{"content":"","date":"April 30, 2021","permalink":"/tags/dataflow/","section":"タグ一覧","summary":"","title":"Dataflow"},{"content":"いやー最近(2021 年 4 月頃)、Professional Data Engineer の取得を目指して Dataflow を触って遊んでたら、こんな現象が何回も出てきて困ったんすよね、、、っていうことで共有します。\n現象 # Dataflow でジョブを流したら速攻でエラー終了するんです。そして、次のようなエラーメッセージがポロンって出てくるわけです。むーん。\nWorkflow failed. Causes: There was a problem refreshing your credentials. Please check: 1. Dataflow API is enabled for your project. 2. Make sure both the Dataflow service account and the controller service account have sufficient permissions. If you are not specifying a controller service account, ensure the default Compute Engine service account [PROJECT_NUMBER]-compute@developer.gserviceaccount.com exists and has sufficient permissions. If you have deleted the default Compute Engine service account, you must specify a controller service account. For more information, see: https://cloud.google.com/dataflow/docs/concepts/security-and-permissions#security_and_permissions_for_pipelines_on_google_cloud_platform. , There is no cloudservices robot account for your project. Please ensure that the Dataflow API is enabled for your project. チェックしてみてって言われてるから調べてみる、、、 # まずは「あれ、API 有効化してなかったけなー？」なんて思いながら「1. Dataflow API is enabled for your project.」を確認してみるわけですね。\n、、、はい、もちろん有効化済みですよーっと。そらそうだ。\nでは次に「じゃあ、サービスアカウントの設定が変だったんだな、きっと！」なんて思いつつ「2. Make sure both the Dataflow service account and the controller service account have sufficient permissions.」の方も確認していきましょう。\n、、、はい、Editor 権限を付与しておりとくに問題はなさそうに見えますよーっと。\nんー、オワタ、速攻で手詰まってしまったｗ やっぱり、Google Cloud のエラーメッセージはあてにならんことが多いよなぁ、、、\nってことで Stack Overflow を見に行くわけだよね # 、、、とここで、何かがフッと降りてきて、次の行動をとってみた、、、\nおもむろにジョブを再実行してみる # 「設定は正しいはずなんだけどなぁ」なんてことを思いながら、おもむろに再実行してみると、、、なんということでしょう。\nさっきはエラーになったジョブが動くんだ、なぜかこれが。そらぁ設定ちゃんとされてるもんね。むしろ、じゃあ、なんでさっきエラーになったんだよ！っていうね^^;\n終わりに # ということで今回も内容のない記事ですいませんでした。とはいえ、同じ現象に陥って困っている方はちょっと時間をおいて再実行してみても損はないのかな、と^^;\n","date":"April 30, 2021","permalink":"/posts/2021/04/gcp-dataflow-failed-refleshing-credentials/","section":"記事一覧","summary":"いやー最近(2021 年 4 月頃)、Professional Data Engineer の取得を目指して Dataflow を触って遊んでたら、こんな現象が何回も出てきて困ったんすよね、、、っていうことで共有します。","title":"Google Cloud の Dataflow でクレデンシャルの更新に失敗したというエラーが発生する件について"},{"content":"","date":"March 25, 2021","permalink":"/tags/google-cloud-functions/","section":"タグ一覧","summary":"","title":"Google Cloud Functions"},{"content":"いやー最近(2021 年 3 月頃)、AWS と GCP で遊んでいたらこんな現象が出てきて困ったんすよね、、、ってことで共有します。ちなみに、2021/3/25 2021/7/21 時点でもまだ同じ現象は出ますね。\n現象 # 日本リージョン(東京/大阪)にデプロイした Google Cloud Functions から、日本からの受付のみ許可するように地域制限をかけた Amazon CloudFront にアクセスをしました。期待はもちろん成功なんですが、、、\nジャジャーン！はい、キタコレ、拒否！！、、、きっと仲が良くないん（ry\nCloudFront ってどうやって地理を判定しているの？ # 「Amazon CloudFront デベロッパーガイド」を見ると、サードパーティの GeoIP データベースを使っていますよってことです。あー、なるほど正確性は「99.8%」なのか。\nてか、サードパーティの GeoIP データベースってどこよ？と思ったら「Amazon CloudFront API Reference」の方には MaxMind GeoIP databases って明記されておりました。\nなるほど、じゃあ、ここのデータベースに Google Cloud Functions で利用されている IP アドレス範囲が入ってくればいいのか、、、まぁ AWS を疑ってたわけだけど AWS は全然悪くないのね。\n余談、どんなコード書いたの？ # 「Cloud Functions でどんなコード書いて試したの？」って聞かれたので「ん、こんな感じの雑なので試しましたよ」って答えておく。（でも、ほんとはググってほしい）\n作成例）main.py\ndef hello_world(request): import requests response = requests.get(\u0026#39;http://XXXXXXXXXXXXX/index.html\u0026#39;) return response.text 作成例）requirements.txt\nrequests 最後に # AWS から見るとやっぱ Google Cloud って外国扱いなんかな、と思ったら AWS 全然悪くないじゃない。疑ってすいませんでした。ということで、Google Cloud Functions の IP アドレスも MaxMind GeoIP データベースにいつの日か登録されることを祈っております。\n","date":"March 25, 2021","permalink":"/posts/2021/03/access-denied-from-cloud-functions-to-cloudfront/","section":"記事一覧","summary":"いやー最近(2021 年 3 月頃)、AWS と GCP で遊んでいたらこんな現象が出てきて困ったんすよね、、、ってことで共有します。ちなみに、2021/3/25 2021/7/21 時点でもまだ同じ現象は出ますね。","title":"Google Cloud Functionsから地域制限をかけたAmazon CloudFrontへアクセスすると拒否される件について"},{"content":"AWS資格コンプを目指して、同日に Machine Learning - Specialty (MLS) を受けるついでに Developer - Associate (DVA) を 2021/1/29 に受験してきました。その時のメモです。\n受験メモ # 受験者情報 # AWS、Azure、Google Cloudの三大クラウド上級資格を持っているので一見ではそこそこのエンジニアに見えるものの、実際はAWSを業務で扱いだして約10か月、Azureは約5か月、Google Cloudは約3か月の駆け出しクラウドエンジニアです。\nNo. 項目 取得数 取得済み資格/合格済み試験 1 AWS資格 10 CLF、SAA、SOA、SAP、DOP、SCS、ANS、AXS、DBS、DAS 2 Azure資格 4 AZ-900、AZ-104、AZ-303/304、AZ-400 3 Google Cloud資格 1 PCA 結果 # 862点 (合格)\n試験対策例 # No. 対策項目 実施時間(例) 1 公式のサンプル問題をやる 約0.5時間 2 問題集をやる 約1時間 アドバイスなど # サンプル問題を見てもらうとわかるのですが、考えれば解ける問題ではなく知ってないと解けない問題が多く、上位のDevOps持っているからと言ってノーガードで行ってしまうと苦戦を強いられる可能性大です。私も期限が残ってたAWS WEB問題集を直前に軽くやっておいて良かったなと感じています。\n終わりに # 記事の執筆時点ではすでに結果が出ているのですが残りはあと1個、AWS資格コンプがいよいよ近づいてきましたｗ\n","date":"February 28, 2021","permalink":"/posts/2021/02/aws-dva-c01-exam/","section":"記事一覧","summary":"AWS資格コンプを目指して、同日に Machine Learning - Specialty (MLS) を受けるついでに Developer - Associate (DVA) を 2021/1/29 に受験してきました。その時のメモです。","title":"AWS Certified Developer - Associate (DVA-C01) 受験メモ"},{"content":"AWS資格コンプを目指して最後の1個、Machine Learning - Specialty (MLS) を 2021/2/26 に受験してきました。その時のメモです。\n受験メモ # 受験者情報 # AWS、Azure、Google Cloudの三大クラウド上級資格を持っているので一見ではそこそこのエンジニアに見えるものの、実際はAWSを業務で扱いだして約10か月、Azureは約5か月、Google Cloudは約3か月の駆け出しクラウドエンジニアです。\nNo. 項目 取得数 取得済み資格/合格済み試験 1 AWS資格 11 CLF、SAA、SOA、DVA、SAP、DOP、\nSCS、ANS、AXS、DBS、DAS 2 Azure資格 4 AZ-900、AZ-104、AZ-303/304、AZ-400 3 Google Cloud資格 1 PCA 結果 # 797点 (合格)\n試験対策例 # No. 対策項目 実施時間(例) 1 機械学習の基礎を勉強する 約2時間 x 3日 2 公式のサンプル問題をやる 約0.5時間 3 AWS WEB問題集をやる 約2時間 x 2日 アドバイスなど # 最初にサンプル問題を見てもらうとわかるとおり、他の試験とはかなり傾向が違っていて機械学習やデータ分析に関する一般的な知識を多く問われる試験になっています。AWSサービスやアーキテクチャに関する設問(全体の3、4割程度？)は簡単だけど、それ以外のところはAIに関する知識がないと手も足も出ない印象です。\n私自身は機械学習やデータ分析についてはそこまでなじみがなかったため基本からの学習を開始。とはいえ、どの本がいいかもわからなかったため、適当に目についた以下の本にまずは目をとおしてみました。どれも良本だとは思いますが、個人的にはどれもガッチリとハマった感じはしなかったので、本試験対策として読むにはあまり適切な本ではなかった可能性はあります。\nPython機械学習プログラミング マンガでわかる機械学習 Pythonと実データで遊んで学ぶデータ分析講座 ディープラーニング実装入門 AWS WEB問題集については当時21問しかなかったのですが、解説のところに書かれているリンクなども含めて読み込んだのはとても役に立ったと感じています。\n終わりに # ということで、無事に最後のMLS試験も突破できました。クラウドを真剣に業務で扱うようになって1年弱、なんとかAWS資格をコンプすることができました。めでたしめでたし。\n","date":"February 28, 2021","permalink":"/posts/2021/02/aws-mls-c01-exam/","section":"記事一覧","summary":"AWS資格コンプを目指して最後の1個、Machine Learning - Specialty (MLS) を 2021/2/26 に受験してきました。その時のメモです。","title":"AWS Certified Machine Learning - Specialty (MLS-C01) 受験メモ"},{"content":"AWS資格コンプを目指して AWS Certified Data Analytics - Specialty (DAS) を 2021/2/16 に受験してきました。その時のメモです。\n受験メモ # 受験者情報 # AWS、Azure、Google Cloudの三大クラウド上級資格を持っているので一見ではそこそこのエンジニアに見えるものの、実際はAWSを業務で扱いだして約10か月、Azureは約5か月、Google Cloudは約3か月の駆け出しクラウドエンジニアです。\nNo. 項目 取得数 取得済み資格/合格済み試験 1 AWS資格 9 CLF、SAA、SOA、SAP、DOP、SCS、ANS、AXS、DBS 2 Azure資格 4 AZ-900、AZ-104、AZ-303/304、AZ-400 3 Google Cloud資格 1 PCA 結果 # 765点 (合格)\n試験対策例 # No. 対策項目 実施時間(例) 1 公式のサンプル問題をやる 約0.5時間 2 問題集をやる 約2時間 x 5日 3 不安な部分をサービス別資料を読んで補強する 約2時間 アドバイスなど # データ分析の通常プロセス（収集、格納、加工、分析、可視化）にかかわる問題がまんべんなく出てきた感じです。Kinesis(Data Stream、Data Firehose、Data Analytics)、S3、Redshift、EMR、Glue、Athena、QuickSight、ElasticSearchとKibanaあたりの知識は必須です。また、EMRなどで使われているApache系ツールについてもざっくり何やるものかくらいは把握しておきましょう。\nちなみに、点数取れてないので偉そうなことは言えませんが、ポイントとしては以下のような感じかと思います。\n収集はストリーミングで取り込むならKinesis系、たまにMSK(Kinesisで扱えない大きなサイズとか)、一発ガツンと移行ならSnowballなどのデータ移行ツール。 加工はEMR(Hadoop、Spark、Hive、Presto等のApache系ツール)が基本、楽したいときはGlue、たまにBatchで短時間回すといったケースもアリ。EMRのワーカーやBatchは、コスト面からスポットの利用や不要なら止めておく。Athenaとかで分析しやすいように取り込み段階で形式変更、結合、圧縮を。Redshiftへ取り込みやすいようにスライス数の倍数に分割とかも。 格納先はS3、Redshift(複雑なSQL使う系)、DynamoDB(細かいデータたくさん系) 可視化はQuickSight、ESならKibana、場合によりJupyterノートブック的なのも。 終わりに # 点数取れてないので偉そうなことは言えませんが、しっかり対策していけばそこまで難しい試験ではない印象でした。AWS資格コンプまであと2個、やっと終わりが見えてきました。来週、いっきに2試験を受けてコンプしてきます。\n","date":"February 20, 2021","permalink":"/posts/2021/02/aws-das-c01-exam/","section":"記事一覧","summary":"AWS資格コンプを目指して AWS Certified Data Analytics - Specialty (DAS) を 2021/2/16 に受験してきました。その時のメモです。","title":"AWS Certified Data Analytics - Specialty (DAS-C01) 受験メモ"},{"content":"AWS資格コンプを目指して AWS Certified Database - Specialty (DBS) を 2021/2/8 に受験してきました。その時のメモです。\n受験メモ # 受験者情報 # AWS、Azure、Google Cloudの三大クラウド上級資格を持っているので一見ではそこそこのエンジニアに見えるものの、実際はAWSを業務で扱いだして約10か月、Azureは約5か月、Google Cloudは約3か月の駆け出しクラウドエンジニアです。\nNo. 項目 取得数 取得済み資格/合格済み試験 1 AWS資格 8 CLF、SAA、SOA、SAP、DOP、SCS、ANS、AXS 2 Azure資格 4 AZ-900、AZ-104、AZ-303/304、AZ-400 3 Google Cloud資格 1 PCA 結果 # 780点 (合格)\n試験対策例 # No. 対策項目 実施時間(例) 1 公式のサンプル問題をやる 約0.5時間 2 サービス別資料(とくにRDS、Aurora、DynamoDB)を読み込む 約2時間 x 5日 3 Exam ReadinessのQuizをやる 約1時間 アドバイスなど # データベースのサービス別資料をしっかり読み込んでおきましょう。どういうユースケースで各DBを選択するかというのはSAPでやっているかと思うのでSAP対策のDB周りをやっておくと良いかと思います。\nDBの中身の話はほぼほぼない感じなのでDB苦手だよって人でも行けます。GSI？そんなものは1問くらいしか出てこないのでテスト対策という意味では捨てでも良いでしょう。\n終わりに # 久々に時間いっぱい使ってしまいました^^; AWS資格コンプまであと3個、つらい戦いが続きますが来週は Data Analytics - Specialty (DAS) を受けてきます。\n","date":"February 13, 2021","permalink":"/posts/2021/02/aws-dbs-c01-exam/","section":"記事一覧","summary":"AWS資格コンプを目指して AWS Certified Database - Specialty (DBS) を 2021/2/8 に受験してきました。その時のメモです。","title":"AWS Certified Database - Specialty (DBS-C01) 受験メモ"},{"content":"業務では一切さわらないAlexaですが、AWS資格コンプを目指して Alexa Skill Builder - Specialty (AXS) を 2021/1/29 に受験してきました。その時のメモです。\n受験メモ # 受験者情報 # AWS、Azure、Google Cloudの三大クラウド上級資格を持っているので一見ではそこそこのエンジニアに見えるものの、実際はAWSを業務で扱いだして約9か月、Azureは約4か月、Google Cloudは約2か月の駆け出しクラウドエンジニアです。\nNo. 項目 取得数 取得済み資格/合格済み試験 1 AWS資格 6 CLF、SAA、SAP、DOP、SCS、ANS 2 Azure資格 4 AZ-900、AZ-104、AZ-303/304、AZ-400 3 Google Cloud資格 1 PCA 結果 # 780点 (合格)\n試験対策例 # No. 対策項目 実施時間(例) 1 公式のサンプル問題をやる 約0.5時間 2 Alexaに関する入門書を読む 約1時間 3 問題集をやってみる 約1時間 x 5日 4 公式ドキュメントを読む 約1時間 アドバイスなど # Alexa以外にもLambda、S3、CFn、DynamoDB、CloudWatch、IAMなんかの知識も問われます。とはいえ、上級資格をすでに持っている方にとってAWSサービスに関する設問は、それまで深い知識を求められるわけではないのでチャンス問題だと思います。確実に点数に変えていきましょう。\n次に勉強法についてですが、私の勉強法は間違いなく良くなかったと感じてます。決してマネはしない方が良いと思います。\nまず入門書はKindle Unlimitedで使えそうな教材を探して「Amazon Alexaプログラミング入門 impress top gearシリーズ」を選択しました。この本自体は悪くなかったと思うのですが、私がこの本に向き合う姿勢が明らかに良くなかったです。具体的には、全部細かく目を通していくのはしんどかったので1時間程度で流し読みしてしまいました。（すいません。サラーっと流しすぎたので正直中身は覚えてないです、、、）\n次に問題集、これまたKindle Unlimitedで使えそうな教材を探して「AWS Certified Alexa Skill Builder - Specialty: Prep Exams (English Edition)」を選択しました。この問題集をこなすのに一番リソースを割いたわけなのですが、問題集を本試験の約1時間前やり終えた後に「よしやることやった、試験がんばろう！」じゃなくて「これはやばいぞ、この問題集で大丈夫だったのか！？」と悪い予感しかなくいそいで公式ドキュメントをわーーーっと見て悪あがきをするくらいでした。実際、本試験の問題よりもかなり簡単で、まったく似たような問題も出てこなかったのでやはり使えなかったという印象です。（いや、そう思っているだけで実はこれが効果的だったのだろうか、、、）\n今思えば、Exam Readinessとか、Udemyなどの教材もあるのでこちらを活用した方がよかったと思っています。\n終わりに # ということで、最後までバタバタして試験に挑んだっわけですが無事に合格できてラッキーでした。絶対に落ちたなと思ったくらいつらい戦いでした。次受けたら落ちると思います。そんくらいギリギリの戦いでした。もう受けなくていいと思うとホッとします。\n","date":"January 31, 2021","permalink":"/posts/2021/01/aws-axs-c01-exam/","section":"記事一覧","summary":"業務では一切さわらないAlexaですが、AWS資格コンプを目指して Alexa Skill Builder - Specialty (AXS) を 2021/1/29 に受験してきました。その時のメモです。","title":"AWS Certified Alexa Skill Builder - Specialty (AXS-C01) 受験メモ"},{"content":"同日に Alexa Skill Builder - Specialty (AXS) を受けるついでに SysOps Administrator Associate (SOA) を 2021/1/29 に受験してきました。その時のメモです。\n受験メモ # 受験者情報 # AWS、Azure、Google Cloudの三大クラウド上級資格を持っているので一見ではそこそこのエンジニアに見えるものの、実際はAWSを業務で扱いだして約9か月、Azureは約4か月、Google Cloudは約2か月の駆け出しクラウドエンジニアです。\nNo. 項目 取得数 取得済み資格/合格済み試験 1 AWS資格 6 CLF、SAA、SAP、DOP、SCS、ANS 2 Azure資格 4 AZ-900、AZ-104、AZ-303/304、AZ-400 3 Google Cloud資格 1 PCA 結果 # 731点 (合格)\n試験対策例 # No. 対策項目 実施時間(例) 1 試験ガイドを見て出題範囲を確認する 約0.5時間 アドバイスなど # SAPなどよりも細かい知識、たとえば具体的な設定オプションに関する知識などを問われる印象でした。考えても答えが導けない、知らないと解けない問題が多かった印象です。上級資格を持っているからとノーガードで行くと足元を救われる可能性ありです。せめてサンプル問題くらいはやってから行くことをオススメします。\n終わりに # AXS試験の前座と完全になめておりましたが、試験中は心臓ドキドキ、変な汗がぶわーっと出ておりました。終了ボタンを押すのがとても怖かったのですが「合格」と出てくれてラッキーでした。SAPやDOPを更新すると、こちらも勝手に更新される=もう試験を受けなくていい、と思うとホッとしてます^^;\n","date":"January 31, 2021","permalink":"/posts/2021/01/aws-soa-c01-exam/","section":"記事一覧","summary":"同日に Alexa Skill Builder - Specialty (AXS) を受けるついでに SysOps Administrator Associate (SOA) を 2021/1/29 に受験してきました。その時のメモです。","title":"AWS Certified SysOps Administrator - Associate (SOA-C01) 受験メモ"},{"content":"ジツは昨年、こっそり受験してしっかり不合格となっていた AWS Certified Advanced Networking - Specialty (ANS) のリベンジを 2021/1/12 にはたしてきました。そのときのメモです。\n受験メモ # 受験者情報 # AWS、Azure、Google Cloudの三大クラウド上級資格を持っているので一見ではそこそこのエンジニアに見えるものの、実際はAWSを業務で扱いだして約9か月、Azureは約4か月、Google Cloudは約2か月の駆け出しクラウドエンジニアです。\nNo. 項目 取得数 取得済み資格/合格済み試験 1 AWS資格 5 CLF、SAA、SAP、DOP、SCS 2 Azure資格 3 AZ-900、AZ-104、(AZ-304)、AZ-400 3 Google Cloud資格 1 PCA 結果 # 56% (不合格、2020/9/11受験) 780点 (合格、2021/1/12受験) 試験対策例 # No. 試験回次 対策項目 実施時間(例) 1 1回目 AWS WEB問題集をやる 約2時間 × 3日 2 2回目 試験ガイドを見て出題範囲を確認する 約0.5時間 3 2回目 サービス別資料(主にDirect Connect)を読み込む 約2時間 × 3日 アドバイスなど # Direct Connectに関する知識はサービス別資料を読み込むなりしてしっかり身につけましょう。またネットワークのトラブルシューティングに関する設問もそこそこ出てきます。フローログやパケットキャプチャなどでは何が記録されるのかといった基本はしっかり身につけていきましょう。\nあと、この試験は非常に残念なことに、Transit Gatewayが出てくる前に使われていたTransit VPCなどの今はもう使われない廃れた技術に関する設問も出てきます。こちらは完全に試験対策だと割り切って過去の資料を漁りましょう。どんな技術に関する設問が出るかは試験のガイドラインをしっかり目を通しておくことをオススメします。\n問題集は過去にSAP試験を受ける際に契約したAWS WEB問題集の期限が残っていたのでこちらを活用しました。当時は問題数がやや少なくあまり充実はしていなかった印象でしたが、今思うとちゃんと試験ガイドラインやサービス別資料を呼んだあとにやっていればもっと有用だったのかなと思います。\n終わりに # やはり試験ガイドを見ないまま試験へ臨むのは良くないですね^^; 不合格を受けた1回目はTransit Gatewayなどの現在使われている技術はもちろん知っていたのですが、なんせ新参者なので今は廃れてなくなった過去のサービスまでは完全にノーマークでした。もちろん言い訳でしかないですけどｗ\n「Transit Gatewayっていう名前になる前はTransit VPCって言ってたのかな？」とか「むしろ誤訳か？」とすら思いながらテストを受けていたので、不合格になるべくしてなってしまった、という感じでしょうか。もちろん敗因はそれだけではなく、そもそもこの試験でメインと言ってもいいDXの知識がかなりあいまいな状態だったとかもあったと思いますが^^; いやー、無事リベンジを果たせてラッキーでした。\nちなみに、何度も書いてきましたがこの試験はすでに廃れた使われていない技術に関する設問も出てきます。これらはトリビア以外の何者でもなく、完全に業務では使えません。AWS資格のコンプを目指す人以外はひとまずANS-C00バージョンのテストはスルーして、ANS-C01へアップデートされて実状に沿った内容へ改定されてからの受験を強くオススメします。\n","date":"January 16, 2021","permalink":"/posts/2021/01/aws-ans-c00-exam/","section":"記事一覧","summary":"ジツは昨年、こっそり受験してしっかり不合格となっていた AWS Certified Advanced Networking - Specialty (ANS) のリベンジを 2021/1/12 にはたしてきました。そのときのメモです。","title":"AWS Certified Advanced Networking - Specialty (ANS-C00) 受験メモ"},{"content":"Azure を業務で扱いだして 2~3 か月くらい経ったので、そろそろ Solutions Architect Expert を取りに行かねばな、と思い立って AZ-303 (2021/1/5) と AZ-304 (2020/11/27) の試験を受けに行った時のメモです。\n受験メモ # 受験者情報 # AWS、Azure、Google Cloudの三大クラウド上級資格を持っているので一見ではそこそこのエンジニアに見えるものの、実際はAWSを業務で扱いだして約9か月、Azureは約4か月、Google Cloudは約2か月の駆け出しクラウドエンジニアです。\nNo. 項目 取得数 取得済み資格/合格済み試験 1 AWS資格 5 CLF、SAA、SAP、DOP、SCS 2 Azure資格 3 AZ-900、AZ-104、AZ-400 3 Google Cloud資格 1 PCA 結果 # AZ-303: 760 点 (合格) AZ-304: 895 点 (合格) 試験対策例 # 合格者のブログを漁って勉強の計画などを立てる (サボったので 0 時間) 試験のアウトラインを読んで不安な箇所がないかを確認する (約 1 時間) 自信のない部分を Microsoft Learn などでお勉強する (サボったので 0 時間) Udemy などで評価の高い問題集をやる (サボったので 0 時間) 感想 # AZ-303 は、AZ-304 よりも偏りなくまんべんなくいろんな分野の問題が出た印象。テクノロジーと銘打つ試験の通り、AZ-304 よりも個々のサービスについて深くて細かいノウハウを要求された印象。 AZ-304 は幅広く、いろいろな分野の問題が出てくるが、Azure AD のハイブリッドクラウドの設計が感覚的に 1 割くらいと多かった印象。デザインと銘打つ試験の通り、細かい仕様よりもシステムを設計する上での考え方などが重要な印象。上流設計をメインでされている方は AZ-303 よりも余裕をもって臨めるかと。 Azure は AZ-303/AZ-304 の 2 つの試験をパスしないと SA になれないので正直 AWS や GCP よりも手間がかかりましたね。試験の難易度は感覚的に AWS \u0026raquo;\u0026gt; Azure = GCP の印象。※あくまで個人的な意見なのであしからず。 アドバイスなど # AZ-303 と AZ-304 の出題範囲はおおよそ同じなので、2 つの試験はあまり間隔を開けずに受験することをオススメします。 AZ-303 は、たとえば、SQL サーバを冗長化するには RG は同じである必要があるか？SQL サーバをリージョン間で冗長化するための前提条件は？みたいな感じの細かい知識を求める問題が多いです。知らないと勘になってしまう(=選択肢は絞れるけど、どんなに考えても導き出せない)ので試験ガイダンスにしたがって穴を埋めるのが良いかと思います。 AZ-304 は、FgCF (Financial-grade Cloud Fundamentals) や Azure Well-Architecture フレームワークの知識を叩き込んでいけばどうにかなると思います。また、他クラウドのべスプラが頭に入っている方であれば、Azure についてそのままズバリを知らなくてもこうあるべきだ、を考えれば答えが導き出せるかも！？ 本試験を受けた知人が以下の問題集から似たような問題がちょいちょい出てきたので点数の底上げに役立った、とのこと。評価も高いですし勉強素材の選択肢の1つとして検討してみても良いかも！？ 終わりに # 繰り返しになりますが、Microsoft 赤間氏が GitHub で公開してくれている FgCF (Financial-grade Cloud Fundamentals) のコンテンツは Azure を設計する上で重要なエッセンスがギュッと詰まっていて、金融って銘打ってますけど他業界でも十分使える内容となっています。試験対策とか置いといて、Azure って何から勉強すればいいの？という方には「まずは FgCF を見んしゃい！」と言わせていただきたいですね。\nということで、Microsoft 認定 Solutions Architect Expert (AZ-303/AZ-304) を受験したときのお話でした。\n、、、余談ですが、テクノロジー (AZ-303) の方はノーガード戦法だと辛いだろうなと思って試験前に大型連休を挟んだ　はず　だったんですけどね、、、いやー、合格できてラッキーでした^^;\n","date":"January 9, 2021","permalink":"/posts/2021/01/azure-az-303-304-exam/","section":"記事一覧","summary":"Azure を業務で扱いだして 2~3 か月くらい経ったので、そろそろ Solutions Architect Expert を取りに行かねばな、と思い立って AZ-303 (2021/1/5) と AZ-304 (2020/11/27) の試験を受けに行った時のメモです。","title":"Microsoft Certified Azure Solutions Architect Expert (AZ-303/AZ-304) 受験メモ"},{"content":"難易度の高い他の資格試験を受けるついでに Cloud Practitioner を 2020/11/27 に受験してきました。その時のメモです。\n受験メモ # 受験者情報 # AWS、Azure、Google Cloudの三大クラウド上級資格を持っているので一見ではそこそこのエンジニアに見えるものの、実際はAWSを業務で扱いだして約7~8か月、Azureは約3か月、Google Cloudは約1か月の駆け出しクラウドエンジニアです。\nNo. 項目 取得数 取得済み資格/合格済み試験 1 AWS資格 4 SAA、SAP、DOP、SCS 2 Azure資格 3 AZ-900、AZ-104、AZ-400 3 Google Cloud資格 1 PCA 結果 # 934点 (合格)\n試験対策例 # No. 対策項目 実施時間(例) 1 試験ガイドを見て出題範囲を確認する 約10分 アドバイスなど # 上級資格を持っているなら何も恐れることはありません。準備不要で試験会場に乗り込んでOKです。\n終わりに # こう言っては何ですがAWS資格のコンプリートを目指す人じゃない限り、こちらの資格はスルーして最初から中級、上級を目指すのが良いと思います。\nまた、コンプリートを目指す人であっても大は小を兼ねるではないですが、より上位の資格対策の勉強をしておけばこちらの試験の対策はいらないと思います。初級から段階的に取っていきたくなる気持ちもわからなくはないですが、最初からもっと難しいところを目指して勉強し、時間に余裕のあるときについでで初級/中級も受けるって形でもいいのかなって個人的には思います。\n","date":"November 29, 2020","permalink":"/posts/2020/11/aws-clf-c01-exam/","section":"記事一覧","summary":"難易度の高い他の資格試験を受けるついでに Cloud Practitioner を 2020/11/27 に受験してきました。その時のメモです。","title":"AWS Certified Cloud Practitioner (CLF-C01) 受験メモ"},{"content":"11月頃から Google Cloud を業務で扱う予定ができたことと、とくに段階を踏まずに最初から上級資格を狙えるということで、Professional Cloud Architect を 2020/10/30 に受験してきました。その時のメモです。\n受験メモ # 受験者情報 # AWSを業務で扱いだして約半年、Azureは約2か月、Google Cloudは初心者、AWSもAzureも上級資格を持っているので対外的にはそこそこのクラウドエンジニアです。\nNo. 項目 取得数 取得済み資格/合格済み試験 1 AWS認定資格 4 SAA、SAP、DOP、SCS 2 Azure認定資格 3 AZ-900、AZ-104、AZ-400 結果 # 合格 (点数不明)\n試験対策例 # No. 対策項目 実施時間(例) 1 試験ガイドを見て出題範囲を確認する 約0.5時間 2 Google提供の学習コンテンツQwiklabsで勉強する 約1時間 x 2週間 3 公式模擬試験(無料)を実施する 約0.5時間 × 2、3回 4 問題集を実施する 約2時間 × 3日 アドバイスなど # いきなり勉強を開始するよりもまずは公式の模擬試験(無料)を受けてみて、どんな問題が出るのかザっと見てみるのが良いと思います。また、試験ではケーススタディ3種をベースにした設問が2~3割ほどあるので、こちらは事前に認定資格サイトで事例を読んでおくのが吉です。知識としてはGoogle Cloudが提供する各種サービスの基本とKubernetesの知識をひととおりおさえておけば十分だと思います。\n問題集としてはApp Storeから入手した無料のGCP認定プロフェッショナルクラウドアーキテクトアプリ(現在は配信停止)を実施しました。もちろん無料なので問題数は少ないもの試験では大変お世話になった印象です。\nQwiklabsについてはかなりボリュームはあるものの、Google Cloudを実際に操作するハンズオンラボが充実しているため、時間に余裕があるならばGoogle Cloudの操作に慣れるという意味ではとても役に立つ教材だと思いました。ただ、こちらの教材は一部古いサービス名のままであったり、ハンズオンラボの操作指示が古いUIを元になっていて今現在のUIとの乖離があったりと自分で読み替えて実施する必要があるので注意は必要でした。\n終わりに # サービスの種類が少ないからかAWSのSAP資格と比較すると断然楽に感じました。また、Azure資格のように実際のUI操作順序に関する問題もないので画面操作になれていなくても何も問題ありません。無償教材も豊富なので教材に困ることもないと思います。\nただひとつ難点を挙げると、AWSやAzureの試験とは違って日本語と英語の切り替えができません。日本語の怪しい箇所があってもその正誤を確認する術がないというのがイケてないポイントだなと思いました。\nとはいえ、上級資格の中では比較的取りやすい部類に入ってくると思うので、ぜひ狙ってみてはいかがでしょうか。\n","date":"November 1, 2020","permalink":"/posts/2020/11/gcp-professional-cloud-architect-exam/","section":"記事一覧","summary":"11月頃から Google Cloud を業務で扱う予定ができたことと、とくに段階を踏まずに最初から上級資格を狙えるということで、Professional Cloud Architect を 2020/10/30 に受験してきました。その時のメモです。","title":"Google Cloud Certified Professional Cloud Architect 受験メモ"},{"content":"","date":"October 30, 2020","permalink":"/tags/qwiklabs/","section":"タグ一覧","summary":"","title":"Qwiklabs"},{"content":"Google Cloud Certified Professional Cloud Architect 資格取得に向けた学習の一環で、Google Courses powered by Qwiklabs の「Cloud Architecture: Design, Implement, and Manage」のチャレンジラボクエストに挑戦しました。チャレンジラボの中にはクリア条件が不明瞭で手こずったラボもありましたので同様に詰まっている方の助けになればいいなぁと思います。\nQwiklabs クエストを攻略する # 次の 7 つのチャレンジラボすべてをクリアするとクエスト攻略となります。\nNo. チャレンジラボ名 1 Google Cloud の基本スキル: チャレンジラボ（GSP101） 2 リモート起動スクリプトを使用した Compute インスタンスのデプロイ（GSP301） 3 Deployment Manager を使用したファイアウォールと起動スクリプトの構成（GSP302） 4 Windows の要塞ホストを使用したセキュアな RDP の構成（GSP303） 5 Windows の要塞ホストを使用したセキュアな RDP の構成（GSP303） 6 Kubernetes クラスタでのコンテナ化されたアプリケーションのスケールアウトと更新（GSP305） 7 MySQL データベースの Google Cloud SQL への移行（GSP306） Lab1. Google Cloud の基本スキル: チャレンジラボ（GSP101） # 「GSP101」は、Compute Engine の VM インスタンスを作成して、手動でゲスト OS 上に apache2 をインストールするだけのラボです。苦労するところはとくにないかな、と。\n解答 Lab2. リモート起動スクリプトを使用した Compute インスタンスのデプロイ（GSP301） # 「GSP301」は、前のラボと同じ内容を起動スクリプトを使って行うラボです。具体的には、Compute Engine の VM インスタンスを作成する際に、startup-script-url オプションにスクリプトを配置した Cloud Storage の URL を指定させて自動で WEB サーバを構築します。\n注意点としては、スクリプトを格納した Cloud Storage へアクセスできるように Compute Engine へ権限付与するのを忘れずに、です。\n解答 Lab3. Deployment Manager を使用したファイアウォールと起動スクリプトの構成（GSP302） # 「GSP302」は、Deployment Manager を使って Compute Engine の VM インスタンスと Firewall をデプロイするラボです。\n筆者は、最後のチェックポイント「Check that Deployment manager includes startup script and firewall resources」がなかなか条件を満たせずに苦労しました。いや、環境自体はデプロイ成功して、外部から WEB アクセスも通っているんですがクリア扱いにならないという…。\n結論としては、.jinja ファイルに上記 2 つのリソース定義を含めろよって言っていたようで、Firewall のリソース定義を.yaml から.jinja ファイルへ持っていく必要がありました。（逆に.jinja の VM インスタンス定義を.yaml へ移動でもよかったのかも）\n解答 Lab4. Windows の要塞ホストを使用したセキュアな RDP の構成（GSP303） # 「GSP303」は、踏み台サーバ（Bastion）を用意してその踏み台を経由してサーバをデプロイしていくラボです。\n文中にリージョンを制限される記載はないものの東日本リージョンではサブネット構築のチェックポイントをクリアできず、米国リージョンに変更する必要がありました。筆者は us-central1 を選択しました。\nちなみに、ラボを進めるにあたり GCP 上の VM インスタンスへの RDP 接続が必要になりますのでプロキシ環境化のクライアントからの実施はやや厳しいかもしれないです。\n解答 Lab5. Kubernetes クラスタへの Docker イメージのビルドとデプロイ（GSP304） # 「GSP304」は、Docker イメージのビルド、Container Registory へのイメージプッシュ、Kubernetes クラスタにアプリをデプロイという一覧の操作をしていくラボです。\nContainer Registory にプッシュする際はイメージ名にgcr.io/[PROJECT ID]/[IMAGE NAME]:[TAG NAME]とつける点に注意しましょう。\nk8s へのアプリデプロイは全部コマンドを手打ちし終わってから気づきましたが、manifests ディレクトリにサンプル yaml が入っていたのでこちらを編集してkubectl create -fしてもよかったのかもしれませんね。\n解答 Lab6. Kubernetes クラスタでのコンテナ化されたアプリケーションのスケールアウトと更新（GSP305） # 「GSP305」は、k8s へデプロイされているアプリのコンテナイメージ更新およびポッド数の変更操作をしていくラボです。\n操作については特筆する点はないかと思います。イメージ変更やレプリカ数の変更が簡単に Cloud Console（GUI）でもできてしまうのは他のメガクラウドと比べて GCP の優秀なところですね。\n解答 Lab7. MySQL データベースの Google Cloud SQL への移行（GSP306） # 「GSP306」は、Google Cloud SQL デプロイ、既存 MySQL サーバからのデータ移行、アプリで利用するデータベース切替の一連操作をしていくラボです。\nデータ移行のところは mysqldump コマンドでデータベースをエクスポートして Cloud Storage にアップロード、Cloud Console（GUI）を使って SQL へデータをインポートしていきましょう。それ以外の操作はチェックポイントの条件を満たしていけば特筆しなくてもクリアできるかと思います。\n解答 終わりに # やはりクラウドは触って試してみるのがお勉強として一番効率いいと思いますので、ぜひ皆さんも触ってみてください。いやーしかし、GCP もそうなんですがメガクラウドベンダーから提供されている教材は充実していますね。これらが無償で受けられるとは…まいったね、こりゃ。\nしかも、Google さんが提供するパートナー認定資格キックスタートプログラムを通じて、今回紹介した「Cloud Architecture: Design, Implement, and Manage」と、その前段のクエスト「Cloud Architecture」の 2 つをクリアすると Professional Cloud Architect 認定資格試験の無料バウチャーまでもらえます。太っ腹だなぁ＾＾；\n","date":"October 30, 2020","permalink":"/posts/2020/10/qwiklabs-quest-google-cloud-architecture/","section":"記事一覧","summary":"Google Cloud Certified Professional Cloud Architect 資格取得に向けた学習の一環で、Google Courses powered by Qwiklabs の「Cloud Architecture: Design, Implement, and Manage」のチャレンジラボクエストに挑戦しました。チャレンジラボの中にはクリア条件が不明瞭で手こずったラボもありましたので同様に詰まっている方の助けになればいいなぁと思います。","title":"Qwiklabs クエストの Cloud Architecture: Design, Implement, and Manage を攻略する"},{"content":"Azure を業務で扱い出して約2週間が経ったこともあり、そろそろ上級資格を狙ってみようと Microsoft Certified: DevOps Engineer Expert (AZ-400) を 2020/9/25 に受験してきました。その時のメモです。\n受験メモ # 受験者情報 # AWSを業務で扱いだして半年弱、Azureは約2週間、AWS認定資格的には対外的にそれっぽく見えてきたクラウドエンジニアです。\nNo. 項目 取得数 取得済み資格/合格済み試験 1 AWS認定資格 4 SAA、SAP、DOP、SCS 2 Azure認定資格 2 AZ-900、AZ-104 結果 # 777点 (合格)\n試験対策例 # No. 対策項目 実施時間(例) 1 Microsoft Learnで勉強する 約2時間 × 3日間 アドバイスなど # App InsightsやAMonitorを使ったアプリのトラシュー関連の問題をよく見た一方で、Azure DevOpsサービスに関する問いは少なかった印象です。一般的なCI/CDツールに関する問いも散見されました。デプロイ方式（B/G、Canaryなど）は概要さえ押さえておけばOKだったように思います。\nDevOpsの試験だから、おそらくAzure DevOpsだけ勉強していけば大丈夫だろうと油断していると足元を救われると思います。基本中の基本ですが、憶測だけで適当な範囲を勉強していくのではなく、認定資格の画面からダウンロードできる「認定資格スキルのアウトライン」から出題範囲や配点割合をちゃんと確認してから勉強をすることをオススメします。\nなお、MS Learn教材をひととおりやればある程度の知識は身に付くと思いますが、一般的にCI/CDで使われるツール（Git、Subversion、Maven、SonarQube、Jenkins、jMeter、Selenium、Terraform、Ansibleなど）の概要くらいは頭に入れておきましょう。AWSと違ってAzure固有のサービスに関する知識ではなく一般知識を問われる印象です。\n終わりに # 本当はAzure Solution Architect Expertの方を取るべき立場なのですが正直試験を2つも受けるのがやや面倒だったということもあり、手っ取り早く上級資格保有者になるためDevOps Engineer Expertを選択しました^^;\nいやー、DevOpsというくらいなのでおそらくAzure DevOpsサービスについての設問ばかりであろうと勝手な憶測の元、Azure DevOpsの勉強のみをして本試験に挑んでしまったのは完全に失敗でしたね。\nフタを開けてみればAzure DevOpsに関する出題割合は1割くらいしかなく、変な汗がいっぱい出て完全に負け戦の模様だったのですが、脳内にある知識をフル活用してなんとか合格をもぎ取ることができてとてもラッキーでした。\nとはいえ、このままではいずれ失敗をするので次回からは試験の出題範囲や配点割合をしっかり目を通してから勉強をすすめようと思う今日この頃でした^^; めでたし、めでたし。\n","date":"September 27, 2020","permalink":"/posts/2020/09/azure-az-400-exam/","section":"記事一覧","summary":"Azure を業務で扱い出して約2週間が経ったこともあり、そろそろ上級資格を狙ってみようと Microsoft Certified: DevOps Engineer Expert (AZ-400) を 2020/9/25 に受験してきました。その時のメモです。","title":"Microsoft Certified: DevOps Engineer Expert (AZ-400) 受験メモ"},{"content":"9月から Azure を業務で扱うことになったため、せめて中級の Azure Administrator Associate (AZ-104) くらいは取っておくか、と思って 2020/9/3 に受験してきました。その時のメモです。\n受験メモ # 受験者情報 # AWSを業務で扱いだして約4か月、Azureは初心者、AWS認定資格的には対外的にそれっぽく見えてきたクラウドエンジニアです。\nNo. 項目 取得数 取得済み資格/合格済み試験 1 AWS認定資格 4 SAA、SAP、DOP、SCS 2 Azure認定資格 1 AZ-900 結果 # 776点 (合格)\n試験対策例 # No. 対策項目 実施時間(例) 1 Microsoft主催のAZ-104研修を受講する 5日間 2 自身のない箇所をMicrosoft Learnで補習する 約2時間 × 2日間 アドバイスなど # 実機テストはなかったのですが操作手順を問われる場面もそこそこあったため実機は触っていた方が良いです。MS Learnでも無料で環境には触れるのでやっておくと吉です。リソース移行に関係する問いも多かった印象なので、リソースグループをまたいだリソース移行なんかの制限もしっかり頭に入れておきましょう。\nなお、集合研修は短期間でガっと知識を詰め込める反面、周りの受講者のペースが自分のペースと合わないと苦痛でしかないため、自分で学習を推進できる方はムリに集合研修を受ける必要はないと思います。私は合わなかったのでもう受講することはないかも、、、\n終わりに # 試験については、じっくりやっても1時間くらい余りました。とはいえ、自信満々で終わった、というよりは知らないと解けない問題が結構あったのでこれ以上考えてもムダという判断になるまでが早かった、という感じでしたが^^;\nちなみに、どうでもいい話なのですが、Azure試験はAWS試験と違って部分点がもらえるという非常に優しい仕様のため、部分点という甘い誘惑があるせいでより試験勉強に手を抜きたくなる衝動にかられましたｗ\n","date":"September 6, 2020","permalink":"/posts/2020/09/azure-az-104-exam/","section":"記事一覧","summary":"9月から Azure を業務で扱うことになったため、せめて中級の Azure Administrator Associate (AZ-104) くらいは取っておくか、と思って 2020/9/3 に受験してきました。その時のメモです。","title":"Microsoft Certified: Azure Administrator Associate (AZ-104) 受験メモ"},{"content":"先月SAPを受験したときに契約した問題集の利用期限が残っててもったいなかったため、AWS Certified Security - Specialty (SCS) を 2020/8/28 に受験してきたのでその時のメモです。\n受験メモ # 受験者情報 # AWSを業務で扱いだして約4か月、SAP/DOPの上級資格2冠を達成し、対外的にはそれっぽくなってきた駆け出しクラウドエンジニアです。\nNo. 項目 取得数 取得済み資格/合格済み試験 1 AWS認定資格 3 SAA、SAP、DOP 2 Azure認定資格 1 AZ-900 結果 # 846点 (合格)\n試験対策例 # No. 対策項目 実施時間(例) 1 合格者のブログを漁って勉強の計画などを立てる 約1時間 2 試験のアウトラインを読んで不安な部分を確認する 約1時間 3 問題集をやりこむ、間違った箇所の解説を読み込む 約1時間 × 2週間 アドバイスなど # Solution Architect Professional(SAP)やDevOps Engineer Professional(DOP)の勉強で活用した次の問題集を2周ほどやりこむことでひとまず合格はできました。似ている問題の出現率は結構低かった印象です。\nとはいえ、IAMのJSONを読ませる問題やキーマネジメント関連の問いがそこそこ本試験では出てきたので、問題集の解説を読んでIAM周りやキーマネジメントあたりの知識を蓄積していけば大丈夫だと思います。\n終わりに # SAPとDOP受けた後だからか、AWS WEB問題集の問題が初見で9割くらい解けたので簡単に感じました。60分くらい時間余りました。もっと試験準備の手を抜いても良かったかもしれません。\n","date":"August 30, 2020","permalink":"/posts/2020/08/aws-scs-c01-exam/","section":"記事一覧","summary":"先月SAPを受験したときに契約した問題集の利用期限が残っててもったいなかったため、AWS Certified Security - Specialty (SCS) を 2020/8/28 に受験してきたのでその時のメモです。","title":"AWS Certified Security - Specialty (SCS-C01) 受験メモ"},{"content":"先月SAPを受験したときに契約した問題集の利用期限が残っててもったいなかったため、AWS Certified DevOps Engineer - Professional (DOP) を 2020/8/17 に受験してきたのでその時のメモです。\n受験メモ # 受験者情報 # AWSを業務で扱いだして約3か月、先月のSAP取得でやっと脱・初級者を果たした駆け出しクラウドエンジニアです。\nNo. 項目 取得数 取得済み資格/合格済み試験 1 AWS認定資格 2 SAA、SAP 2 Azure認定資格 1 AZ-900 結果 # 880点 (合格)\n試験対策例 # No. 対策項目 実施時間(例) 1 合格者のブログを漁って勉強の計画などを立てる 約1時間 2 試験のアウトラインを読んで不安な部分を確認する 約1時間 3 問題集をやりこむ、間違った箇所の解説を読み込む 約2時間 × 2週間 アドバイスなど # 私はCode系サービスや各種PaaSでのB/GデプロイやCanaryの実装方式、監視系はザックリと頭に入れた上で、Solution Architect Professional(SAP)の勉強で活用した次の問題集を2、3周ほどやりこむことでひとまず合格はできました。\nなお、AWS試験の全体的な傾向として、基本的にAWSサービスのみで完結する、っていうのを念頭に置いておくといいかもしれませんね。\n終わりに # AWS WEB問題集の本試験モードというのを半分くらいやったところでいける雰囲気を感じたのでいざ試験へ、って感じでした。日ごろからCI/CDに慣れ親しんでたのと、SAPの勉強を直前にしていたこともあり個人的には楽な印象でした。\n点数が合格ラインよりもそこそこ上振ってしまったため、もう少し勉強時間を減らしても良かったかも、と反省しています。\n","date":"August 22, 2020","permalink":"/posts/2020/08/aws-dop-c01-exam/","section":"記事一覧","summary":"先月SAPを受験したときに契約した問題集の利用期限が残っててもったいなかったため、AWS Certified DevOps Engineer - Professional (DOP) を 2020/8/17 に受験してきたのでその時のメモです。","title":"AWS Certified DevOps Engineer - Professional (DOP-C01) 受験メモ"},{"content":"AWS を業務で扱いだして 2~3 か月くらい経ち、そろそろ脱・初級者の登竜門である AWS Certified Solution Architect - Professional (SAP) を取りに行かねばな、と思い立って 2020/7/31 に受験してきたときのメモです。\n受験メモ # 受験者情報 # AWSを業務で扱いだして3か月、保有資格は以下のみ、と非常にしょぼい感じです。\nNo. 項目 取得数 取得済み資格/合格済み試験 1 AWS認定資格 1 SAA 2 Azure認定資格 1 AZ-900 結果 # 808点（合格）\n試験対策例 # No. 対策項目 実施時間(例) 1 合格者のブログを漁って勉強の計画などを立てる 約1時間 2 試験のアウトラインを読んで不安な部分を確認する 約1時間 3 問題集をやりこむ、間違った箇所の解説を読み込む 約2時間 × 2週間 アドバイスなど # 私は合格者のブログで紹介されていた次の問題集をやりこむだけでひとまず合格はできました。こちらの問題集は数が多いのと、最初の方の問題は内容がやや古いので、自分は腹くくって#32以降を3週くらいしました。それでもクリアできました。\n個人的にはただ問題をやるのではなく、間違った問題の解説をよく読み、頭に入れるのが大切だったのかなと思います。\n終わりに # SAP試験は範囲が広くて個人的には結構難しかったと思います。使った教材(AWS WEB問題集)で見たことのある問題がそこそこ出てきたので、それで点数の底上げができて何とかギリギリ受かった感じです。\nとはいえ、合格は合格、これでやっと晴れて初心者/初級者からの卒業、モンスターハンターで言えばクック先生を一人で倒せたって感じでしょうかねｗ\nちなみに、試験で勉強したことは業務で100%活かせるか？と言われると、全然そんなことはないと思います。感覚的には2、3割くらいか使えれば御の字じゃないでしょうかね^^;\nどうせクラウドを扱っているとどんどん新しい技術が出てきて試験関係なく勉強することになります。そのため、これは試験勉強と割り切ってガっと短期間で知識を詰め込んで、試験をパパっとパスして本当に業務で必要な知識の習得に時間を割り当てるのが吉なのかな、っていうのが持論でございます。（単に低い点数の言い訳、とも言いますｗ）\n","date":"August 2, 2020","permalink":"/posts/2020/08/aws-sap-c01-exam/","section":"記事一覧","summary":"AWS を業務で扱いだして 2~3 か月くらい経ち、そろそろ脱・初級者の登竜門である AWS Certified Solution Architect - Professional (SAP) を取りに行かねばな、と思い立って 2020/7/31 に受験してきたときのメモです。","title":"AWS Certified Solution Architect - Professional (SAP-C01) 受験メモ"},{"content":"","date":"May 2, 2020","permalink":"/tags/aws-cli/","section":"タグ一覧","summary":"","title":"AWS CLI"},{"content":"","date":"May 2, 2020","permalink":"/tags/proxy/","section":"タグ一覧","summary":"","title":"Proxy"},{"content":"","date":"May 2, 2020","permalink":"/tags/windows/","section":"タグ一覧","summary":"","title":"Windows"},{"content":"みなさん、こんにちは。今回は社内プロキシ環境などから eksctl コマンドや kubectl コマンドを使って Amazon EKS を操作するためのクライアント側の設定方法を記載していきたいと思います。\nプロキシ環境下でクラウドベースの開発をしているエンジニアさんはプロキシに泣かされるってこと多いんじゃないかなと思います。いや、ほんとプロキシ関連の調査で時間をとられるのつらいですよね^^;\n設定手順 # まずは AWS CLI のセットアップをします # 1. AWS CLI をインストールします # AWS 公式ドキュメントの通りに MSI インストーラをダウンロード＆実行してください。\n2. AWS 認証情報を設定します # 次のコマンドを実行して認証情報を設定します。\nPS C:\\\u0026gt; aws configure 3. AWS CLI のプロキシ設定をします # 次のコマンドを実行してから設定を適用するために PowerShell アプリを起動しなおします。\nPS C:\\\u0026gt; setx HTTP_PROXY http://\u0026lt;proxy username\u0026gt;:\u0026lt;proxy password\u0026gt;@\u0026lt;proxy hostname or ip address\u0026gt;:\u0026lt;proxy port\u0026gt; PS C:\\\u0026gt; setx HTTPS_PROXY http://\u0026lt;proxy username\u0026gt;:\u0026lt;proxy password\u0026gt;@\u0026lt;proxy hostname or ip address\u0026gt;:\u0026lt;proxy port\u0026gt; 次に EKS CLI ツールのセットアップをします # 次にパッケージマネージャの Chocolatey を使って eksctl や kubectl などをインストールしていきましょう。\n1. Chocolatey をインストールします # 公式サイトからインストールスクリプトをダウンロードし、PowerShell を管理者として起動して環境変数を設定した上でインストールスクリプトを実行します。\nPS C:\\\u0026gt; $env:chocolateyProxyLocation = \u0026#34;\u0026lt;proxy hostname or ip address\u0026gt;:\u0026lt;proxy port\u0026gt;\u0026#34; PS C:\\\u0026gt; $env:chocolateyProxyUser = \u0026#34;\u0026lt;proxy username\u0026gt;\u0026#34; PS C:\\\u0026gt; $env:chocolateyProxyPassword = \u0026#34;\u0026lt;proxy password\u0026gt;\u0026#34; PS C:\\\u0026gt; Set-ExecutionPolicy RemoteSigned PS C:\\\u0026gt; .\\$downloadPath\\install.ps1 2. Chocolatey のプロキシ設定をします # 次のコマンドを実行してプロキシ設定をします。\nPS C:\\\u0026gt; chocolatey config set proxy \u0026#34;\u0026lt;proxy hostname or ip address\u0026gt;:\u0026lt;proxy port\u0026gt;\u0026#34; PS C:\\\u0026gt; chocolatey config set proxyUser \u0026#34;\u0026lt;proxy username\u0026gt;\u0026#34; PS C:\\\u0026gt; chocolatey config set proxyPassword \u0026#34;\u0026lt;proxy password\u0026gt;\u0026#34; 3. eksctl などをインストールします # 次のコマンドを実行して eksctl などをインストールしていきましょう。\nPS C:\\\u0026gt; chocolatey install -y eksctl aws-iam-authenticator kubernetes-cli ということでここまででひとまずセットアップは終了です。\nでは既存の EKS クラスタへの接続確認をしましょう # 1. まずは kubeconfig を作成します # 次のコマンドを実行して kubeconfig（kubectl 接続設定ファイル）を作成します。\nPS C:\\\u0026gt; aws eks --region $region update-kubeconfig --name $cluster $region、$cluster部分は自分の環境に沿った値へ変換して実行してください。 2. kubectl を実行してみよう # 次のコマンドなどを実行して情報が取得できたら終わりです。\nPS C:\\\u0026gt; kubectl get svc PS C:\\\u0026gt; kubectl get pods 「error: You must be logged in to the server (Unauthorized)」で kubectl がエラーになった場合は RABC ではじかれている可能性が高いです。スイッチロールするなりしてアクセスできる状態にした上で再実行してください。 終わりに # ということで、今回は Amazon EKS をプロキシ環境下の Windows 10 マシンから CLI 操作する方法でした。プロキシ環境で戦うエンジニアさんの力に少しでもなれれば幸いです。\n","date":"May 2, 2020","permalink":"/posts/2020/05/aws-eks-cli-through-proxy-from-windows10-client/","section":"記事一覧","summary":"みなさん、こんにちは。今回は社内プロキシ環境などから eksctl コマンドや kubectl コマンドを使って Amazon EKS を操作するためのクライアント側の設定方法を記載していきたいと思います。","title":"プロキシ環境下の Windows 10 マシンから Amazon EKS を CLI で操作する"},{"content":"","date":"May 1, 2020","permalink":"/tags/ssh/","section":"タグ一覧","summary":"","title":"SSH"},{"content":"","date":"May 1, 2020","permalink":"/tags/teraterm/","section":"タグ一覧","summary":"","title":"TeraTerm"},{"content":"みなさん、こんにちは。今回は社内プロキシ環境下から Amazon EC2 などの外部環境へ SSH でアクセスしたいといった場合の接続方法です。なお、今回は TeraTerm を利用した手順となっています。\n接続手順 # 1. TeraTerm をインストールする際に「TTProxy」にチェックして入れます\n2. TeraTerm を起動して「設定 \u0026gt; プロキシ」を選択します\n3. プロキシ情報を入力します\n4. あとは「ファイル \u0026gt; 新しい接続」からいつも通りアクセスすれば OK です\n終わりに # 今更感はありますが「プロキシ環境下の Windows 10 マシンから外部へ SSH でアクセスする方法」でした。ちなみに今のご時世、EC2 などへのアクセスであれば SSH ポートは開けずに System Manager セッションマネージャなどを使う、が正解だと思いますけどね。\n","date":"May 1, 2020","permalink":"/posts/2020/05/ssh-through-proxy-from-windows10-client/","section":"記事一覧","summary":"みなさん、こんにちは。今回は社内プロキシ環境下から Amazon EC2 などの外部環境へ SSH でアクセスしたいといった場合の接続方法です。なお、今回は TeraTerm を利用した手順となっています。","title":"プロキシ環境下の Windows 10 マシンから外部へ SSH でアクセスする"},{"content":"","date":"January 1, 1","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"}]