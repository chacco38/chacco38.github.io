[{"content":"みなさん、こんにちは。今回はさまざまなAWSサービスをKubernetesから管理できるようにするAWS Controllers for Kubernetes(ACK)のお話です。\nみなさんはAmazon EKSを活用してKubernetesクラスタをAWS上で動かすとなった際に、他のマネージドサービスの利用はどうされていますか。もちろんすべてKubernetes上で動かしてシステムを完結させるという選択肢もあるかと思いますが、やはり多くの方が他のAWSのマネージドサービスの併用も検討されるのではないでしょうか。その一方で、これら併用環境のコード化 (IaC、Infrastructure as Code) を実現しようとすると、Kubernetesアプリケーションの管理はHelm、AWSリソースの管理はTerraform、などという別々のツールでの管理になってしまいがちです。\nそんな悩みを解決する1つの手段がAWS Load Balancer ControllerやAWS Controllers for KubernetesといったKubernetesクラスタ機能を拡張する各種コントローラの活用です。これらのコントローラを利用することで、AWSリソースについてもKubernetesマニフェストファイルで定義できるようになり、Kubernetes側に運用管理を寄せてシンプル化することが可能です。\n今回はそのうちの1つ、さまざまなAWSサービスを管理できるようにするAWS Controllers for Kubernetesについて、簡単なサンプルを交えて紹介していきたいと思います。これからAmazon EKS上にアプリケーションを展開しようと考えている方は参考にしてみてはいかがでしょうか。\nAWS Controllers for Kubernetes(ACK)とは # AWS Controllers for Kubernetes(ACK)は、さまざまなAWSサービスをKubernetesクラスタから管理するためのKubernetes API拡張コントローラ群の総称です。このコントローラ群を活用することで、Kubernetesクラスタから直接AWSサービスの定義、作成を行うことが可能になり、アプリケーションとその依存関係にあるデータベース、メッセージキュー、オブジェクトストレージなどのマネージドサービスを含むすべてをKubernetesにて一元管理することが可能となります。なお、現時点のACKでは、次のAWSサービス向けコントローラがディベロッパープレビュー機能として利用可能となっています。\n Amazon API Gateway V2 Amazon Application Auto Scaling Amazon DynamoDB Amazon ECR Amazon EKS Amazon ElastiCache Amazon EC2 Amazon MQ Amazon OpenSearch Service Amazon RDS Amazon SageMaker Amazon SNS AWS Step Functions Amazon S3   ACKを導入してみよう # Step1. 作業環境の設定 # 今回はAmazon EKSやACKの管理を行う環境としてAWS CloudShellを利用していきたいと思います。まずは操作に必要な各種ツールの設定をAWS CloudShellにしてきましょう。\n#1. AWS CLIの設定 # AWSリソースの操作を行えるように次のコマンドを実行し、AWS CLIの設定を行いましょう。\n実行例）AWS CLIの設定\naws configure #2. kubectlコマンドのインストール # 次にKubernetes管理ツールのkubectlコマンドをインストールしましょう。\n 実行例）kubectlコマンドのインストール\n# kubectl コマンドのダウンロード curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.21.2/2021-07-05/bin/linux/amd64/kubectl # 実行権限の付与 chmod +x kubectl # 実行ファイルのパスを設定 mkdir -p ${HOME}/bin \u0026amp;\u0026amp; mv kubectl ${HOME}/bin \u0026amp;\u0026amp; export PATH=${PATH}:${HOME}/bin # シェルの起動時に $HOME/bin をパスへ追加 echo \u0026#39;export PATH=${PATH}:${HOME}/bin\u0026#39; \u0026gt;\u0026gt; ~/.bashrc # インストールが成功していることを確認 kubectl version --short --client インストールに成功していれば出力例のようにバージョン情報の出力を確認できます。\n出力例）\n$ kubectl version --short --client Client Version: v1.21.2-13+d2965f0db10712 #3. eksctlコマンドのインストール # 続いてAmazon EKS管理ツールのeksctlコマンドをインストールしましょう。\n 実行例）eksctlコマンドのインストール\n# eksctl の最新バージョンをダウンロード curl -L \u0026#34;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; | tar xz -C /tmp # 実行ファイルをパスの通ったディレクトリへ移動 mv /tmp/eksctl ${HOME}/bin # インストールが成功していることを確認 eksctl version インストールに成功していれば出力例のようにバージョン情報の出力を確認できます。\n出力例）\n$ eksctl version 0.83.0 #4. helmコマンドのインストール # 最後にKubernetes上で稼働するアプリケーションを管理するためのツールであるhelmコマンドをインストールしましょう。\n 実行例）helmコマンドのインストール\n# 前提パッケージのインストール sudo yum install -y openssl # インストールスクリプトのダウンロード curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 \u0026gt; get_helm.sh # 実行権限の付与 chmod 700 get_helm.sh # インストールスクリプトの実行 ./get_helm.sh # インストールが成功していることを確認 helm version --short インストールに成功していれば出力例のようにバージョン情報の出力を確認できます。\n出力例）\n$ helm version --short v3.8.0+gd141386 以上で作業環境(AWS CloudShell)の設定は完了です。\nStep2. EKSクラスタの作成 # 続いてACKを導入する対象のAmazon EKSクラスタを作成していきましょう。今回はあくまで検証なのでeksctlコマンドでサクッと作成していきましょう。\n 実行例）EKSクラスタの作成\n# 環境変数の設定 export CLUSTER=\u0026#34;matt-tokyo-cluster\u0026#34; # EKS クラスタの作成 eksctl create cluster --name ${CLUSTER} --version 1.21 # サービスアカウントでの IAM ロール使用を許可 eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER} --approve ここまで終わりましたらACKを利用するための事前準備は完了です。\nStep3. コントローラのデプロイ # それではACKをAmazon EKSクラスタにデプロイしていきましょう。Amazon ECRパブリックギャラリーにて各AWSサービスに対応するコントローラ用Helmチャートが公開されておりますのでこちらを利用してデプロイしていきたいと思います。\n #1. インストールスクリプトの作成 # ACKは各AWSサービスに対応するコントローラをそれぞれ導入し、サービスアカウントの設定をしていく必要があるのですが、公式ドキュメントを見るとかなり煩雑な手順になっていることがわかります。そこでコマンド1つでインストールできるようにスクリプト化してみました。\n    再実行を想定してない作りになっているのでご注意ください。  作成例）install.sh\n#!/bin/bash set -eux # 引数の確認 if [ $# -eq 0 ] \u0026amp;\u0026amp; [ -z \u0026#34;${SERVICE}\u0026#34; ]; then echo \u0026#34;Error: usage: ./install.sh \u0026lt;SERVICE_NAME\u0026gt;\u0026#34; exit 1 elif [ $# -eq 1 ] \u0026amp;\u0026amp; [ -n \u0026#34;$1\u0026#34; ]; then SERVICE=$1 fi # 環境変数の設定 export HELM_EXPERIMENTAL_OCI=1 CHART_EXPORT_PATH=\u0026#34;/tmp/chart\u0026#34; ACK_K8S_NAMESPACE=${ACK_K8S_NAMESPACE:-\u0026#34;ack-system\u0026#34;} ACK_SERVICE_CONTROLLER=\u0026#34;ack-${SERVICE}-controller\u0026#34; AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \u0026#34;Account\u0026#34; --output text) CLUSTER=${CLUSTER:-\u0026#34;my-tokyo-cluster\u0026#34;} OIDC_PROVIDER=$(aws eks describe-cluster --name ${CLUSTER} \\  --query \u0026#34;cluster.identity.oidc.issuer\u0026#34; --output text \\  | sed -e \u0026#34;s/^https:\\/\\///\u0026#34;) # 最新リリースバージョン名の取得 RELEASE_VERSION=$(curl -sL \\  https://api.github.com/repos/aws-controllers-k8s/${SERVICE}-controller/releases/latest \\  | grep \u0026#39;\u0026#34;tag_name\u0026#34;:\u0026#39; | cut -d\u0026#39;\u0026#34;\u0026#39; -f4) if [ -z \u0026#34;${RELEASE_VERSION}\u0026#34; ]; then # latestリリースが作成されていない場合は一覧から最新バージョンを取得 RELEASE_VERSION=$(curl -sL \\  https://api.github.com/repos/aws-controllers-k8s/${SERVICE}-controller/releases \\  | grep \u0026#39;\u0026#34;tag_name\u0026#34;:\u0026#39; | head -n 1 | cut -d\u0026#39;\u0026#34;\u0026#39; -f4) # リリースを何も作成されていない場合はv0.0.1を設定(SNSコントローラ向け) RELEASE_VERSION=${RELEASE_VERSION:-v0.0.1} fi # Helmチャートのダウンロードディレクトリの作成 mkdir -p ${CHART_EXPORT_PATH} # Helmチャートのダウンロード helm pull oci://public.ecr.aws/aws-controllers-k8s/${SERVICE}-chart \\  --version $RELEASE_VERSION -d ${CHART_EXPORT_PATH} # アーカイブの解凍 tar xvf ${CHART_EXPORT_PATH}/${SERVICE}-chart-${RELEASE_VERSION}.tgz \\  -C ${CHART_EXPORT_PATH} # 解凍後のパスチェック if [ -d ${CHART_EXPORT_PATH}/${SERVICE}-chart ]; then CHART_PATH=${CHART_EXPORT_PATH}/${SERVICE}-chart else # SNSコントローラ向け CHART_PATH=${CHART_EXPORT_PATH}/${ACK_SERVICE_CONTROLLER} fi # コントローラのインストール helm install --create-namespace ${ACK_SERVICE_CONTROLLER} \\  --namespace ${ACK_K8S_NAMESPACE} \\  --set aws.region=\u0026#34;${AWS_REGION}\u0026#34; ${CHART_PATH} # IAMロール定義ファイルの作成 cat \u0026lt;\u0026lt;EOF \u0026gt; /tmp/${ACK_SERVICE_CONTROLLER}.json { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Federated\u0026#34;: \u0026#34;arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_PROVIDER}\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;${OIDC_PROVIDER}:sub\u0026#34;: \u0026#34;system:serviceaccount:${ACK_K8S_NAMESPACE}:${ACK_SERVICE_CONTROLLER}\u0026#34; } } } ] } EOF # IAMロールの作成 aws iam create-role \\  --role-name ${ACK_SERVICE_CONTROLLER} \\  --assume-role-policy-document file:///tmp/${ACK_SERVICE_CONTROLLER}.json # 推奨IAMポリシーの設定 POLICY_ARN_STRINGS=$(curl -sL \\  https://raw.githubusercontent.com/aws-controllers-k8s/${SERVICE}-controller/main/config/iam/recommended-policy-arn) if [ \u0026#34;404: Not Found\u0026#34; != \u0026#34;${POLICY_ARN_STRINGS}\u0026#34; ]; then while IFS= read -r POLICY_ARN; do aws iam attach-role-policy \\  --role-name \u0026#34;${ACK_SERVICE_CONTROLLER}\u0026#34; \\  --policy-arn \u0026#34;${POLICY_ARN}\u0026#34; done \u0026lt;\u0026lt;\u0026lt; \u0026#34;${POLICY_ARN_STRINGS}\u0026#34; fi # 推奨IAMポリシーの設定(EKSコントローラ向け) INLINE_POLICY=$(curl -sL \\  https://raw.githubusercontent.com/aws-controllers-k8s/${SERVICE}-controller/main/config/iam/recommended-inline-policy) if [ \u0026#34;404: Not Found\u0026#34; != \u0026#34;${INLINE_POLICY}\u0026#34; ]; then aws iam put-role-policy \\  --role-name \u0026#34;${ACK_SERVICE_CONTROLLER}\u0026#34; \\  --policy-name \u0026#34;ack-recommended-policy\u0026#34; \\  --policy-document \u0026#34;${INLINE_POLICY}\u0026#34; fi # サービスアカウントにIAMロールを関連付け kubectl -n ${ACK_K8S_NAMESPACE} annotate serviceaccount \\  ${ACK_SERVICE_CONTROLLER} \\  eks.amazonaws.com/role-arn=$(aws iam get-role \\  --role-name=${ACK_SERVICE_CONTROLLER} --query Role.Arn --output text) # Deploymentリソースを再起動 kubectl -n ${ACK_K8S_NAMESPACE} rollout restart deployment \\  $(kubectl -n ${ACK_K8S_NAMESPACE} get deployment \\  -o custom-columns=Name:metadata.name --no-headers \\  | grep ${ACK_SERVICE_CONTROLLER}) #2. コントローラのインストール # それではコントローラを導入していきたいと思います。本記事では例としてAmazon Simple Storage Service(S3)サービスに対応したコントローラをインストールしていきたいと思います。\n実行例）コントローラのインストール(S3の場合)\n# 環境変数の設定 export SERVICE=\u0026#34;s3\u0026#34; export ACK_K8S_NAMESPACE=\u0026#34;ack-system\u0026#34; export ACK_SERVICE_CONTROLLER=\u0026#34;ack-${SERVICE}-controller\u0026#34; # コントローラのインストール bash install.sh # 確認 helm list -n ${ACK_K8S_NAMESPACE} -o yaml -f ${ACK_SERVICE_CONTROLLER} インストールに成功していれば出力例のようにコントローラが表示されるようになります。\n出力例）\n$ helm list -n ${ACK_K8S_NAMESPACE} -o yaml -f ${ACK_SERVICE_CONTROLLER} - app_version: v0.0.13 chart: s3-chart-v0.0.13 name: ack-s3-controller namespace: ack-system revision: \u0026#34;1\u0026#34; status: deployed updated: 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC なお、今回紹介したS3以外のコントローラを導入する際は、環境変数SERVICEの値を各 AWS サービスに対応する文字列へ置換することでインストールすることが可能です。各サービスに対応する文字列は次の通りです。\n   AWS サービス名 SERVICE 値     Amazon API Gateway V2 apigatewayv2   Amazon Application Auto Scaling applicationautoscaling   Amazon DynamoDB dynamodb   Amazon ECR ecr   Amazon EKS eks   Amazon ElastiCache elasticache   Amazon EC2 ec2   Amazon MQ mq   Amazon OpenSearch Service opensearchservice   Amazon RDS rds   Amazon SageMaker sagemaker   Amazon SNS sns   AWS Step Functions sfn   Amazon S3 s3    ちなみに、全部入れるとこのような感じでコントローラだらけになってしまいます^^;\n出力例）\n$ helm list -n ${ACK_K8S_NAMESPACE} NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION ack-apigatewayv2-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed apigatewayv2-chart-v0.0.15 v0.0.15 ack-applicationautoscaling-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed applicationautoscaling-chart-v0.2.4 v0.2.4 ack-dynamodb-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed dynamodb-chart-v0.0.14 v0.0.14 ack-ec2-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed ec2-chart-v0.0.7 v0.0.7 ack-ecr-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed ecr-chart-v0.0.19 v0.0.19 ack-eks-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed eks-chart-v0.0.8 v0.0.8 ack-elasticache-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed elasticache-chart-v0.0.14 v0.0.14 ack-mq-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed mq-chart-v0.0.12 v0.0.12 ack-opensearchservice-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed opensearchservice-chart-v0.0.9 v0.0.9 ack-rds-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed rds-chart-v0.0.17 v0.0.17 ack-s3-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed s3-chart-v0.0.13 v0.0.13 ack-sagemaker-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed sagemaker-chart-v0.3.0 v0.3.0 ack-sfn-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed sfn-chart-v0.0.11 v0.0.11 ack-sns-controller ack-system 1 2022-02-xx xx:xx:xx.xxxxxxxxx +0000 UTC deployed ack-sns-controller-v0.0.1 v0.0.1 ACKを使ってみよう # 今回は一部のAWSサービス向けコントローラを簡単なサンプルを用いて使い方を紹介していきたいと思います。すべてのコントローラの使い方については公式ドキュメントを参照ください。\n Amazon ECRコントローラの使い方 # ACK環境では、次のサンプルのようにKubernetes Repositoryリソースを定義をすることによってECRリポジトリをプロビジョニングできます。定義可能なスペックの詳細については次の公式リファレンスドキュメントを参照ください。\n 作成例）repository.yaml (ECRリポジトリリソース定義サンプル)\napiVersion:ecr.services.k8s.aws/v1alpha1kind:Repositorymetadata:name:\u0026#34;matt-ecr-repository\u0026#34;spec:name:\u0026#34;matt-ecr-repository\u0026#34;imageScanningConfiguration:scanOnPush:trueそれでは次のコマンドを実行してサンプルECRリポジトリリソースをデプロイしていきましょう。\n実行例）サンプルECRリポジトリのデプロイ\nexport NAMESPACE=\u0026#34;sample\u0026#34; # サンプルECRリポジトリ用Namespaceの作成 kubectl create namespace ${NAMESPACE} # サンプルECRリポジトリのデプロイ kubectl apply -n ${NAMESPACE} -f repository.yaml # サンプルECRリポジトリがデプロイされたことを確認 kubectl get -n ${NAMESPACE} repository ECRリポジトリリソースのデプロイに成功した場合は次の出力例のようにRepositoryリソース一覧に表示されます。\n出力例）\n$ kubectl get -n ${NAMESPACE} repository NAME AGE matt-ecr-repository 10s 念のため、AWS CLIを実行してECRリポジトリが作成されているかを確認してみましょう。\n実行例）ECRリポジトリの確認\naws ecr describe-repositories --repository-name matt-ecr-repository AWS CLIでもECRリポジトリが確認できたら成功です。\n出力例）\n$ aws ecr describe-repositories --repository-name matt-ecr-repository repositories: - createdAt: \u0026#39;2022-02-xxTxx:xx:xx+00:00\u0026#39; encryptionConfiguration: encryptionType: AES256 imageScanningConfiguration: scanOnPush: true imageTagMutability: MUTABLE registryId: \u0026#39;xxxxxxxxxxxx\u0026#39; repositoryArn: arn:aws:ecr:ap-northeast-1:xxxxxxxxxxxx:repository/matt-ecr-repository repositoryName: matt-ecr-repository repositoryUri: xxxxxxxxxxxx.dkr.ecr.ap-northeast-1.amazonaws.com/matt-ecr-repository 最後にサンプルを削除して動作確認は終了です。お疲れ様でした。\n実行例）サンプルの削除\nkubectl delete namespace ${NAMESPACE} Amazon S3コントローラの使い方 # ACK環境では、次のサンプルのようにKubernetes Bucketリソースを定義をすることによってS3バケットをプロビジョニングできます。定義可能なスペックの詳細については次の公式リファレンスドキュメントを参照ください。\n 作成例）bucket.yaml (S3バケットリソース定義サンプル)\napiVersion:s3.services.k8s.aws/v1alpha1kind:Bucketmetadata:name:\u0026#34;matt-s3-bucket\u0026#34;spec:name:\u0026#34;matt-s3-bucket\u0026#34;publicAccessBlock:blockPublicACLs:trueblockPublicPolicy:trueversioning:status:Enabledencryption:rules:- bucketKeyEnabled:falseapplyServerSideEncryptionByDefault:sseAlgorithm:AES256それでは次のコマンドを実行してサンプルS3バケットリソースをデプロイしていきましょう。\n実行例）サンプルS3バケットのデプロイ\nexport NAMESPACE=\u0026#34;sample\u0026#34; # サンプルS3バケット用Namespaceの作成 kubectl create namespace ${NAMESPACE} # サンプルS3バケットのデプロイ kubectl apply -n ${NAMESPACE} -f bucket.yaml # サンプルS3バケットがデプロイされたことを確認 kubectl get -n ${NAMESPACE} bucket S3バケットリソースのデプロイに成功した場合は次の出力例のようにBucketリソース一覧に表示されます。\n出力例）\n$ kubectl get -n ${NAMESPACE} bucket NAME AGE matt-s3-bucket 1m22s 念のため、AWS CLIを実行してS3バケットが作成されているかを確認してみましょう。\n実行例）S3バケットが作成されているかを確認\naws s3 ls | grep \u0026#34;matt-s3-bucket\u0026#34; AWS CLIでもS3バケットが確認できたら成功です。\n出力例）\n$ aws s3 ls | grep \u0026#34;matt-s3-bucket\u0026#34; 2022-02-xx xx:xx:xx matt-s3-bucket 最後にサンプルを削除して動作確認は終了です。お疲れ様でした。\n実行例）サンプルの削除\nkubectl delete namespace ${NAMESPACE} 終わりに # 今回はさまざまなAWSサービスをAmazon Elastic Kubernetes Service(EKS)上で管理できるようにするAWS Controllers for Kubernetesのご紹介でしたがいかがだったでしょうか。\n現時点ではディベロッパープレビュー機能のためプロダクション用途での活用はまだまだオススメできませんが、もしAWS Controllers for Kubernetesの活用を検討されている方は参考にしていただければ幸いです。\n以上、さまざまなAWSサービスをKubernetesから管理できるようにする「AWS Controllers for Kubernetes」のご紹介でした。\n  AWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 Kubernetes は、The Linux Foundation の米国およびその他の国における登録商標または商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。  ","date":"February 16, 2022","permalink":"/posts/2022/02/aws-controllers-for-kubernetes/","section":"記事一覧","summary":"みなさん、こんにちは。今回はさまざまなAWSサービスをKubernetesから管理できるようにするAWS Controllers for Kubernetes(ACK)のお話です。","title":"AWS Controllers for Kubernetesを使って各種AWSサービスをマニフェストファイルで管理しよう"},{"content":"みなさん、こんにちは。以前に「複数リージョンのGKEクラスタとAnthos Service Meshでマルチクラスタメッシュ環境を構築してみた」という記事を書いたのですが、今回はその環境をTerraformを使って構築してみました。もしこれから「ASM環境をTerraformで」と検討している方は参考にしてみてはいかがでしょうか。\nとはいえ、本記事の執筆時点(2022年1月末)ではTerraform公式モジュールがASMのv1.11以降に対応しておらず正直使いモノにならなかったこともあり、やや苦しい実装になってしまっています。素直にASMの導入以降はTerraform以外を使うのが良いかと思いますが、あくまで本記事はご参考ということでその点ご承知おきいただけると幸いです。\n構築するシステムについて # 次の図に示すように限定公開クラスを有効化した複数リージョンのGKEクラスタに対してAnthos Service Mesh(マネージドコントロールプレーン)を導入した環境となっています。なお、アプリケーションのコンテナについてはインフラとは異なるリポジトリで管理するのが一般的かと思うので今回は除外しています。\n   Terraformのサンプルコードを書いてみた # それでは今回作成したTerraformのサンプルコードを紹介していきたいと思います。まずはディレクトリ構造ですが、今回はenvironmentsディレクトリ配下へ環境ごとにサブディレクトリを作成し、Workspaceは使わずに別ファイルとして管理する形を想定した作りにしてます。\n作成例）ディレクトリ構成\n. |-- environments | `-- poc | |-- backend.tf | |-- main.tf | `-- variables.tf `-- modules |-- networks | |-- main.tf | |-- variables.tf | `-- outputs.tf |-- gke | |-- main.tf | |-- variables.tf | `-- outputs.tf `-- asm |-- main.tf |-- variables.tf |-- scripts | |-- install.sh | |-- destroy.sh | `-- create-mesh.sh `-- manifests |-- istio-ingressgateway-pods | |-- namespace.yaml | |-- deployment.yaml | |-- serviceaccount.yaml | `-- role.yaml `-- istio-ingressgateway-services |-- multiclusterservice.yaml |-- backendconfig.yaml `-- multiclusteringress.yaml    # ファイル名 概要     1 environments/poc/backend.tf PoC環境のtfstateファイル保存先定義   2 environments/poc/main.tf PoC環境の定義   3 environments/pod/variables.tf PoC環境の外部変数定義   4 modules/networks/main.tf ネットワーク設定用モジュールの定義   5 modules/networks/variables.tf ネットワーク設定用モジュールの外部変数定義   6 modules/networks/outputs.tf ネットワーク設定用モジュールのアウトプット定義   7 modules/gke/main.tf GKE設定用モジュールの定義   8 modules/gke/variables.tf GKE設定用モジュールの外部変数定義   9 modules/gke/outputs.tf GKE設定用モジュールのアウトプット定義   10 modules/asm/main.tf ASM設定用モジュールの定義   11 modules/asm/variables.tf ASM設定用モジュールの外部変数定義   12 modules/asm/scripts/install.sh ASMのインストールスクリプト   13 modules/asm/scripts/destroy.sh ASMのアンインストールスクリプト   14 modules/asm/scripts/create-mesh.sh ASMのマルチクラスタメッシュ作成スクリプト   15 modules/asm/manifests/istio-ingressgateway-pods/* Istio IngressGatewayコンテナのKubernetesマニフェストファイル群   16 modules/asm/manifests/istio-ingressgateway-services/* Istio IngressGatewayサービスのKubernetesマニフェストファイル群    PoC環境定義 # environments/poc/backend.tf # PoC環境のtfstateファイルをGoogle Cloud Storage(GCS)上で管理するための設定を定義しています。\n作成例）./environments/poc/backend.tf\nterraform { backend \u0026#34;gcs\u0026#34; { bucket = \u0026#34;matt-gcs-tfstate\u0026#34; prefix = \u0026#34;multi-asm-poc\u0026#34; } } environments/poc/main.tf # PoC環境の定義をしています。実際の処理はモジュール側で定義しており、このファイルではPoC環境固有の設定値定義がメインの役割となっています。\n作成例）./environments/poc/main.tf\nlocals { network = \u0026#34;matt-vpc\u0026#34; tokyo_subnet = \u0026#34;matt-tokyo-priv-snet\u0026#34; tokyo_subnet_ip_range = \u0026#34;172.16.0.0/16\u0026#34; tokyo_router = \u0026#34;matt-tokyo-router\u0026#34; tokyo_nat = \u0026#34;matt-tokyo-nat\u0026#34; osaka_subnet = \u0026#34;matt-osaka-priv-snet\u0026#34; osaka_subnet_ip_range = \u0026#34;172.24.0.0/16\u0026#34; osaka_router = \u0026#34;matt-osaka-router\u0026#34; osaka_nat = \u0026#34;matt-osaka-nat\u0026#34; tokyo_cluster = \u0026#34;matt-tokyo-cluster-1\u0026#34; tokyo_master_ip_range = \u0026#34;192.168.0.0/28\u0026#34; tokyo_pod_ip_range = \u0026#34;10.16.0.0/14\u0026#34; tokyo_service_ip_range = \u0026#34;10.20.0.0/20\u0026#34; osaka_cluster = \u0026#34;matt-osaka-cluster-1\u0026#34; osaka_master_ip_range = \u0026#34;192.168.8.0/28\u0026#34; osaka_pod_ip_range = \u0026#34;10.32.0.0/14\u0026#34; osaka_service_ip_range = \u0026#34;10.36.0.0/20\u0026#34; } module \u0026#34;networks\u0026#34; { source = \u0026#34;../../modules/networks\u0026#34; project_id = var.project_id network = local.network tokyo_subnet = local.tokyo_subnet tokyo_subnet_ip_range = local.tokyo_subnet_ip_range tokyo_subnet_2nd_ip_range_1 = local.tokyo_pod_ip_range tokyo_subnet_2nd_ip_range_2 = local.tokyo_service_ip_range tokyo_router = local.tokyo_router tokyo_nat = local.tokyo_nat osaka_subnet = local.osaka_subnet osaka_subnet_ip_range = local.osaka_subnet_ip_range osaka_subnet_2nd_ip_range_1 = local.osaka_pod_ip_range osaka_subnet_2nd_ip_range_2 = local.osaka_service_ip_range osaka_router = local.osaka_router osaka_nat = local.osaka_nat } module \u0026#34;gke\u0026#34; { source = \u0026#34;../../modules/gke\u0026#34; project_id = var.project_id network = module.networks.network tokyo_cluster = local.tokyo_cluster tokyo_subnet = local.tokyo_subnet tokyo_master_ip_range = local.tokyo_master_ip_range osaka_cluster = local.osaka_cluster osaka_subnet = local.osaka_subnet osaka_master_ip_range = local.osaka_master_ip_range } module \u0026#34;asm\u0026#34; { source = \u0026#34;../../modules/asm\u0026#34; project_id = var.project_id network = module.networks.network tokyo_cluster = module.gke.tokyo_cluster tokyo_pod_ip_range = local.tokyo_pod_ip_range osaka_cluster = module.gke.osaka_cluster osaka_pod_ip_range = local.osaka_pod_ip_range } environments/pod/variables.tf # terraform plan/apply コマンド実行時に -var=\u0026quot;project_id=${PROJECT_ID}\u0026quot; のような形で外部から与える変数を定義しています。\n作成例）./environments/poc/variables.tf\nvariable \u0026#34;project_id\u0026#34; {} ネットワークモジュール定義 # modules/networks/main.tf # ネットワーク設定としてVPCおよびCloud NATの定義をしています。今回の例ではTerraform公式モジュールを活用してみました。\n作成例）./modules/networks/main.tf\nmodule \u0026#34;vpc\u0026#34; { source = \u0026#34;terraform-google-modules/network/google\u0026#34; version = \u0026#34;4.1.0\u0026#34; description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/network/google/4.1.0\u0026#34; project_id = var.project_id network_name = var.network shared_vpc_host = false subnets = [ { subnet_name = var.tokyo_subnet subnet_ip = var.tokyo_subnet_ip_range subnet_region = \u0026#34;asia-northeast1\u0026#34; subnet_private_access = true }, { subnet_name = var.osaka_subnet subnet_ip = var.osaka_subnet_ip_range subnet_region = \u0026#34;asia-northeast2\u0026#34; subnet_private_access = true } ] secondary_ranges = { (var.tokyo_subnet) = [ { range_name = \u0026#34;${var.tokyo_subnet}-pods\u0026#34; ip_cidr_range = var.tokyo_subnet_2nd_ip_range_1 }, { range_name = \u0026#34;${var.tokyo_subnet}-services\u0026#34; ip_cidr_range = var.tokyo_subnet_2nd_ip_range_2 }, ] (var.osaka_subnet) = [ { range_name = \u0026#34;${var.osaka_subnet}-pods\u0026#34; ip_cidr_range = var.osaka_subnet_2nd_ip_range_1 }, { range_name = \u0026#34;${var.osaka_subnet}-services\u0026#34; ip_cidr_range = var.osaka_subnet_2nd_ip_range_2 }, ] } } module \u0026#34;cloud_router_tokyo\u0026#34; { source = \u0026#34;terraform-google-modules/cloud-router/google\u0026#34; version = \u0026#34;1.3.0\u0026#34; description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/cloud-router/google/1.3.0\u0026#34; name = var.tokyo_router project = var.project_id region = \u0026#34;asia-northeast1\u0026#34; network = module.vpc.network_name nats = [{ name = var.tokyo_nat }] } module \u0026#34;cloud_router_osaka\u0026#34; { source = \u0026#34;terraform-google-modules/cloud-router/google\u0026#34; version = \u0026#34;1.3.0\u0026#34; description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/cloud-router/google/1.3.0\u0026#34; name = var.osaka_router project = var.project_id region = \u0026#34;asia-northeast2\u0026#34; network = module.vpc.network_name nats = [{ name = var.osaka_nat }] } modules/networks/variables.tf # ネットワークモジュールの外部変数を定義しています。\n作成例）./modules/networks/variables.tf\nvariable \u0026#34;project_id\u0026#34; {} variable \u0026#34;network\u0026#34; {} variable \u0026#34;tokyo_subnet\u0026#34; {} variable \u0026#34;tokyo_subnet_ip_range\u0026#34; {} variable \u0026#34;tokyo_subnet_2nd_ip_range_1\u0026#34; {} variable \u0026#34;tokyo_subnet_2nd_ip_range_2\u0026#34; {} variable \u0026#34;tokyo_router\u0026#34; {} variable \u0026#34;tokyo_nat\u0026#34; {} variable \u0026#34;osaka_subnet\u0026#34; {} variable \u0026#34;osaka_subnet_ip_range\u0026#34; {} variable \u0026#34;osaka_subnet_2nd_ip_range_1\u0026#34; {} variable \u0026#34;osaka_subnet_2nd_ip_range_2\u0026#34; {} variable \u0026#34;osaka_router\u0026#34; {} variable \u0026#34;osaka_nat\u0026#34; {} modules/networks/outputs.tf # ネットワークモジュールの出力変数を定義しています。\n作成例）./modules/networks/outputs.tf\noutput \u0026#34;network\u0026#34; { value = module.vpc.network_name } GKEモジュール定義 # modules/gke/main.tf # 東京/大阪リージョンのGKEクラスタを定義しています。こちらもネットワークモジュール同様にTerraform公式モジュールを活用してみました。\n   記事執筆時点(2022年1月末)では、コントロールプレーンのグローバルアクセスを有効化するオプションがTerraform公式private-clusterサブモジュールv19.0.0(latest)になかったため、Terraform公式beta-private-clusterサブモジュールv19.0.0(latest)を活用しています。  作成例）./modules/gke/main.tf\nmodule \u0026#34;gke_tokyo\u0026#34; { source = \u0026#34;terraform-google-modules/kubernetes-engine/google//modules/beta-private-cluster\u0026#34; version = \u0026#34;19.0.0\u0026#34; description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/kubernetes-engine/google/19.0.0/submodules/beta-private-cluster\u0026#34; project_id = var.project_id name = var.tokyo_cluster region = \u0026#34;asia-northeast1\u0026#34; network = var.network subnetwork = var.tokyo_subnet ip_range_pods = \u0026#34;${var.tokyo_subnet}-pods\u0026#34; ip_range_services = \u0026#34;${var.tokyo_subnet}-services\u0026#34; enable_private_endpoint = false enable_private_nodes = true master_global_access_enabled = true master_ipv4_cidr_block = var.tokyo_master_ip_range release_channel = var.release_channel node_pools = [{ name = \u0026#34;default-tokyo-pool\u0026#34; machine_type = \u0026#34;e2-standard-4\u0026#34; min_count = 1 max_count = 3 initial_node_count = 1 }] remove_default_node_pool = true } module \u0026#34;gke_osaka\u0026#34; { source = \u0026#34;terraform-google-modules/kubernetes-engine/google//modules/beta-private-cluster\u0026#34; version = \u0026#34;19.0.0\u0026#34; description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/kubernetes-engine/google/19.0.0/submodules/beta-private-cluster\u0026#34; project_id = var.project_id name = var.osaka_cluster region = \u0026#34;asia-northeast2\u0026#34; network = var.network subnetwork = var.osaka_subnet ip_range_pods = \u0026#34;${var.osaka_subnet}-pods\u0026#34; ip_range_services = \u0026#34;${var.osaka_subnet}-services\u0026#34; enable_private_endpoint = false enable_private_nodes = true master_global_access_enabled = true master_ipv4_cidr_block = var.osaka_master_ip_range release_channel = var.release_channel node_pools = [{ name = \u0026#34;default-osaka-pool\u0026#34; machine_type = \u0026#34;e2-standard-4\u0026#34; min_count = 1 max_count = 3 initial_node_count = 1 }] remove_default_node_pool = true } modules/gke/variables.tf # GKEモジュールの外部変数を定義しています。\n作成例）./modules/gke/variables.tf\nvariable \u0026#34;project_id\u0026#34; {} variable \u0026#34;network\u0026#34; {} variable \u0026#34;tokyo_cluster\u0026#34; {} variable \u0026#34;tokyo_subnet\u0026#34; {} variable \u0026#34;tokyo_master_ip_range\u0026#34; {} variable \u0026#34;osaka_cluster\u0026#34; {} variable \u0026#34;osaka_subnet\u0026#34; {} variable \u0026#34;osaka_master_ip_range\u0026#34; {} variable \u0026#34;release_channel\u0026#34; { default = \u0026#34;STABLE\u0026#34; } modules/gke/outputs.tf # GKEモジュールの出力変数を定義しています。\n作成例）./modules/gke/outputs.tf\noutput \u0026#34;tokyo_cluster\u0026#34; { value = module.gke_tokyo.name } output \u0026#34;osaka_cluster\u0026#34; { value = module.gke_osaka.name } ASMモジュール定義 # modules/asm/main.tf # 東京/大阪リージョンのGKEクラスタにASMのインストール、マルチクラスタメッシ作成、Ingressゲートウェイのデプロイを定義しています、、、とはいえ、サンプルコードを書いといてなんですが大変苦しい実装になっていますので個人的には現時点では素直にTerraform以外を使用した方が良いと感じてます^^;\n   記事執筆時点(2022年1月末)では、Terraform公式asmサブモジュールv19.0.0(latest)がASM v11.0以降に対応できていなかったため、Terraform公式gcloudモジュールおよびkubectl-wrapperサブモジュールv3.1.0(latest)を活用してシェルスクリプトでゴリゴリ実装しており、非常に微妙な作りになっております。     今回の例ではTerraform公式firewall-rulesサブモジュールv4.1.0(latest)を活用してファイアウォールルールを定義していますが、rules内の変数定義が省略できず使い勝手はよろしくないため、google_compute_firewallリソースをそのまま定義した方が個人的には良いと感じてます。  作成例）./modules/asm/main.tf\nmodule \u0026#34;asm_tokyo\u0026#34; { source = \u0026#34;terraform-google-modules/gcloud/google//modules/kubectl-wrapper\u0026#34; version = \u0026#34;3.1.0\u0026#34;#description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/gcloud/google/3.1.0/submodules/kubectl-wrapper\u0026#34;  project_id = var.project_id cluster_name = var.tokyo_cluster cluster_location = var.tokyo_location kubectl_create_command = \u0026#34;${path.module}/scripts/install.sh ${var.project_id}${var.tokyo_cluster}${var.tokyo_location}${var.release_channel}\u0026#34; kubectl_destroy_command = \u0026#34;${path.module}/scripts/destroy.sh ${var.project_id}${var.tokyo_cluster}${var.tokyo_location}\u0026#34; } module \u0026#34;asm_osaka\u0026#34; { source = \u0026#34;terraform-google-modules/gcloud/google//modules/kubectl-wrapper\u0026#34; version = \u0026#34;3.1.0\u0026#34;#description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/gcloud/google/3.1.0/submodules/kubectl-wrapper\u0026#34;  project_id = var.project_id cluster_name = var.osaka_cluster cluster_location = var.osaka_location kubectl_create_command = \u0026#34;${path.module}/scripts/install.sh ${var.project_id}${var.osaka_cluster}${var.osaka_location}${var.release_channel}\u0026#34; kubectl_destroy_command = \u0026#34;${path.module}/scripts/destroy.sh ${var.project_id}${var.osaka_cluster}${var.osaka_location}\u0026#34; module_depends_on = [module.asm_tokyo.wait] } module \u0026#34;asm_firewall_rules\u0026#34; { source = \u0026#34;terraform-google-modules/network/google//modules/firewall-rules\u0026#34; version = \u0026#34;4.1.0\u0026#34;#description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/network/google/4.1.0/submodules/firewall-rules\u0026#34;  project_id = var.project_id network_name = var.network rules = [{ name = \u0026#34;${var.network}-istio-multicluster-pods\u0026#34; description = null direction = \u0026#34;INGRESS\u0026#34; priority = 900 ranges = [\u0026#34;${var.tokyo_pod_ip_range}\u0026#34;, \u0026#34;${var.osaka_pod_ip_range}\u0026#34;] source_tags = null source_service_accounts = null target_tags = [\u0026#34;gke-${var.tokyo_cluster}\u0026#34;, \u0026#34;gke-${var.osaka_cluster}\u0026#34;] target_service_accounts = null allow = [ { protocol = \u0026#34;tcp\u0026#34; ports = null }, { protocol = \u0026#34;udp\u0026#34; ports = null }, { protocol = \u0026#34;icmp\u0026#34; ports = null }, { protocol = \u0026#34;esp\u0026#34; ports = null }, { protocol = \u0026#34;ah\u0026#34; ports = null }, { protocol = \u0026#34;sctp\u0026#34; ports = null } ] deny = [] log_config = { metadata = \u0026#34;EXCLUDE_ALL_METADATA\u0026#34; } }] } module \u0026#34;asm_multi_mesh\u0026#34; { source = \u0026#34;terraform-google-modules/gcloud/google\u0026#34; version = \u0026#34;3.1.0\u0026#34;#description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/gcloud/google/3.1.0\u0026#34;  platform = \u0026#34;linux\u0026#34; additional_components = [\u0026#34;kubectl\u0026#34;, \u0026#34;beta\u0026#34;] create_cmd_entrypoint = \u0026#34;${path.module}/scripts/create-mesh.sh\u0026#34; create_cmd_body = \u0026#34;${var.project_id}${var.project_id}/${var.tokyo_location}/${var.tokyo_cluster}${var.project_id}/${var.osaka_location}/${var.osaka_cluster}\u0026#34; module_depends_on = [module.asm_osaka.wait] } module \u0026#34;asm_mcs_api\u0026#34; { source = \u0026#34;terraform-google-modules/gcloud/google\u0026#34; version = \u0026#34;3.1.0\u0026#34;#description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/gcloud/google/3.1.0\u0026#34;  platform = \u0026#34;linux\u0026#34; additional_components = [\u0026#34;kubectl\u0026#34;, \u0026#34;beta\u0026#34;] create_cmd_entrypoint = \u0026#34;gcloud\u0026#34; create_cmd_body = \u0026#34;container hub ingress enable --config-membership=${var.tokyo_cluster}\u0026#34; destroy_cmd_entrypoint = \u0026#34;gcloud\u0026#34; destroy_cmd_body = \u0026#34;container hub ingress disable\u0026#34; module_depends_on = [module.asm_multi_mesh.wait] } module \u0026#34;asm_tokyo_ingressgateway\u0026#34; { source = \u0026#34;terraform-google-modules/gcloud/google//modules/kubectl-wrapper\u0026#34; version = \u0026#34;3.1.0\u0026#34;#description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/gcloud/google/3.1.0/submodules/kubectl-wrapper\u0026#34;  project_id = var.project_id cluster_name = var.tokyo_cluster cluster_location = var.tokyo_location kubectl_create_command = \u0026#34;kubectl apply -f ${path.module}/manifests/istio-ingressgateway-pods\u0026#34; kubectl_destroy_command = \u0026#34;kubectl delete ns istio-system --ignore-not-found\u0026#34; module_depends_on = [module.asm_mcs_api.wait] } module \u0026#34;asm_osaka_ingressgateway\u0026#34; { source = \u0026#34;terraform-google-modules/gcloud/google//modules/kubectl-wrapper\u0026#34; version = \u0026#34;3.1.0\u0026#34;#description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/gcloud/google/3.1.0/submodules/kubectl-wrapper\u0026#34;  project_id = var.project_id cluster_name = var.osaka_cluster cluster_location = var.osaka_location kubectl_create_command = \u0026#34;kubectl apply -f ${path.module}/manifests/istio-ingressgateway-pods\u0026#34; kubectl_destroy_command = \u0026#34;kubectl delete ns istio-system --ignore-not-found\u0026#34; module_depends_on = [module.asm_tokyo_ingressgateway.wait] } module \u0026#34;asm_mcs_ingressgateway\u0026#34; { source = \u0026#34;terraform-google-modules/gcloud/google//modules/kubectl-wrapper\u0026#34; version = \u0026#34;3.1.0\u0026#34;#description = \u0026#34;https://registry.terraform.io/modules/terraform-google-modules/gcloud/google/3.1.0/submodules/kubectl-wrapper\u0026#34;  project_id = var.project_id cluster_name = var.tokyo_cluster cluster_location = var.tokyo_location kubectl_create_command = \u0026#34;kubectl apply -f ${path.module}/manifests/istio-ingressgateway-services\u0026#34; kubectl_destroy_command = \u0026#34;kubectl delete -f ${path.module}/manifests/istio-ingressgateway-services --ignore-not-found\u0026#34; module_depends_on = [module.asm_osaka_ingressgateway.wait] } modules/asm/variables.tf # ASMモジュールの外部変数を定義しています。\n作成例）./modules/asm/variables.tf\nvariable \u0026#34;project_id\u0026#34; {} variable \u0026#34;network\u0026#34; {} variable \u0026#34;tokyo_cluster\u0026#34; {} variable \u0026#34;tokyo_location\u0026#34; { default = \u0026#34;asia-northeast1\u0026#34; } variable \u0026#34;tokyo_pod_ip_range\u0026#34; {} variable \u0026#34;osaka_cluster\u0026#34; {} variable \u0026#34;osaka_location\u0026#34; { default = \u0026#34;asia-northeast2\u0026#34; } variable \u0026#34;osaka_pod_ip_range\u0026#34; {} variable \u0026#34;release_channel\u0026#34; { default = \u0026#34;STABLE\u0026#34; } modules/asm/scripts/install.sh # ASMのインストール処理を定義したスクリプトファイルです。ASM v11.0から正式ツールとなったasmcliコマンドを使用して、マネージドコントロールプレーン構成を作成しています。\n作成例）./modules/asm/scripts/install.sh\n#!/usr/bin/env bash  set -e PROJECT_ID=${1} CLUSTER_NAME=${2} CLUSTER_LOCATION=${3} RELEASE_CHANNEL=${4} curl https://storage.googleapis.com/csm-artifacts/asm/asmcli \u0026gt; asmcli chmod +x asmcli ./asmcli install \\  --project_id ${PROJECT_ID} \\  --cluster_name ${CLUSTER_NAME} \\  --cluster_location ${CLUSTER_LOCATION} \\  --managed \\  --channel ${RELEASE_CHANNEL} \\  --enable-all modules/asm/scripts/destroy.sh # ASMの削除処理を定義したスクリプトファイルです。ASM関連のNamespaceを削除し、Anthosクラスタからの登録解除を実行しています。\n作成例）./modules/asm/scripts/destroy.sh\n#!/usr/bin/env bash  set -e PROJECT_ID=${1} CLUSTER_NAME=${2} CLUSTER_LOCATION=${3} kubectl delete ns asm-system istio-system --ignore-not-found gcloud container hub memberships unregister ${CLUSTER_NAME} \\  --project=${PROJECT_ID} \\  --gke-cluster=${CLUSTER_LOCATION}/${CLUSTER_NAME} modules/asm/scripts/create-mesh.sh # マルチクラスタメッシュ作成処理を定義したスクリプトファイルです。\n作成例）./modules/asm/scripts/create-mesh.sh\n#!/usr/bin/env bash  set -e PROJECT_ID=\u0026#34;${1}\u0026#34; CLUSTER_1=\u0026#34;${2}\u0026#34; CLUSTER_2=\u0026#34;${3}\u0026#34; curl https://storage.googleapis.com/csm-artifacts/asm/asmcli \u0026gt; asmcli chmod +x asmcli ./asmcli create-mesh ${PROJECT_ID} ${CLUSTER_1} ${CLUSTER_2} modules/asm/manifests/istio-ingressgateway-pods/* # Istio IngressゲートウェイコンテナのKubernetesマニフェストファイルです。GitHubにて公開されている次のサンプルをベースにしています。\n 作成例）./modules/asm/manifests/istio-ingressgateway-pods/namespace.yaml\napiVersion:v1kind:Namespacemetadata:name:istio-systemlabels:istio.io/rev:asm-managed-stable作成例）./modules/asm/manifests/istio-ingressgateway-pods/deployment.yaml\napiVersion:apps/v1kind:Deploymentmetadata:name:istio-ingressgatewaynamespace:istio-systemspec:replicas:3selector:matchLabels:app:istio-ingressgatewayistio:ingressgatewaytemplate:metadata:annotations:inject.istio.io/templates:gatewaylabels:app:istio-ingressgatewayistio:ingressgatewayspec:containers:- name:istio-proxyimage:autoresources:limits:cpu:2000mmemory:1024Mirequests:cpu:100mmemory:128MiserviceAccountName:istio-ingressgateway---apiVersion:policy/v1beta1kind:PodDisruptionBudgetmetadata:name:istio-ingressgatewaynamespace:istio-systemspec:maxUnavailable:1selector:matchLabels:istio:ingressgatewayapp:istio-ingressgateway---apiVersion:autoscaling/v2beta1kind:HorizontalPodAutoscalermetadata:name:istio-ingressgatewaynamespace:istio-systemspec:maxReplicas:5metrics:- resource:name:cputargetAverageUtilization:80type:ResourceminReplicas:3scaleTargetRef:apiVersion:apps/v1kind:Deploymentname:istio-ingressgateway作成例）./modules/asm/manifests/istio-ingressgateway-pods/serviceaccount.yaml\napiVersion:v1kind:ServiceAccountmetadata:name:istio-ingressgatewaynamespace:istio-system作成例）./modules/asm/manifests/istio-ingressgateway-pods/role.yaml\napiVersion:rbac.authorization.k8s.io/v1kind:Rolemetadata:name:istio-ingressgatewaynamespace:istio-systemrules:- apiGroups:[\u0026#34;\u0026#34;]resources:[\u0026#34;secrets\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;]---apiVersion:rbac.authorization.k8s.io/v1kind:RoleBindingmetadata:name:istio-ingressgatewaynamespace:istio-systemroleRef:apiGroup:rbac.authorization.k8s.iokind:Rolename:istio-ingressgatewaysubjects:- kind:ServiceAccountname:istio-ingressgatewaymodules/asm/manifests/istio-ingressgateway-services/* # Istio Ingressゲートウェイ用マルチクラスタIngress/ServiceのKubernetesマニフェストファイルです。\n作成例）./modules/asm/manifests/istio-ingressgateway-services/multiclusterservice.yaml\napiVersion:networking.gke.io/v1kind:MultiClusterServicemetadata:name:istio-ingressgatewaynamespace:istio-systemannotations:cloud.google.com/backend-config:\u0026#39;{\u0026#34;default\u0026#34;: \u0026#34;ingress-backendconfig\u0026#34;}\u0026#39;labels:app:istio-ingressgatewayistio:ingressgatewayspec:template:spec:ports:- name:status-portport:15021protocol:TCPtargetPort:15021- name:http2port:80- name:httpsport:443selector:istio:ingressgatewayapp:istio-ingressgateway作成例）./modules/asm/manifests/istio-ingressgateway-services/backendconfig.yaml\napiVersion:cloud.google.com/v1kind:BackendConfigmetadata:name:ingress-backendconfignamespace:istio-systemspec:healthCheck:requestPath:/healthz/readyport:15021type:HTTP作成例）./modules/asm/manifests/istio-ingressgateway-services/multiclusteringress.yaml\napiVersion:networking.gke.io/v1beta1kind:MultiClusterIngressmetadata:name:istio-ingressgatewaynamespace:istio-systemlabels:app:istio-ingressgatewayistio:ingressgatewayspec:template:spec:backend:serviceName:istio-ingressgatewayservicePort:80デプロイ用のCloud Buildパイプラインも書いてみたけれど、、、 # terraform init/plan/applyコマンドを順に実行するだけですが、手動だとどんなに簡単なコマンドであってもミスが生じてしまう可能性はあるためパイプライン化してみました。環境名のブランチpocに対してPushが入ったら起動するといったイメージにしております。\n、、、と本来であればこれでもパイプラインは動くはずなのですが、残念なことにTerraform公式asmサブモジュールv19.0.0(latest)、gcloudモジュールおよびkubectl-wrapperサブモジュールv3.1.0(latest)をTerraform公式Dockerイメージ上で動かすとエラーが発生してしまいます。非常に微妙ですが、今回のサンプルコードではDockerイメージをカスタマイズするか、あきらめて手動で実行をする必要がございます(TT)\n   記事執筆時点(2022年1月末)では、Terraform公式asmサブモジュールv19.0.0(latest)、gcloudモジュールおよびkubectl-wrapperサブモジュールv3.1.0(latest)をTerraform公式Dockerイメージ上で動かすとエラーになりますのでご注意ください。  作成例）cloudbuild.yaml\nsubstitutions:_TERRAFORM_VERSION:1.1.4steps:- id:\u0026#34;terraform init\u0026#34;name:\u0026#34;hashicorp/terraform:${_TERRAFORM_VERSION}\u0026#34;entrypoint:\u0026#34;sh\u0026#34;args:- \u0026#34;-cx\u0026#34;- |cd environments/${BRANCH_NAME} terraform init -reconfigure- id:\u0026#34;terraform plan\u0026#34;name:\u0026#34;hashicorp/terraform:${_TERRAFORM_VERSION}\u0026#34;entrypoint:\u0026#34;sh\u0026#34;args:- \u0026#34;-cx\u0026#34;- |cd environments/${BRANCH_NAME} terraform plan -var=\u0026#34;project_id=${PROJECT_ID}\u0026#34;- id:\u0026#34;terraform apply\u0026#34;name:\u0026#34;hashicorp/terraform:${_TERRAFORM_VERSION}\u0026#34;entrypoint:\u0026#34;sh\u0026#34;args:- \u0026#34;-cx\u0026#34;- |cd environments/${BRANCH_NAME} terraform apply -auto-approve -var=\u0026#34;project_id=${PROJECT_ID}\u0026#34;エラーメッセージ出力例）\nmodule.asm.module.asm_tokyo.module.gcloud_kubectl.null_resource.additional_components[0]: Creating... module.asm.module.asm_tokyo.module.gcloud_kubectl.null_resource.additional_components[0]: Provisioning with \u0026#39;local-exec\u0026#39;... module.asm.module.asm_tokyo.module.gcloud_kubectl.null_resource.additional_components[0] (local-exec): Executing: [\u0026#34;/bin/sh\u0026#34; \u0026#34;-c\u0026#34; \u0026#34;.terraform/modules/asm.asm_tokyo/scripts/check_components.sh gcloud kubectl\u0026#34;] module.asm.module.asm_tokyo.module.gcloud_kubectl.null_resource.additional_components[0] (local-exec): /bin/sh: .terraform/modules/asm.asm_tokyo/scripts/check_components.sh: not found ╷ │ Error: local-exec provisioner error │ │ with module.asm.module.asm_tokyo.module.gcloud_kubectl.null_resource.additional_components[0], │ on .terraform/modules/asm.asm_tokyo/main.tf line 174, in resource \u0026#34;null_resource\u0026#34; \u0026#34;additional_components\u0026#34;: │ 174: provisioner \u0026#34;local-exec\u0026#34; { │ │ Error running command │ \u0026#39;.terraform/modules/asm.asm_tokyo/scripts/check_components.sh gcloud │ kubectl\u0026#39;: exit status 127. Output: /bin/sh: │ .terraform/modules/asm.asm_tokyo/scripts/check_components.sh: not found │ ╵ 終わりに # 今回はGKE+ASMのマルチクラスタメッシュ環境をTerraformを使って、しかも普段あまり積極的な活用はしないTerraform公式モジュールをあえて多用して^^; 構築してみましたがいかがだったでしょうか。もしこれから「ASM環境をTerraformで」と検討している方は参考にしてみてはいかがでしょうか。\nとはいえ、サンプルコードを書いといてなんですがASMの導入からはシェルスクリプトを多用した大変苦しい実装になっておりますし、個人的には現時点ではASMの導入以降は素直にTerraform以外を使用した方が良いと感じてます^^; とりあえず、苦しいことだけは伝わったかと思います。あくまで本記事はご参考ということで、その点ご承知おきいただけると幸いです。\n  Google Cloud は、Google LLC の商標または登録商標です。 Terraform は、HashiCorp, Inc. の米国およびその他の国における商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。  ","date":"February 14, 2022","permalink":"/posts/2022/02/gcp-deploy-asm-with-terraform/","section":"記事一覧","summary":"みなさん、こんにちは。以前に「複数リージョンのGKEクラスタとAnthos Service Meshでマルチクラスタメッシュ環境を構築してみた」という記事を書いたのですが、今回はその環境をTerraformを使って構築してみました。もしこれから「ASM環境をTerraformで」と検討している方は参考にしてみてはいかがでしょうか。","title":"Terraformを使ってGKE+ASMのマルチクラスタメッシュ環境を構築してみた"},{"content":"みなさん、こんにちは。AWS マネジメントコンソールを使っていると、ごくごく稀に表示言語を代えたくなることはありませんか。私は日本語⇔英語を切り替えたくなるケースがたまにあります。\nただ、個人的に「設定」と言えばなんとなく右上の項目からできそうなイメージがあり、「あれ、設定ってどこから変えるんだっけ、、、アカウントへ飛んだ先にあったっけ、、、(答え、アカウントへ飛んだ先にはない)」みたいにウッカリ設定方法を忘れて途方にくれてしまうことがあります。みなさんはそんな経験ございませんかね？ということで自分への備忘も込め、今回は表示言語の設定方法について紹介していきたいと思います。\n   表示言語の変更方法 # では早速答えですが、言語の設定方法はAWS マネジメントコンソールの左下部分にございます。(なるほど、ここだったか、、、)\n   終わりに # いまさらの情報でしたがいかがだったでしょうか。こんな記事でもだれかの役に立っていただければ幸いです。以上、AWS マネジメントコンソールの表示言語を変更する方法でした。\n  AWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。  ","date":"January 13, 2022","permalink":"/posts/2022/01/aws-console-language-settings/","section":"記事一覧","summary":"みなさん、こんにちは。AWS マネジメントコンソールを使っていると、ごくごく稀に表示言語を代えたくなることはありませんか。私は日本語⇔英語を切り替えたくなるケースがたまにあります。","title":"AWSマネジメントコンソールの表示言語を変更する方法"},{"content":"みなさん、こんにちは。AWS CloudShellを使っているとファイルにペーストした際、勝手にインデントが入って「あー！」っとなったことの一度くらいはあるのではないでしょうか。今回はそんなときの解決方法を紹介していきたいと思います。\n   インデントの自動挿入なしでペーストする方法 # いくつか方法はありますが、解の1つは「ペーストモードを使う」です。編集モードへ入る前に :set paste もしくは :set paste! を実行しましょう。ペーストモードにすると出力例のようにインデントが追加されずに期待通りの動きになりますね。\n   起動時に自動で設定する方法 # とはいえ、エディタを起動するたびに毎回ペーストモードの設定をするのは面倒です。そんなときは vim の設定ファイル ~/.vimrc を作成しましょう。これでエディタが起動した際に自動で適用されるようになります。めでたしめでたし。\n作成例）~/.vimrc\nset paste 終わりに # いまさらの情報でしたがいかがだったでしょうか。もちろんコードを編集するときは自動でインデントを追加してくれるのはうれしいのですが、個人的にはAWS CloudShell上ではクリップボードからコピーしてくることも結構多いのでデフォルトペーストモードにしておき、必要に応じて :set nopaste もしくは :set paste! しております。よろしければ参考にしていただければと思います。\n以上、AWS CloudShell上のviエディタでインデントの自動挿入なしでペーストする方法でした。\n  AWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。  ","date":"December 20, 2021","permalink":"/posts/2021/12/aws-cloudshell-autoindent-settings/","section":"記事一覧","summary":"みなさん、こんにちは。AWS CloudShellを使っているとファイルにペーストした際、勝手にインデントが入って「あー！」っとなったことの一度くらいはあるのではないでしょうか。今回はそんなときの解決方法を紹介していきたいと思います。","title":"AWS CloudShell上のviエディタでインデントの自動挿入なしでペーストする方法"},{"content":"みなさん、こんにちは。今回はタイトルに掲げたとおり、最近の更新で Google Cloud コンソール(GUI)から Google Kubernetes Engine(GKE) Standard クラスタを作成する際に、次のようなすてきなオプションがプレビュー機能として追加されました。今回はこのオプションを使ってどのような構成が作られるのか実験したので共有したいと思います。\n   いきなりですが、結論です！ # Q1. In-cluster とマネージドコントロールプレーンのどちらで構築される？ # 答え、マネージドコントロールプレーンにて構築されます。現時点でこの設定を変更することはできません。\nQ2. Anthos Service Mesh のバージョンは？ # 答え、GKE でリリースチャンネルを採用した場合は GKE と同じチャンネルになります。GKE で静的リリースを採用した場合は Reguler チャンネルとなります。現時点でこの設定を変更することはできません。\nQ3. カスタム CA を扱うことはできるか？ # 答え、扱えません。マネージドコントロールプレーンでの導入となるため、カスタム CA を扱うことができる Istio CA を選択することはできません。\nQ4. 限定クラスタにした場合は別途 15017/TCP を許可する必要があるか？ # 答え、不要です。ルールを追加しなくてもサイドカー自動インジェクションは問題なく動きます。\nQ5. Ingress ゲートウェイはデフォルトで作られるか？ # 答え、Ingress ゲートウェイはデフォルトでは作られません。別途ユーザにてデプロイする必要があります。\n終わりに # 今回は GKE クラスタ作成時の Anthos Service Mesh 有効化オプションの実験結果の共有でしたがいかがだったでしょうか。\nGKE と Anthos Service Mesh で採用するリリースチャンネルを変えたい、カスタム CA を使いたいといったケースでは従来どおり CLI を利用する必要がありますが、これらの要件がなければ GUI からポチポチするだけでとっても簡単に環境を作れるようになりそうですね。\n   2021 年 12 月時点ではGUI の Anthos Service Mesh 有効化オプションはプレビュー段階であり、今回紹介した挙動から変わる可能性がありますのでご注意ください。    Google Cloud は、Google LLC の商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。  ","date":"December 16, 2021","permalink":"/posts/2021/12/gcp-new-asm-installation-options/","section":"記事一覧","summary":"みなさん、こんにちは。今回はタイトルに掲げたとおり、最近の更新で Google Cloud コンソール(GUI)から Google Kubernetes Engine(GKE) Standard クラスタを作成する際に、次のようなすてきなオプションがプレビュー機能として追加されました。今回はこのオプションを使ってどのような構成が作られるのか実験したので共有したいと思います。","title":"GKE クラスタ作成時のオプションで Anthos Service Mesh を有効化できるようになりました"},{"content":"みなさん、こんにちは。今回は Amazon Elastic Kubernetes Service(EKS) を利用する際に併せて利用したい AWS Load Balancer Controller のお話です。\nみなさんは Amazon EKS を活用して Kubernetes クラスタを AWS 上で動かすとなった際に、他のマネージドサービスの利用はどうされていますか。もちろんすべて Kubernetes 上で動かしてシステムを完結させるという選択肢もあるかと思いますが、やはり多くの方が他の AWS のマネージドサービスの併用も検討されるのではないでしょうか。その一方で、これら併用環境のコード化 (IaC、Infrastructure as Code) を実現しようとすると、Kubernetes アプリケーションの管理は Helm で、AWS リソースの管理は Terraform で、などという別々のツールでの管理になってしまいがちです。\nそんな悩みを解決する1つの手段が AWS Load Balancer Controller や AWS Controllers for Kubernetes といった Kubernetes クラスタ機能を拡張する各種コントローラの活用です。これらのコントローラを利用することで、AWS リソースについても Kubernetes マニフェストファイルで定義できるようになり、Kubernetes 側に運用管理を寄せてシンプル化できます。\n今回はそのうちの1つ、Elastic Load Balancing(ELB) を Kubernetes クラスタで管理できるようにする AWS Load Balancer Controller について、簡単なサンプルアプリケーションを交えて紹介していきたいと思います。これから Amazon EKS 上にアプリケーションを展開しようと考えている方は参考にしてみてはいかがでしょうか。\nAWS Load Balancer Controller とは # AWS Load Balancer Controller (旧AWS ALB Ingress Controller) は、ELB を Kubernetes クラスタから管理するためのコントローラです。このコントローラを活用することで、Kubernetes Ingress リソースとして L7 ロードバランサの Application Load Balancer(ALB) を、Kuberntes Service リソースとして L4 ロードバランサの Network Load Balancer(NLB) を利用することができるようになります。\nKubernetes Service/Ingress リソースでの処理を外部ロードバランサである ELB へ切り出すことによって、ワークロードへの性能影響の低減、ノードリソースの利用効率の向上、Service/Ingress リソースのスケーリングなどを AWS 側へ任せることで運用負荷の低減といったことができる見込みです。\n AWS Load Balancer Controller を導入してみよう # Step1. 作業環境の設定 # 今回は Amazon EKS や AWS Load Balancer Controller の管理を行う環境として AWS CloudShell を利用していきたいと思います。まずは操作に必要な各種ツールの設定を AWS CloudShell にしてきましょう。\nAWS CLI の設定 # AWS リソースの操作を行えるように次のコマンドを実行し、AWS CLI の設定を行いましょう。\n実行例）AWS CLIの設定\naws configure kubectl コマンドのインストール # 次に Kubernetes 管理ツールの kubectl コマンドをインストールしましょう。\n 実行例）kubectlコマンドのインストール\n# kubectl コマンドのダウンロード curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.21.2/2021-07-05/bin/linux/amd64/kubectl # 実行権限の付与 chmod +x kubectl # 実行ファイルのパスを設定 mkdir -p ${HOME}/bin \u0026amp;\u0026amp; mv kubectl ${HOME}/bin \u0026amp;\u0026amp; export PATH=${PATH}:${HOME}/bin # シェルの起動時に $HOME/bin をパスへ追加 echo \u0026#39;export PATH=${PATH}:${HOME}/bin\u0026#39; \u0026gt;\u0026gt; ~/.bashrc # インストールが成功していることを確認 kubectl version --short --client インストールに成功していれば出力例のようにバージョン情報の出力を確認できます。\n出力例）\n$ kubectl version --short --client Client Version: v1.21.2-13+d2965f0db10712 eksctl コマンドのインストール # 続いて Amazon EKS 管理ツールの eksctl コマンドをインストールしましょう。\n 実行例）eksctlコマンドのインストール\n# eksctl の最新バージョンをダウンロード curl -L \u0026#34;https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\u0026#34; | tar xz -C /tmp # 実行ファイルをパスの通ったディレクトリへ移動 mv /tmp/eksctl ${HOME}/bin # インストールが成功していることを確認 eksctl version インストールに成功していれば出力例のようにバージョン情報の出力を確認できます。\n出力例）\n$ eksctl version 0.76.0 helm コマンドのインストール # 最後に Kubernetes 上で稼働するアプリケーションを管理するためのツールである helm コマンドをインストールしましょう。\n 実行例）helmコマンドのインストール\n# 前提パッケージのインストール sudo yum install -y openssl # インストールスクリプトのダウンロード curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 \u0026gt; get_helm.sh # 実行権限の付与 chmod 700 get_helm.sh # インストールスクリプトの実行 ./get_helm.sh # インストールが成功していることを確認 helm version --short インストールに成功していれば出力例のようにバージョン情報の出力を確認できます。\n出力例）\n$ helm version --short v3.7.2+g663a896 以上で作業環境(AWS CloudShell)の設定は完了です。\nStep2. EKS クラスタの作成 # 続いて AWS Load Balancer Controller を導入する対象の Amazon EKS クラスタを作成していきましょう。今回は Kubernetes ノードには AWS Fargate を使用していきたいと思います。それでは eksctl コマンドを実行してクラスタを作成しましょう。\n 実行例）EKSクラスタの作成\n# 環境変数の設定 export CLUSTER=\u0026#34;my-tokyo-cluster\u0026#34; # EKS クラスタの作成 eksctl create cluster --name ${CLUSTER} --version 1.21 --fargate # サービスアカウントでの IAM ロール使用を許可 eksctl utils associate-iam-oidc-provider --cluster ${CLUSTER} --approve ここまで終わりましたら AWS Load Balancer Controller を利用するための事前準備は完了です。\nStep3. コントローラのデプロイ # それでは AWS Load Balancer Controller を Amazon EKS クラスタにデプロイしていきましょう。\n サービスアカウントの作成 # まずは AWS Load Balancer Controller 用のサービスアカウントの作成を行っていきます。今回は kube-system Namespace に aws-load-balancer-controller という名前でサービスアカウントを作成して行きたいと思います。それでは次のコマンドを実行してサービスアカウントを作成しましょう。\n実行例）サービスアカウントの作成\nAWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \u0026#34;Account\u0026#34; --output text) POLICY_NAME=\u0026#34;myAWSLoadBalancerControllerIAMPolicy\u0026#34; SERVICE_ACCOUNT=\u0026#34;aws-load-balancer-controller\u0026#34; # IAM ポリシー定義ファイルのダウンロード curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json # IAM ポリシーの作成 aws iam create-policy \\  --policy-name ${POLICY_NAME} \\  --policy-document file://iam_policy.json # サービスアカウントの作成 eksctl create iamserviceaccount \\  --name=${SERVICE_ACCOUNT} \\  --cluster=${CLUSTER} \\  --namespace=\u0026#34;kube-system\u0026#34; \\  --attach-policy-arn=arn:aws:iam::${AWS_ACCOUNT_ID}:policy/${POLICY_NAME} \\  --override-existing-serviceaccounts \\  --approve コントローラのインストール # 次に AWS Load Balancer Controller をインストールしていきましょう。なお、AWS Load Balancer Controller のインストール方法はいくつか用意されていますが、AWS Fargate 環境の場合は Helm を利用したインストール方法を選択する必要があるためご注意ください。\n実行例）コントローラのインストール\n# EKS 用の Helm レポジトリを追加 helm repo add eks https://aws.github.io/eks-charts # TargetGroupBinding カスタムリソースをインストール kubectl apply -k \u0026#34;github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\u0026#34; # Helm チャートからインストール helm install aws-load-balancer-controller eks/aws-load-balancer-controller \\  --set clusterName=${CLUSTER} \\  --set serviceAccount.create=false \\  --set vpcId=$(aws cloudformation describe-stacks \\  --stack-name \u0026#34;eksctl-${CLUSTER}-cluster\u0026#34; \\  --query \u0026#39;Stacks[0].Outputs[?OutputKey==`VPC`].OutputValue\u0026#39; \\  --output text) \\  --set serviceAccount.name=${SERVICE_ACCOUNT} \\  -n kube-system # 確認 kubectl get deployment -n kube-system aws-load-balancer-controller インストールに成功していれば出力例のようにコントローラが一覧へ表示されるようになります。\n出力例）\n$ kubectl get deployment -n kube-system aws-load-balancer-controller NAME READY UP-TO-DATE AVAILABLE AGE aws-load-balancer-controller 2/2 2 2 42s 以上で AWS Load Balancer Controller の導入は完了です。\nAWS Load Balancer Controller を使ってみよう # Network Load Balancer (Serviceリソース) の使い方 # AWS Load Balancer Controller 環境では、次のサンプルのように Kubernetes Service リソース定義にて「LoadBalancer タイプの指定」と「アノテーションとして各種パラメータを指定」をすることによって Network Load Balancer(NLB) をプロビジョニングできます。\n 作成例）Serviceリソース定義サンプル\napiVersion:v1kind:Servicemetadata:name:sample-serviceannotations:service.beta.kubernetes.io/aws-load-balancer-type:externalservice.beta.kubernetes.io/aws-load-balancer-scheme:internet-facingservice.beta.kubernetes.io/aws-load-balancer-nlb-target-type:ipspec:type:LoadBalancerports:- port:80protocol:TCPselector:app:sample-appアノテーションに指定する主要なパラメータとしては次の通りです。アノテーションに指定可能なパラメータの一覧につきましては次の URL を参照してください。\n    アノテーション名 説明     service.beta.kubernetes.io/aws-load-balancer-type ロードバランサのプロビジョニングに使用するコントローラを指定します。AWS Load Balancer Controller の場合は \u0026ldquo;external\u0026rdquo; を指定します。   service.beta.kubernetes.io/aws-load-balancer-scheme ロードバランサのスキームを指定します。内部向けの場合は \u0026ldquo;internal\u0026quot;、インターネット向けの場合は \u0026ldquo;internet-facing\u0026rdquo; を指定します。   service.beta.kubernetes.io/aws-load-balancer-nlb-target-type バックエンドに指定するターゲットタイプを指定します。トラフィックを直接 Pod にルーティングするには \u0026ldquo;ip\u0026rdquo; を指定します。   service.beta.kubernetes.io/aws-load-balancer-healthcheck-port バックエンドのヘルスチェック用ポート(\u0026rdquo;traffic-port\u0026quot;/\u0026quot;ポート番号\u0026quot;)を指定します。デフォルトは \u0026ldquo;traffic-port\u0026rdquo; です。   service.beta.kubernetes.io/aws-load-balancer-healthcheck-protocol バックエンドのヘルスチェック用プロトコル(\u0026quot;tcp\u0026quot;/\u0026quot;http\u0026quot;/\u0026quot;https\u0026quot;)を指定します。デフォルトは \u0026ldquo;tcp\u0026rdquo; です。   service.beta.kubernetes.io/aws-load-balancer-healthcheck-path バックエンドの HTTP/HTTPS ヘルスチェック用パスを指定します。デフォルトは \u0026ldquo;/\u0026rdquo; です。    NLB Service のサンプル # それではサンプルアプリケーションをデプロイして NLB Service リソースを実際に動かしてみましょう。今回は次のサンプルアプリケーションをベースに少しだけ手を加えたものを用いて動作確認をしていきたいと思います。\n 作成例）helloworld-nlb-sample.yaml (サンプルアプリケーション定義ファイル)\napiVersion:v1kind:Servicemetadata:name:helloworldlabels:app:helloworldannotations:service.beta.kubernetes.io/aws-load-balancer-type:externalservice.beta.kubernetes.io/aws-load-balancer-scheme:internet-facingservice.beta.kubernetes.io/aws-load-balancer-nlb-target-type:ipservice.beta.kubernetes.io/aws-load-balancer-healthcheck-protocol:httpservice.beta.kubernetes.io/aws-load-balancer-healthcheck-path:/healthspec:type:LoadBalancerports:- port:80targetPort:5000name:httpselector:app:helloworld---apiVersion:apps/v1kind:Deploymentmetadata:name:helloworld-v1labels:app:helloworldversion:v1spec:replicas:1selector:matchLabels:app:helloworldversion:v1template:metadata:labels:app:helloworldversion:v1spec:containers:- name:helloworldimage:docker.io/istio/examples-helloworld-v1ports:- containerPort:5000それでは次のコマンドを実行してサンプルアプリケーションをデプロイしていきましょう。\n実行例）サンプルアプリケーションのデプロイ\nexport FARGATEPROFILE=\u0026#34;sample-app\u0026#34; export NAMESPACE=\u0026#34;sample\u0026#34; # Fargate プロファイルの作成 eksctl create fargateprofile --cluster ${CLUSTER} --name ${FARGATEPROFILE} --namespace ${NAMESPACE} # サンプルアプリケーション用 Namespace の作成 kubectl create namespace ${NAMESPACE} # サンプルアプリケーションのデプロイ kubectl apply -n ${NAMESPACE} -f helloworld-nlb-sample.yaml # サービスがデプロイされたことを確認 kubectl get -n ${NAMESPACE} service helloworld NLB Service のデプロイに成功した場合は次の出力例のように Service リソース一覧に表示されます。\n出力例）\n$ kubectl get -n ${NAMESPACE} service helloworld NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE helloworld LoadBalancer 10.100.114.79 k8s-sample-hellowor-xxxxxxxxxx-xxxxxxxxxxxxxxxx.elb.ap-northeast-1.amazonaws.com 80:32642/TCP 32h NLB のヘルスチェックが正常になるまで数分待った後、NLB のパブリックエンドポイントにアクセスしてみましょう。\n実行例）サンプルアプリケーションへアクセス\n# NLB の DNS 名を取得 ENDPOINT=$(kubectl get -n ${NAMESPACE} service helloworld \\  -o custom-columns=HOSTNAME:status.loadBalancer.ingress[0].hostname \\  --no-headers) # テストの実行 curl http://${ENDPOINT}/hello 期待通りのレスポンスが返ってくることが確認できたら成功です。\n出力例）\n$ curl http://${ENDPOINT}/hello Hello version: v1, instance: helloworld-v1-b9d9d6679-hdqsq 最後にサンプルアプリケーションを削除して動作確認は終了です。お疲れ様でした。\n実行例）サンプルアプリケーションの削除\nkubectl delete namespace ${NAMESPACE} Application Load Balancer (Ingressリソース) の使い方 # AWS Load Balancer Controller 環境では、次のサンプルのように「Kubernetes IngressClass リソースにてコントローラの指定」を、「Kubernetes Ingress リソース定義のアノテーションとして各種パラメータを指定」をすることによって Application Load Balancer(ALB) をプロビジョニングできます。\n    Kubernetes Ingress リソースのアノテーションに kubernetes.io/ingress.class: alb を指定することでも作成できますが、Kubernetes 1.18 以降は kubernetes.io/ingress.class の利用は非推奨となっておりますのでご注意ください。  作成例）Ingressリソース定義サンプル\napiVersion:networking.k8s.io/v1kind:IngressClassmetadata:name:sample-ingress-classspec:controller:ingress.k8s.aws/alb---apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:sample-ingressannotations:alb.ingress.kubernetes.io/scheme:internet-facingalb.ingress.kubernetes.io/target-type:ipspec:ingressClassName:sample-ingress-classrules:- http:paths:- path:/hellopathType:Exactbackend:service:name:sample-serviceport:number:80アノテーションに指定する主要なパラメータとしては次の通りです。なお、ALB Ingress は AWS WAF や AWS Shield による脅威からの保護、Amazon Cognito 認証などさまざまなマネージドサービスとの連携が可能です。アノテーションに指定可能なパラメータの一覧につきましては次の URL を参照してください。\n    アノテーション名 説明     alb.ingress.kubernetes.io/group.name 複数の Ingress リソースを 1 つの ALB に統合する際にグループ名を指定します。   alb.ingress.kubernetes.io/scheme ロードバランサのスキームを指定します。内部向けの場合は \u0026ldquo;internal\u0026quot;、インターネット向けの場合は \u0026ldquo;internet-facing\u0026rdquo; を指定します。   alb.ingress.kubernetes.io/listen-ports ロードバランサの受付ポートを指定します。デフォルトは \u0026rsquo;[{\u0026ldquo;HTTP\u0026rdquo;: 80}]\u0026rsquo; | \u0026lsquo;[{\u0026ldquo;HTTPS\u0026rdquo;: 443}]\u0026rsquo; です。   alb.ingress.kubernetes.io/target-type バックエンドに指定するターゲットタイプを指定します。トラフィックを直接 Pod にルーティングするには \u0026ldquo;ip\u0026rdquo; を指定します。   alb.ingress.kubernetes.io/healthcheck-port バックエンドのヘルスチェック用ポート(\u0026rdquo;traffic-port\u0026quot;/\u0026quot;ポート番号\u0026quot;)を指定します。デフォルトは \u0026ldquo;traffic-port\u0026rdquo; です。   alb.ingress.kubernetes.io/healthcheck-protocol バックエンドのヘルスチェック用プロトコル(\u0026quot;http\u0026quot;/\u0026quot;https\u0026quot;)を指定します。デフォルトは \u0026ldquo;http\u0026rdquo; です。   alb.ingress.kubernetes.io/healthcheck-path バックエンドの HTTP/HTTPS ヘルスチェック用パスを指定します。デフォルトは \u0026ldquo;/\u0026rdquo; です。    ALB Ingress のサンプル # それではサンプルアプリケーションをデプロイして ALB Ingress リソースを実際に動かしてみましょう。今回は次のサンプルアプリケーションをベースに少しだけ手を加えたものを用いて動作確認をしていきたいと思います。\n 作成例）helloworld-alb-sample.yaml (サンプルアプリケーション定義ファイル)\napiVersion:networking.k8s.io/v1kind:IngressClassmetadata:name:alb-ingressspec:controller:ingress.k8s.aws/alb---apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:helloworldannotations:alb.ingress.kubernetes.io/scheme:internet-facingalb.ingress.kubernetes.io/target-type:ipalb.ingress.kubernetes.io/healthcheck-path:/healthspec:ingressClassName:alb-ingressrules:- http:paths:- path:/hellopathType:Exactbackend:service:name:helloworldport:number:80---apiVersion:v1kind:Servicemetadata:name:helloworldlabels:app:helloworldservice:helloworldspec:ports:- port:80targetPort:5000name:httpselector:app:helloworld---apiVersion:apps/v1kind:Deploymentmetadata:name:helloworld-v1labels:app:helloworldversion:v1spec:replicas:1selector:matchLabels:app:helloworldversion:v1template:metadata:labels:app:helloworldversion:v1spec:containers:- name:helloworldimage:docker.io/istio/examples-helloworld-v1ports:- containerPort:5000それでは次のコマンドを実行してサンプルアプリケーションをデプロイしていきましょう。\n実行例）サンプルアプリケーションのデプロイ\nexport FARGATEPROFILE=\u0026#34;sample-app\u0026#34; export NAMESPACE=\u0026#34;sample\u0026#34; # Fargate プロファイルの作成 eksctl create fargateprofile --cluster ${CLUSTER} --name ${FARGATEPROFILE} --namespace ${NAMESPACE} # サンプルアプリケーション用 Namespace の作成 kubectl create namespace ${NAMESPACE} # サンプルアプリケーションのデプロイ kubectl apply -n ${NAMESPACE} -f helloworld-alb-sample.yaml # Ingress がデプロイされたことを確認 kubectl get -n ${NAMESPACE} ingress helloworld ALB Ingress のデプロイに成功した場合は次の出力例のように Ingress リソース一覧に表示されます。\n出力例）\n$ kubectl get -n ${NAMESPACE} ingress helloworld NAME CLASS HOSTS ADDRESS PORTS AGE helloworld \u0026lt;none\u0026gt; * k8s-sample-hellowor-xxxxxxxxxx-xxxxxxxx.ap-northeast-1.elb.amazonaws.com 80 70s DNS への登録などが反映されるまで数分待った後、ALB のパブリックエンドポイントにアクセスしてみましょう。\n実行例）サンプルアプリケーションへアクセス\n# NLB の DNS 名を取得 ENDPOINT=$(kubectl get -n ${NAMESPACE} ingress helloworld \\  -o custom-columns=HOSTNAME:status.loadBalancer.ingress[0].hostname \\  --no-headers) # テストの実行 curl http://${ENDPOINT}/hello 期待通りのレスポンスが返ってくることが確認できたら成功です。\n出力例）\n$ curl http://${ENDPOINT}/hello Hello version: v1, instance: helloworld-v1-b9d9d6679-rpfzl 最後にサンプルアプリケーションを削除して動作確認は終了です。お疲れ様でした。\n実行例）サンプルアプリケーションの削除\nkubectl delete namespace ${NAMESPACE} 終わりに # 今回は Amazon Elastic Kubernetes Service(EKS) を利用する際に併せて利用したい AWS Load Balancer Controller のご紹介でしたが、いかがだったでしょうか？\n今回は IaC を実現する 1 つの手段として、というカットでの紹介でしたが Kubernetes Ingress/Service リソースを ELB にすることでさまざまなメリットが期待できますので、Amazon EKS をご利用の際は AWS Load Balancer Controller の併用も視野に入れてみてはいかがでしょうか。\nなお、今回は紹介しませんでしたが AWS Load Balancer Controller は使いたいけれど、ELB リソースのライフサイクルは Kubernetes からは切り離したいというケースもあるかと思います。そういった場合は AWS Load Balancer Controller の TargetGroupBinding 機能を利用することで実現可能なので参考にしていただければと思います。TargetGroupBinding 機能の詳細については公式ドキュメントを参照してください。\n 以上、Kubernetes Service/Ingress リソースと Elastic Load Balancing(ELB) との統合を実現する「AWS Load Balancer Controller」のご紹介でした。\n  AWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 Kubernetes は、The Linux Foundation の米国およびその他の国における登録商標または商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。  ","date":"December 14, 2021","permalink":"/posts/2021/12/aws-load-balancer-controller/","section":"記事一覧","summary":"みなさん、こんにちは。今回は Amazon Elastic Kubernetes Service(EKS) を利用する際に併せて利用したい AWS Load Balancer Controller のお話です。","title":"AWS Load Balancer Controller を使って ELB を Kubernetes のマニフェストファイルで管理しよう"},{"content":"みなさん、こんにちは。今回は単一リージョンに展開した複数 GKE クラスタを単一の Anthos Service Mesh 環境に追加し、GKE クラスタ間で負荷分散を行う方法についてご紹介していきたいと思います。\n複数 GKE クラスタでマルチクラスタメッシュを構築することにより、片方の GKE クラスタを先にバージョンアップし、サービスメッシュのトラフィック制御を使ってバージョンアップしたクラスタ側に少量のトラフィックを流して問題がないことを確認しながら段階的に比重をあげていく、といった「基盤部分も含めたカナリアリリース」のユースケースも容易に実現できるようになる見込みです。\nもちろん公式ドキュメントにもマルチクラスタメッシュの構築に関する記載はあるのですが、単にクラスタ間で分散されたことを確認しただけで終わっており、ルーティングの設定やメッシュの外からの通信に関する記載はなかったため、今回はここら辺も含めて一気通貫でご紹介したいと思います。もしこれから Anthos Service Mesh 環境の利用を検討している方は参考にしてみてはいかがでしょうか。\n構築するシステムについて # 次の図に示すように限定公開クラスタおよび承認済みネットワーク機能を有効化した単一リージョンの複数 GKE クラスタに対して Anthos Service Mesh (マネージドコントロールプレーン)を導入し、サービスメッシュ上でサンプルアプリケーションを動かしていきたいと思います。なお、今回の例では GKE、Anthos Service Mesh のいずれのリリースチャンネルについても安定性重視の Stable チャンネルを採用しています。\n   それでは構築していきましょう # 公式ドキュメントを参考にしつつ、公式ドキュメントに書かれていない部分を補足しながら構築をしていきたいと思います。\n Step1. VPC ネットワークの作成 # まずは GKE ノードを配置する VPC ネットワークおよび東京リージョンにサブネットを作成します。今回の例では GKE ノードからプライベートネットワーク経由で Artifact Registry などの他のマネージドサービスへアクセスできるように限定公開の Google アクセスをオンにしています。\n実行例）VPCネットワークの作成\n# 環境変数の設定 export NETWORK=\u0026#34;matt-vpc\u0026#34; export SUBNET=\u0026#34;matt-private-vm\u0026#34; export LOCATION=\u0026#34;asia-northeast1\u0026#34; export IP_RANGE=\u0026#34;172.16.0.0/16\u0026#34; # VPC ネットワークの作成 gcloud compute networks create ${NETWORK} --subnet-mode=custom # サブネットの作成 gcloud compute networks subnets create ${SUBNET} \\  --network=${NETWORK} --range=${IP_RANGE} --region=${LOCATION} \\  --enable-private-ip-google-access プライベートネットワーク経由でインターネット上の Docker Hub などへ接続できるよう Cloud NAT も作成しておきます。\n実行例）Cloud NATの作成\n# 環境変数の設定 export NAT_GATEWAY=\u0026#34;matt-tokyo-nat\u0026#34; export NAT_ROUTER=\u0026#34;matt-tokyo-router\u0026#34; # Cloud Router の作成 (東京リージョン) gcloud compute routers create ${NAT_ROUTER} \\  --network=${NETWORK} --region=${LOCATION} # Cloud NAT の作成 (東京リージョン) gcloud compute routers nats create ${NAT_GATEWAY} \\  --router=${NAT_ROUTER} \\  --router-region=${LOCATION} \\  --auto-allocate-nat-external-ips \\  --nat-all-subnet-ip-ranges \\  --enable-logging Step2. GKE クラスタの作成 # 次に GKE クラスタを作成していきましょう。 Anthos Service Mesh の導入には次のような前提条件を満たす必要があるため、今回はこちらを満たした上で、セキュリティの観点から限定公開クラスタおよび承認済みネットワークの有効化、安定性の観点から Stable チャンネルを指定しています。\n 4 vCPU 以上を搭載したノード 合計 8 vCPU 以上を搭載したノードプール GKE Workload Identity の有効化 GKE リリースチャンネルへの登録 (※1)  ※1: Anthos Service Mesh のマネージドコントロールプレーン機能を使う場合のみ\n実行例）GKEクラスタの作成\n# 環境変数の設定 export PROJECT_ID=`gcloud config list --format \u0026#34;value(core.project)\u0026#34;` export CLUSTER_1=\u0026#34;matt-tokyo-cluster-1\u0026#34; export CLUSTER_2=\u0026#34;matt-tokyo-cluster-2\u0026#34; export MASTER_IP_RANGE_1=\u0026#34;192.168.0.0/28\u0026#34; export MASTER_IP_RANGE_2=\u0026#34;192.168.8.0/28\u0026#34; export CTX_1=\u0026#34;gke_${PROJECT_ID}_${LOCATION}_${CLUSTER_1}\u0026#34; export CTX_2=\u0026#34;gke_${PROJECT_ID}_${LOCATION}_${CLUSTER_2}\u0026#34; # GKE クラスタ #1 の作成 gcloud container clusters create ${CLUSTER_1} \\  --region=${LOCATION} \\  --machine-type=\u0026#34;e2-standard-4\u0026#34; \\  --num-nodes=\u0026#34;1\u0026#34; \\  --enable-autoscaling --min-nodes=\u0026#34;1\u0026#34; --max-nodes=\u0026#34;3\u0026#34; \\  --enable-private-nodes --master-ipv4-cidr=${MASTER_IP_RANGE_1} \\  --enable-master-global-access \\  --enable-ip-alias --network=${NETWORK} --subnetwork=${SUBNET} \\  --enable-master-authorized-networks \\  --workload-pool=\u0026#34;${PROJECT_ID}.svc.id.goog\u0026#34; \\  --release-channel=\u0026#34;stable\u0026#34; # GKE クラスタ #2 の作成 gcloud container clusters create ${CLUSTER_2} \\  --region=${LOCATION} \\  --machine-type=\u0026#34;e2-standard-4\u0026#34; \\  --num-nodes=\u0026#34;1\u0026#34; \\  --enable-autoscaling --min-nodes=\u0026#34;1\u0026#34; --max-nodes=\u0026#34;3\u0026#34; \\  --enable-private-nodes --master-ipv4-cidr=${MASTER_IP_RANGE_2} \\  --enable-master-global-access \\  --enable-ip-alias --network=${NETWORK} --subnetwork=${SUBNET} \\  --enable-master-authorized-networks \\  --workload-pool=\u0026#34;${PROJECT_ID}.svc.id.goog\u0026#34; \\  --release-channel=\u0026#34;stable\u0026#34; 前提条件の詳細については次の公式ドキュメントをご参照ください。\n Step3. Anthos Service Mesh のインストール # (1) 管理ツールのダウンロード # 最初に Anthos Service Mesh v1.11 から正式な管理ツールとなった asmcli をダウンロードします。\n実行例）asmcliツールのダウンロード\ncurl https://storage.googleapis.com/csm-artifacts/asm/asmcli_1.11 \u0026gt; asmcli # 実行権限の付与 chmod +x asmcli (2) GKE クラスタ #1 へのインストール # まずは GKE クラスタ #1 に Anthos Service Mesh をインストールしていきましょう。Kubernetes API へ接続できるように GKE コントロールプレーンの承認済みネットワークに Cloud Shell の IP アドレスを登録し、kubectl を実行できるようにクラスタ認証情報を取得します。\n実行例）クラスタ認証情報の取得(クラスタ#1)\n# CloudShellの承認済みネットワーク登録 gcloud container clusters update ${CLUSTER_1} \\  --region ${LOCATION} \\  --enable-master-authorized-networks \\  --master-authorized-networks \\  \u0026#34;$(dig +short myip.opendns.com @resolver1.opendns.com)/32\u0026#34; # クラスタ認証情報の取得 gcloud container clusters get-credentials ${CLUSTER_1} \\  --region ${LOCATION} 次に asmcli を使って Anthos Service Mesh をインストールします。コマンドが完了するまでおおよそ 5 分程度かかりました。\n実行例）Anthos Service Meshのインストール(クラスタ#1)\n./asmcli install \\  --project_id ${PROJECT_ID} \\  --cluster_location ${LOCATION} \\  --cluster_name ${CLUSTER_1} \\  --managed \\  --channel \u0026#34;stable\u0026#34; \\  --enable-all \\  --output_dir ${CLUSTER_1} 次のようなメッセージが出力されましたらインストールに成功です。\n出力例）\nasmcli: Successfully installed ASM. (3) GKE クラスタ #2 へのインストール # 同様に GKE クラスタ #2 にも Anthos Service Mesh をインストールしましょう。\n実行例）Anthos Service Meshのインストール(クラスタ#2)\n# CloudShellの承認済みネットワーク登録 gcloud container clusters update ${CLUSTER_2} \\  --region ${LOCATION} \\  --enable-master-authorized-networks \\  --master-authorized-networks \\  \u0026#34;$(dig +short myip.opendns.com @resolver1.opendns.com)/32\u0026#34; # クラスタ認証情報の取得 gcloud container clusters get-credentials ${CLUSTER_2} \\  --region ${LOCATION} # Anthos Service Mesh のインストール ./asmcli install \\  --project_id ${PROJECT_ID} \\  --cluster_location ${LOCATION} \\  --cluster_name ${CLUSTER_2} \\  --managed \\  --channel \u0026#34;stable\u0026#34; \\  --enable-all \\  --output_dir ${CLUSTER_2} (4) ファイアウォールルールの更新 (限定公開クラスタ時のみ) # 限定公開クラスタに Anthos Service Mesh をインストールした場合は、コントロールプレーンからのポート 15017 による通信を追加で許可する必要があります。次のコマンド実行してコントロールプレーンからのポート 15017 による通信を許可します。\n実行例）ファイアウォールルールの更新(限定公開クラスタ時のみ)\n# 既存のファイアウォールルールに 15017/TCP の許可ルールを追加 (東京リージョン) gcloud compute firewall-rules update \\  $(gcloud compute firewall-rules list \\  --filter=\u0026#34;name~${CLUSTER_1}-.*-master\u0026#34; --format=\u0026#34;value(name)\u0026#34;) \\  --allow tcp:10250,tcp:443,tcp:15017 # 既存のファイアウォールルールに 15017/TCP の許可ルールを追加 (大阪リージョン) gcloud compute firewall-rules update \\  $(gcloud compute firewall-rules list \\  --filter=\u0026#34;name~${CLUSTER_2}-.*-master\u0026#34; --format=\u0026#34;value(name)\u0026#34;) \\  --allow tcp:10250,tcp:443,tcp:15017 Step4. マルチクラスタメッシュの設定 # (1) クラスタ間通信の許可 # クラスタをまたがってのサービス間通信ができるように次のコマンドを実行してファイアウォールルール \u0026quot;VPCネットワーク名\u0026quot;-istio-multicluster-pods を新たに作成します。\n実行例）クラスタ間通信の許可\n# 環境変数の設定 CLUSTER_1_CIDR=$(gcloud container clusters list \\  --filter=\u0026#34;name~${CLUSTER_1}\u0026#34; --format=\u0026#39;value(clusterIpv4Cidr)\u0026#39;) CLUSTER_2_CIDR=$(gcloud container clusters list \\  --filter=\u0026#34;name~${CLUSTER_2}\u0026#34; --format=\u0026#39;value(clusterIpv4Cidr)\u0026#39;) CLUSTER_1_NETTAG=$(gcloud compute instances list \\  --filter=\u0026#34;name~${CLUSTER_1::20}\u0026#34; --format=\u0026#39;value(tags.items.[0])\u0026#39; | \\  grep ${CLUSTER_1} | sort -u) CLUSTER_2_NETTAG=$(gcloud compute instances list \\  --filter=\u0026#34;name~${CLUSTER_2::20}\u0026#34; --format=\u0026#39;value(tags.items.[0])\u0026#39; | \\  grep ${CLUSTER_2} | sort -u) # クラスタ間通信を許可するファイアウォールルールの作成 gcloud compute firewall-rules create \u0026#34;${NETWORK}-istio-multicluster-pods\u0026#34; \\  --network=${NETWORK} \\  --allow=tcp,udp,icmp,esp,ah,sctp \\  --direction=INGRESS \\  --priority=900 \\  --source-ranges=\u0026#34;${CLUSTER_1_CIDR},${CLUSTER_2_CIDR}\u0026#34; \\  --target-tags=\u0026#34;${CLUSTER_1_NETTAG},${CLUSTER_2_NETTAG}\u0026#34; (2) クラスタ間サービスディスカバリの設定 # 次のコマンドを実行し、クラスタ間でサービスの自動検出ができるように asmcli を使って設定を行います。\n実行例）クラスタ間サービスディスカバリの設定\n./asmcli create-mesh ${PROJECT_ID} \\  ${PROJECT_ID}/${LOCATION}/${CLUSTER_1} \\  ${PROJECT_ID}/${LOCATION}/${CLUSTER_2} (3) シークレット情報の更新 (限定公開クラスタ時のみ) # 限定公開クラスタで構築した場合は、Anthos Service Mesh コントロールプレーンから他の GKE クラスタコントロールプレーンへプライベートネットワーク経由でアクセスできるようにシークレット情報を書き換えましょう。\n実行例）シークレット情報の更新(限定公開クラスタ時のみ)\n# 環境変数の設定 CLUSTER_1_PRIV_IP=$(gcloud container clusters describe \u0026#34;${CLUSTER_1}\u0026#34; \\  --region \u0026#34;${LOCATION}\u0026#34; --format \u0026#34;value(privateClusterConfig.privateEndpoint)\u0026#34;) CLUSTER_2_PRIV_IP=$(gcloud container clusters describe \u0026#34;${CLUSTER_2}\u0026#34; \\  --region \u0026#34;${LOCATION}\u0026#34; --format \u0026#34;value(privateClusterConfig.privateEndpoint)\u0026#34;) # プライベートエンドポイントに書き換えたシークレット情報の作成 (クラスタ#1) ./${CLUSTER_1}/istioctl x create-remote-secret \\  --context=${CTX_1} --name=${CLUSTER_1} \\  --server=https://${CLUSTER_1_PRIV_IP} \u0026gt; ${CTX_1}.secret # プライベートエンドポイントに書き換えたシークレット情報の作成 (クラスタ#2) ./${CLUSTER_1}/istioctl x create-remote-secret \\  --context=${CTX_2} --name=${CLUSTER_2} \\  --server=https://${CLUSTER_2_PRIV_IP} \u0026gt; ${CTX_2}.secret # シークレット情報の更新 (クラスタ#1) kubectl apply -f ${CTX_2}.secret --context=${CTX_1} # シークレット情報の更新 (クラスタ#2) kubectl apply -f ${CTX_1}.secret --context=${CTX_2} ここまで終わりましたらクラスタ間で Kubernetes サービスがロードバランシングされるようになります。\n(4) クラスタ間ロードバランシングの動作確認 # 構築はまだ続きますがいったんこの状態で、Anthos Service Mesh をインストールした際に \u0026ndash;output_dir で指定したディレクトリへ格納されているサンプルアプリケーションの中から HelloWorld と Sleep というアプリケーションを使用して、クラスタ間で負荷が分散されることを実際に確認していきたいと思います。サンプルアプリケーションの詳細につきましては次の URL をご参照ください。\n 現時点では何もルーティング設定をしていないため、次の図のように 50% ずつトラフィックが振り分けられる状態を確認できるかと思います。\n   (a) サンプルアプリケーションのデプロイ\nそれではサンプルアプリケーションをデプロイしていきましょう。まずは次のコマンドでサンプルアプリケーション用の Namespace を新たに作成します。\n実行例）サンプルアプリケーション用Namespaceの作成\n# 環境変数の設定 export SAMPLE_NAMESPACE=\u0026#34;sample\u0026#34; # 両クラスタにサンプルアプリケーション用 Namespace リソースの作成 for CTX in ${CTX_1} ${CTX_2} do kubectl create --context=${CTX} namespace ${SAMPLE_NAMESPACE} kubectl label --context=${CTX} namespace ${SAMPLE_NAMESPACE} \\  istio.io/rev=asm-managed-stable --overwrite done 次に HelloWorld および Sleep アプリケーションをデプロイしましょう。どちらのクラスタ上の Pod に振り分けられたかをわかりやすくするため、クラスタ #1 に HelloWorld アプリケーションの v1、クラスタ #2 に v2 をデプロイしています。\n実行例）サンプルアプリケーションのデプロイ\n# 両クラスタに HelloWorld サービスのデプロイ for CTX in ${CTX_1} ${CTX_2} do kubectl apply --context=${CTX} -n ${SAMPLE_NAMESPACE} \\  -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\  -l service=\u0026#34;helloworld\u0026#34; done # クラスタ #1 に HelloWorld アプリケーションの v1 をデプロイ kubectl apply --context=${CTX_1} -n ${SAMPLE_NAMESPACE} \\  -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\  -l version=\u0026#34;v1\u0026#34; # クラスタ #2 に HelloWorld アプリケーションの v2 をデプロイ kubectl apply --context=${CTX_2} -n ${SAMPLE_NAMESPACE} \\  -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\  -l version=\u0026#34;v2\u0026#34; # 両クラスタに Sleep サービス、アプリケーションのデプロイ for CTX in ${CTX_1} ${CTX_2} do kubectl apply --context=${CTX} -n ${SAMPLE_NAMESPACE} \\  -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/sleep/sleep.yaml done (b) サービス間通信の実行\nそれでは Sleep アプリケーションから HelloWorld アプリケーションへのサービス間通信をしてみましょう。次のコマンドでは各クラスタ上の Sleep アプリケーションからそれぞれ 10 回ずつ HelloWorld サービスへの通信を実施しています。\n実行例）サービス間通信の実行例\nfor CTX in ${CTX_1} ${CTX_2} do for x in `seq 1 10` do kubectl exec --context=\u0026#34;${CTX}\u0026#34; -n sample -c sleep \\  \u0026#34;$(kubectl get pod --context=\u0026#34;${CTX}\u0026#34; -n sample -l \\  app=sleep -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;)\u0026#34; \\  -- curl -sS helloworld.${SAMPLE_NAMESPACE}:5000/hello done echo \u0026#39;---\u0026#39; done 次の出力例のように両クラスタから v1 と v2 の Pod へランダムで 50% ずつトラフィックが振り分けられる状態を確認できるかと思います。\n出力例）\nHello version: v1, instance: helloworld-v1-776f57d5f6-62c9f Hello version: v1, instance: helloworld-v1-776f57d5f6-62c9f Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb : --- : Hello version: v1, instance: helloworld-v1-776f57d5f6-62c9f Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v1, instance: helloworld-v1-776f57d5f6-62c9f 以上でクラスタ間ロードバランシングの動作確認は完了です。\nStep5. 高度なクラスタ間ロードバランシングの設定 # 次にもう少し高度なクラスタ間ロードバランシングの設定をしていきましょう。ユースケースはいくつかありますが、今回は次の図のように GKE クラスタ #2 側の GKE クラスタのアップグレード後にアプリケーションの動作に影響がないかを少量のトラフィックを流して確認し、問題ないことを確認できたらその比重を段階的にあげていく、といったカナリアリリースのシナリオを想定した振り分け制御を行っていきたいと思います。\n   (1) Istio リソースのデプロイ # それでは設定していきましょう。まずは Istio VirtualService リソース1および Istio DestinationRule リソース2の定義ファイルを作成しましょう。例のように VirtualService リソースにサブセットごとの振り分けの重みづけを、DestinationRule リソースにサブセットの定義をします。\n作成例）helloworld-virtualservice.yaml\napiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:helloworldspec:hosts:- helloworldhttp:- route:- destination:host:helloworldsubset:v1weight:80- destination:host:helloworldsubset:v2weight:20作成例）helloworld-destinationrule.yaml\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:helloworldspec:host:helloworldsubsets:- name:v1labels:version:v1- name:v2labels:version:v2次のコマンドで両クラスタに Istio リソースをデプロイしましょう。これで設定は終わりです。\n実行例）Istioリソースのデプロイ\n# 両クラスタに VirtualService リソースをデプロイ for CTX in ${CTX_1} ${CTX_2} do kubectl apply --context=${CTX} -n ${SAMPLE_NAMESPACE} \\  -f helloworld-virtualservice.yaml done # 両クラスタに DestinationRule リソースをデプロイ for CTX in ${CTX_1} ${CTX_2} do kubectl apply --context=${CTX} -n ${SAMPLE_NAMESPACE} \\  -f helloworld-destinationrule.yaml done (2) カナリアリリースの動作確認 # 構築はまだ続きますがいったんこの状態で、クラスタ間で負荷が設定どおりの比重で分散されることを実際に確認していきたいと思います。クラスタ間ロードバランシングの動作確認と同様に Sleep アプリケーションから HelloWorld アプリケーションへのサービス間通信をしてみましょう。\n実行例）サービス間通信の実行例\nfor x in `seq 1 10` do kubectl exec --context=\u0026#34;${CTX_1}\u0026#34; -n sample -c sleep \\  \u0026#34;$(kubectl get pod --context=\u0026#34;${CTX_1}\u0026#34; -n sample -l \\  app=sleep -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;)\u0026#34; \\  -- curl -sS helloworld.${SAMPLE_NAMESPACE}:5000/hello done 10 回の実行では試行回数が少ないため誤差はあるかと思いますが、出力例のように v1 への振り分けが約 80%、v2 への振り分けが約 20% となることが確認できるかと思います。\n出力例）\nHello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v 以上で少し高度なクラスタ間ロードバランシング設定の動作確認もできました。\nStep6. Ingress ゲートウェイの設定 # さてここからは話題がガラッと変わり、メッシュの外からの通信を受け入れるための Ingress ゲートウェイの設定をしていきたいと思います。今回は次の図のように各クラスタに配置された Ingress ゲートウェイアプリケーションを束ねるようにマルチクラスタ Ingress およびマルチクラスタ Service を配置する構成を作っていきます。\n   (1) マルチクラスタ Ingress 機能の有効化 # 最初に次のコマンドを実行し、マルチクラスタ Ingress 機能を有効化しましょう。なお、今回はマルチクラスタ Ingress の設定を行うメインの GKE クラスタ(=構成クラスタ)として、GKE クラスタ #1 を登録しています。\n実行例）マルチクラスタIngress機能の有効化\ngcloud container hub ingress enable \\  --config-membership=${CLUSTER_1} (2) Ingress ゲートウェイ定義ファイルの作成 # 今回は Anthos Service Mesh をインストールした際に --output_dir で指定したディレクトリへ Ingress ゲートウェイのサンプル定義ファイルが配置されているのでこちらをベースに作成していきたいと思います。まずはサンプル定義ファイルを複製し、マルチクラスタ Ingress 構成向けに MultiClusterService3、BackendConfig4、MultiClusterIngress5 の 3 種類のリソース定義ファイルを追加していきましょう。\n実行例）Ingressゲートウェイ定義ファイルの作成準備\n# サンプル定義ファイルを複製 cp -r ${CLUSTER_1}/samples/gateways/istio-ingressgateway . # マルチクラスタ Ingress 定義ファイルを格納するディレクトリを作成 mkdir -p istio-ingressgateway/multicluster # Service リソース定義から MultiClusterService リソース定義ファイルに書き換え mv istio-ingressgateway/service.yaml istio-ingressgateway/multicluster/multiclusterservice.yaml # 新たな定義ファイルを 2 種類作成 touch istio-ingressgateway/multicluster/backendconfig.yaml touch istio-ingressgateway/multicluster/multiclusteringress.yaml それでは MultiClusterService3、BackendConfig4、MultiClusterIngress5 の 3 種類のリソース定義ファイルを編集していきましょう。MultiClusterService は Service リソースをマルチクラスタに対応させたリソースという位置づけのため、基本的に Service リソースの設定値とほぼ変わりません。今回は Ingress をフロントに配置するので LoadBalancer タイプの定義を削除し、デフォルトの Cluster IP に変更しています。\n作成例）istio-ingressgateway/multicluster/multiclusterservice.yaml（差分）\n- apiVersion: v1 + apiVersion: networking.gke.io/v1 - kind: Service + kind: MultiClusterService  metadata: name: istio-ingressgateway + annotations: + cloud.google.com/backend-config: \u0026#39;{\u0026#34;default\u0026#34;: \u0026#34;ingress-backendconfig\u0026#34;}\u0026#39;  labels: app: istio-ingressgateway istio: ingressgateway spec: - ports: - # status-port exposes a /healthz/ready endpoint that can be used with GKE Ingress health checks - - name: status-port - port: 15021 - protocol: TCP - targetPort: 15021 - # Any ports exposed in Gateway resources should be exposed here. - - name: http2 - port: 80 - - name: https - port: 443 - selector: - istio: ingressgateway - app: istio-ingressgateway - type: LoadBalancer + template: + spec: + ports: + # status-port exposes a /healthz/ready endpoint that can be used with GKE Ingress health checks + - name: status-port + port: 15021 + protocol: TCP + targetPort: 15021 + # Any ports exposed in Gateway resources should be exposed here. + - name: http2 + port: 80 + - name: https + port: 443 + selector: + istio: ingressgateway + app: istio-ingressgateway BackendConfig リソースではバックエンドサービスである Ingress ゲートウェイアプリケーションのヘルスチェックに関する定義を記載します。Ingress ゲートウェイはヘルスチェック用パスとして /healthz/ready:15021 を用意しているため、こちらを設定しましょう。\n作成例）istio-ingressgateway/multicluster/backendconfig.yaml（差分）\n+ apiVersion: cloud.google.com/v1 + kind: BackendConfig + metadata: + name: ingress-backendconfig + spec: + healthCheck: + requestPath: /healthz/ready + port: 15021 + type: HTTP MultiClusterIngress は Ingress リソースをマルチクラスタに対応させたリソースという位置づけであり、基本的に Ingress リソースを定義するときと設定値はほぼ同じです。\n作成例）istio-ingressgateway/multicluster/multiclusteringress.yaml（差分）\n+ apiVersion: networking.gke.io/v1beta1 + kind: MultiClusterIngress + metadata: + name: istio-ingressgateway + labels: + app: istio-ingressgateway + istio: ingressgateway + spec: + template: + spec: + backend: + serviceName: istio-ingressgateway + servicePort: 80 (3) Ingress ゲートウェイのデプロイ # まずは Ingress ゲートウェイリソースをデプロイする Namespace を新たに作成します。今回の例では istio-gateway という名前の Namespace を作成しています。\n実行例）Ingress Gateway用のNamespace作成\n# 環境変数の設定 export GATEWAY_NAMESPACE=\u0026#34;istio-gateway\u0026#34; # 両クラスタにサンプルアプリケーション用 Namespace リソースの作成 for CTX in ${CTX_1} ${CTX_2} do kubectl create --context=${CTX} namespace ${GATEWAY_NAMESPACE} kubectl label --context=${CTX} namespace ${GATEWAY_NAMESPACE} \\  istio.io/rev=asm-managed-stable --overwrite done 次のコマンドを実行して Ingress ゲートウェイアプリケーションを両クラスタにデプロイしましょう。\n実行例）Ingress Gatewayアプリケーションのデプロイ\nfor CTX in ${CTX_1} ${CTX_2} do kubectl apply -n ${GATEWAY_NAMESPACE} --context=${CTX} \\  -f istio-ingressgateway done 最後にマルチクラスタ Ingress リソースを構成クラスタである GKE クラスタ #1 に対してデプロイをしましょう。\n実行例）Ingress Gatewayアプリケーションのデプロイ\nkubectl apply -n ${GATEWAY_NAMESPACE} --context=${CTX_1} \\  -f istio-ingressgateway/multicluster 以上で Ingress ゲートウェイのデプロイは終わりです。\n(4) Istio リソースのデプロイ # Ingress ゲートウェイを通じてメッシュの外から HelloWorld アプリケーションへ通信ができるように Istio リソースの定義を行っていきたいと思います。まずは Istio Gateway リソース6および Istio VirtualService リソース1の定義ファイルを作成しましょう。例のように Gateway リソースにメッシュ外から受け付けるポートとプロトコルを定義し、VirtualService リソースには Gateway に入ってきた通信のパターンマッチ条件と振り分け先バックエンドの指定をします。\n作成例）helloworld-gateway.yaml\napiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:helloworld-gatewayspec:selector:istio:ingressgatewayservers:- port:number:80name:httpprotocol:HTTPhosts:- \u0026#34;*\u0026#34;---apiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:helloworld-gatewayspec:hosts:- \u0026#34;*\u0026#34;gateways:- helloworld-gatewayhttp:- match:- uri:exact:/helloroute:- destination:host:helloworldport:number:5000次のコマンドで両クラスタに Istio リソースをデプロイし、アプリケーションへのインバウンド通信ができるように設定しましょう。\n実行例）Istioリソースのデプロイ\nfor CTX in ${CTX_1} ${CTX_2} do kubectl apply -n ${SAMPLE_NAMESPACE} --context=${CTX} \\  -f helloworld-gateway.yaml done 以上でメッシュ外からのアプリケーションへのインバウンド通信もできるようになりました。\n(5) インバウンド通信の動作確認 # それではメッシュ外からのアプリケーションへのインバウンド通信ができることを確認していきましょう。実行例のように Ingress ゲートウェイの外部 IP アドレスに対して curl コマンドを実行し、アクセスをしてみましょう。\n実行例）インバウンド通信の実行\n# Ingress ゲートウェイの外部 IP アドレスの取得 INGRESS_GATEWAY_IP=$(kubectl --context=${CTX_1} \\  -n ${GATEWAY_NAMESPACE} get MultiClusterIngress \\  -o custom-columns=VIP:status.VIP --no-headers) for x in `seq 1 10` do curl http://${INGRESS_GATEWAY_IP}/hello done 次の出力例のように両クラスタから v1 と v2 の Pod へランダムで 50% ずつトラフィックが振り分けられる状態を確認できるかと思います。\n出力例）\nHello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Step7. インバウンド通信に対する高度なロードバランシングの設定 # メッシュの外からアプリケーションへのインバウンド通信に対する高度なロードバランシングの設定をしていきましょう。今回は Step5 のときとは逆に 80% を v2、20% を v1 に割り振るように設定していきたいと思います。\n   (1) Istio リソースの更新 # それでは先ほど作成した Istio リソース定義ファイル helloworld-gateway.yaml の VirtualService リソース部分を編集し、サブセットごとの振り分け比重の定義を追加します。なお、サブセットの定義(DestinationRule リソース)については「Step5. 高度なクラスタ間ロードバランシングの設定」にて設定済みとなりますのでここでは省略します。\n作成例）helloworld-gateway.yaml（差分）\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: helloworld-gateway spec: hosts: - \u0026#34;*\u0026#34; gateways: - helloworld-gateway http: - match: - uri: exact: /hello route: - destination: host: helloworld port: number: 5000 + subset: v1 + weight: 20 + - destination: + host: helloworld + port: + number: 5000 + subset: v2 + weight: 80 それでは次のコマンドで Istio リソースを更新しましょう。これで設定は終わりです。\n実行例）Istioリソースの更新\nfor CTX in ${CTX_1} ${CTX_2} do kubectl apply -n ${SAMPLE_NAMESPACE} --context=${CTX} \\  -f helloworld-gateway.yaml done (2) カナリアリリースの動作確認 # それでは HelloWorld アプリケーションへの振り分けが設定どおりの比重で分散されることを実際に確認していきたいと思います。実行例のように Ingress ゲートウェイの外部 IP アドレスに対して curl コマンドを実行し、アクセスをしてみましょう。\n実行例）インバウンド通信の実行\n# Ingress ゲートウェイの外部 IP アドレスの取得 INGRESS_GATEWAY_IP=$(kubectl --context=${CTX_1} \\  -n ${GATEWAY_NAMESPACE} get MultiClusterIngress \\  -o custom-columns=VIP:status.VIP --no-headers) for x in `seq 1 10` do curl http://${INGRESS_GATEWAY_IP}/hello done 10 回の実行では試行回数が少ないため誤差はあるかと思いますが、出力例のように v1 への振り分けが約 20%、v2 への振り分けが約 80% となることが確認できるかと思います。\n出力例）\nHello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v1, instance: helloworld-v1-58d756cf5d-bs22v Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb Hello version: v2, instance: helloworld-v2-54df5f84b-ffpmb 以上でメッシュの外からアプリケーションへのインバウンド通信に対する高度なロードバランシングの動作確認も終了です。お疲れ様でした。\n終わりに # 今回は単一リージョンに展開した複数 GKE クラスタを単一の Anthos Service Mesh 環境に追加し、GKE クラスタ間で負荷分散を行う方法についてご紹介でしたがいかがだったでしょうか。\n複数 GKE クラスタでマルチクラスタメッシュを構築することにより、片方の GKE クラスタを先にバージョンアップし、サービスメッシュのトラフィック制御を使ってバージョンアップしたクラスタ側に少量のトラフィックを流して問題がないことを確認しながら段階的に比重をあげていく、といった「基盤部分も含めたカナリアリリース」のユースケースも容易に実現できるようになる見込みです。もしこれから Anthos Service Mesh 環境の利用を検討している方はマルチクラスタメッシュ構成についても検討してみてはいかがでしょうか。\n  Google Cloud は、Google LLC の商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。    VirtualService はトラフィックの振り分け、ルーティングを定義する Istio リソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n DestinationRule は転送先サービスのサブセット化や各種トラフィックポリシーを定義する Istio リソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n MultiClusterService は Service リソースを複数のクラスタ上に展開する GKE 独自のカスタムリソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n BackendConfig はバックエンドサービスのヘルスチェックを定義する GKE 独自のカスタムリソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n MultiClusterIngress はマルチクラスタに対応した Ingress リソースを定義する GKE 独自のカスタムリソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Gateway は Ingress/Egress ゲートウェイで受け付けるポート、プロトコルを定義する Istio リソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":"December 9, 2021","permalink":"/posts/2021/12/gcp-multi-asm-cluster/","section":"記事一覧","summary":"みなさん、こんにちは。今回は単一リージョンに展開した複数 GKE クラスタを単一の Anthos Service Mesh 環境に追加し、GKE クラスタ間で負荷分散を行う方法についてご紹介していきたいと思います。","title":"単一リージョンの複数 GKE クラスタと Anthos Service Mesh でマルチクラスタメッシュ環境を構築してみた"},{"content":"みなさん、こんにちは。今回は複数のリージョンに展開する各 GKE クラスタを単一の Anthos Service Mesh 環境に追加し、GKE クラスタ間で負荷分散を行う方法についてご紹介していきたいと思います。\n複数リージョンの GKE クラスタでマルチクラスタメッシュを構築することにより、予期しない大規模災害の発生にも耐えうる高い可用性と回復力の実現、エンドユーザからより近い位置への振り分けによるレイテンシの改善といったことが期待できるようになる見込みです。\nもちろん公式ドキュメントにもマルチクラスタメッシュの構築に関する記載はあるのですが、単にクラスタ間で分散されたことを確認しただけで終わっており、ローカリティを意識したルーティングの設定やメッシュの外からの通信に関する記載はなかったため、今回はここら辺も含めて一気通貫でご紹介したいと思います。もしこれからリージョンをまたがった Anthos Service Mesh 環境の利用を検討している方は参考にしてみてはいかがでしょうか。\n構築するシステムについて # 次の図に示すように限定公開クラスタおよび承認済みネットワーク機能を有効化した複数リージョンの GKE クラスタに対して Anthos Service Mesh (マネージドコントロールプレーン)を導入しています。サービスメッシュ上ではサンプルアプリケーションを動かし、ローカリティを意識した負荷分散についても設定をしていきたいと思います。なお、今回の例では GKE、Anthos Service Mesh のいずれのリリースチャンネルについても安定性重視の Stable チャンネルを採用しています。\n   それでは構築していきましょう # いつも通り公式ドキュメントを参考にしつつ、公式ドキュメントに書かれていない部分を補足しながら構築をしていきたいと思います。\n Step1. VPC ネットワークの作成 # まずは GKE ノードを配置する VPC ネットワークおよび東京リージョンと大阪リージョンにサブネットを作成します。今回の例では GKE ノードからプライベートネットワーク経由で Artifact Registry などの他のマネージドサービスへアクセスできるように限定公開の Google アクセスをオンにしています。\n実行例）VPCネットワークの作成\n# 環境変数の設定 export NETWORK=\u0026#34;matt-vpc\u0026#34; export SUBNET=\u0026#34;matt-private-vm\u0026#34; export LOCATION_1=\u0026#34;asia-northeast1\u0026#34; export LOCATION_2=\u0026#34;asia-northeast2\u0026#34; export IP_RANGE_1=\u0026#34;172.16.0.0/16\u0026#34; export IP_RANGE_2=\u0026#34;172.24.0.0/16\u0026#34; # VPC ネットワークの作成 gcloud compute networks create ${NETWORK} --subnet-mode=custom # サブネットの作成 (東京リージョン) gcloud compute networks subnets create ${SUBNET} \\  --network=${NETWORK} --range=${IP_RANGE_1} --region=${LOCATION_1} \\  --enable-private-ip-google-access # サブネットの作成 (大阪リージョン) gcloud compute networks subnets create ${SUBNET} \\  --network=${NETWORK} --range=${IP_RANGE_2} --region=${LOCATION_2} \\  --enable-private-ip-google-access プライベートネットワーク経由でインターネット上の Docker Hub などへ接続できるよう Cloud NAT も作成しておきます。\n実行例）Cloud NATの作成\n# 環境変数の設定 export NAT_GATEWAY_1=\u0026#34;matt-tokyo-nat\u0026#34; export NAT_GATEWAY_2=\u0026#34;matt-osaka-nat\u0026#34; export NAT_ROUTER_1=\u0026#34;matt-tokyo-router\u0026#34; export NAT_ROUTER_2=\u0026#34;matt-osaka-router\u0026#34; # Cloud Router の作成 (東京リージョン) gcloud compute routers create ${NAT_ROUTER_1} \\  --network=${NETWORK} --region=${LOCATION_1} # Cloud Router の作成 (大阪リージョン) gcloud compute routers create ${NAT_ROUTER_2} \\  --network=${NETWORK} --region=${LOCATION_2} # Cloud NAT の作成 (東京リージョン) gcloud compute routers nats create ${NAT_GATEWAY_1} \\  --router=${NAT_ROUTER_1} \\  --router-region=${LOCATION_1} \\  --auto-allocate-nat-external-ips \\  --nat-all-subnet-ip-ranges \\  --enable-logging # Cloud NAT の作成 (大阪リージョン) gcloud compute routers nats create ${NAT_GATEWAY_2} \\  --router=${NAT_ROUTER_2} \\  --router-region=${LOCATION_2} \\  --auto-allocate-nat-external-ips \\  --nat-all-subnet-ip-ranges \\  --enable-logging Step2. GKE クラスタの作成 # 次に GKE クラスタを作成していきましょう。 Anthos Service Mesh の導入には次のような前提条件を満たす必要があるため、今回はこちらを満たした上で、セキュリティの観点から限定公開クラスタおよび承認済みネットワークの有効化、安定性の観点から Stable チャンネルを指定しています。\n 4 vCPU 以上を搭載したノード 合計 8 vCPU 以上を搭載したノードプール GKE Workload Identity の有効化 GKE リリースチャンネルへの登録 (※1)  ※1: Anthos Service Mesh のマネージドコントロールプレーン機能を使う場合のみ\n実行例）GKEクラスタの作成\n# 環境変数の設定 export PROJECT_ID=`gcloud config list --format \u0026#34;value(core.project)\u0026#34;` export CLUSTER_1=\u0026#34;matt-tokyo-cluster-1\u0026#34; export CLUSTER_2=\u0026#34;matt-osaka-cluster-1\u0026#34; export MASTER_IP_RANGE_1=\u0026#34;192.168.0.0/28\u0026#34; export MASTER_IP_RANGE_2=\u0026#34;192.168.8.0/28\u0026#34; export CTX_1=\u0026#34;gke_${PROJECT_ID}_${LOCATION_1}_${CLUSTER_1}\u0026#34; export CTX_2=\u0026#34;gke_${PROJECT_ID}_${LOCATION_2}_${CLUSTER_2}\u0026#34; # GKE クラスタ(東京リージョン)の作成 gcloud container clusters create ${CLUSTER_1} \\  --region=${LOCATION_1} \\  --machine-type=\u0026#34;e2-standard-4\u0026#34; \\  --num-nodes=\u0026#34;1\u0026#34; \\  --enable-autoscaling --min-nodes=\u0026#34;1\u0026#34; --max-nodes=\u0026#34;3\u0026#34; \\  --enable-private-nodes --master-ipv4-cidr=${MASTER_IP_RANGE_1} \\  --enable-master-global-access \\  --enable-ip-alias --network=${NETWORK} --subnetwork=${SUBNET} \\  --enable-master-authorized-networks \\  --workload-pool=\u0026#34;${PROJECT_ID}.svc.id.goog\u0026#34; \\  --release-channel=\u0026#34;stable\u0026#34; # GKE クラスタ(大阪リージョン)の作成 gcloud container clusters create ${CLUSTER_2} \\  --region=${LOCATION_2} \\  --machine-type=\u0026#34;e2-standard-4\u0026#34; \\  --num-nodes=\u0026#34;1\u0026#34; \\  --enable-autoscaling --min-nodes=\u0026#34;1\u0026#34; --max-nodes=\u0026#34;3\u0026#34; \\  --enable-private-nodes --master-ipv4-cidr=${MASTER_IP_RANGE_2} \\  --enable-master-global-access \\  --enable-ip-alias --network ${NETWORK} --subnetwork=${SUBNET} \\  --enable-master-authorized-networks \\  --workload-pool=\u0026#34;${PROJECT_ID}.svc.id.goog\u0026#34; \\  --release-channel=\u0026#34;stable\u0026#34; 前提条件の詳細については次の公式ドキュメントをご参照ください。\n Step3. Anthos Service Mesh のインストール # (1) 管理ツールのダウンロード # 最初に Anthos Service Mesh v1.11 から正式な管理ツールとなった asmcli をダウンロードします。\n実行例）asmcliツールのダウンロード\ncurl https://storage.googleapis.com/csm-artifacts/asm/asmcli_1.11 \u0026gt; asmcli # 実行権限の付与 chmod +x asmcli (2) 東京 GKE クラスタへのインストール # まずは東京リージョンの GKE クラスタに Anthos Service Mesh をインストールしていきましょう。Kubernetes API へ接続できるように GKE コントロールプレーンの承認済みネットワークに Cloud Shell の IP アドレスを登録し、kubectl を実行できるようにクラスタ認証情報を取得します。\n実行例）クラスタ認証情報の取得(東京リージョン)\n# CloudShellの承認済みネットワーク登録 gcloud container clusters update ${CLUSTER_1} \\  --region ${LOCATION_1} \\  --enable-master-authorized-networks \\  --master-authorized-networks \\  \u0026#34;$(dig +short myip.opendns.com @resolver1.opendns.com)/32\u0026#34; # クラスタ認証情報の取得 gcloud container clusters get-credentials ${CLUSTER_1} \\  --region ${LOCATION_1} 次に asmcli を使って Anthos Service Mesh をインストールします。コマンドが完了するまでおおよそ 5 分程度かかりました。\n実行例）Anthos Service Meshのインストール(東京リージョン)\n./asmcli install \\  --project_id ${PROJECT_ID} \\  --cluster_location ${LOCATION_1} \\  --cluster_name ${CLUSTER_1} \\  --managed \\  --channel \u0026#34;stable\u0026#34; \\  --enable-all \\  --output_dir ${CLUSTER_1} 次のようなメッセージが出力されましたらインストールに成功です。\n出力例）インストール成功時\nasmcli: Successfully installed ASM. (3) 大阪 GKE クラスタへのインストール # 同様に大阪リージョンの GKE クラスタにもインストールをしましょう。\n実行例）Anthos Service Meshのインストール(大阪リージョン)\n# CloudShellの承認済みネットワーク登録 gcloud container clusters update ${CLUSTER_2} \\  --region ${LOCATION_2} \\  --enable-master-authorized-networks \\  --master-authorized-networks \\  \u0026#34;$(dig +short myip.opendns.com @resolver1.opendns.com)/32\u0026#34; # クラスタ認証情報の取得 gcloud container clusters get-credentials ${CLUSTER_2} \\  --region ${LOCATION_2} # Anthos Service Mesh のインストール ./asmcli install \\  --project_id ${PROJECT_ID} \\  --cluster_location ${LOCATION_2} \\  --cluster_name ${CLUSTER_2} \\  --managed \\  --channel \u0026#34;stable\u0026#34; \\  --enable-all \\  --output_dir ${CLUSTER_2} (4) ファイアウォールルールの更新 (限定公開クラスタ時のみ) # 限定公開クラスタに Anthos Service Mesh をインストールした場合は、コントロールプレーンからのポート 15017 による通信を追加で許可する必要があります。次のコマンド実行してコントロールプレーンからのポート 15017 による通信を許可します。\n実行例）ファイアウォールルールの更新\n# 既存のファイアウォールルールに 15017/TCP の許可ルールを追加 (東京リージョン) gcloud compute firewall-rules update \\  `gcloud compute firewall-rules list --filter=\u0026#34;name~${CLUSTER_1}-.*-master\u0026#34; --format=\u0026#34;value(name)\u0026#34;` \\  --allow tcp:10250,tcp:443,tcp:15017 # 既存のファイアウォールルールに 15017/TCP の許可ルールを追加 (大阪リージョン) gcloud compute firewall-rules update \\  `gcloud compute firewall-rules list --filter=\u0026#34;name~${CLUSTER_2}-.*-master\u0026#34; --format=\u0026#34;value(name)\u0026#34;` \\  --allow tcp:10250,tcp:443,tcp:15017 Step4. マルチクラスタメッシュの設定 # (1) クラスタ間通信の許可 # クラスタをまたがってのサービス間通信ができるように次のコマンドを実行してファイアウォールルール \u0026quot;VPCネットワーク名\u0026quot;-istio-multicluster-pods を新たに作成します。\n実行例）クラスタ間通信の許可\n# 環境変数の設定 CLUSTER_1_CIDR=$(gcloud container clusters list \\  --filter=\u0026#34;name~${CLUSTER_1}\u0026#34; --format=\u0026#39;value(clusterIpv4Cidr)\u0026#39;) CLUSTER_2_CIDR=$(gcloud container clusters list \\  --filter=\u0026#34;name~${CLUSTER_2}\u0026#34; --format=\u0026#39;value(clusterIpv4Cidr)\u0026#39;) CLUSTER_1_NETTAG=$(gcloud compute instances list \\  --filter=\u0026#34;name~${CLUSTER_1::16}\u0026#34; --format=\u0026#39;value(tags.items.[0])\u0026#39; | \\  grep ${CLUSTER_1} | sort -u) CLUSTER_2_NETTAG=$(gcloud compute instances list \\  --filter=\u0026#34;name~${CLUSTER_2::16}\u0026#34; --format=\u0026#39;value(tags.items.[0])\u0026#39; | \\  grep ${CLUSTER_2} | sort -u) # クラスタ間通信を許可するファイアウォールルールの作成 gcloud compute firewall-rules create \u0026#34;${NETWORK}-istio-multicluster-pods\u0026#34; \\  --network=${NETWORK} \\  --allow=tcp,udp,icmp,esp,ah,sctp \\  --direction=INGRESS \\  --priority=900 \\  --source-ranges=\u0026#34;${CLUSTER_1_CIDR},${CLUSTER_2_CIDR}\u0026#34; \\  --target-tags=\u0026#34;${CLUSTER_1_NETTAG},${CLUSTER_2_NETTAG}\u0026#34; (2) 承認済みネットワークの追加 (In-cluster かつ承認済みネットワーク有効時のみ) # 今回はマネージドコントロールプレーンを利用しているため設定は不要ですが、Anthos Service Mesh を In-cluster で構築した場合は Anthos Service Mesh コントロールプレーンから他 GKE クラスタのコントロールプレーンにアクセスする必要があるため承認済みネットワークを更新します。\n実行例）承認済みネットワークの追加（In-cluster時のみ）\n# 環境変数の設定 POD_IP_CIDR_1=$(gcloud container clusters describe ${CLUSTER_1} \\  --region ${LOCATION_1} --format \u0026#34;value(ipAllocationPolicy.clusterIpv4CidrBlock)\u0026#34;) POD_IP_CIDR_2=$(gcloud container clusters describe ${CLUSTER_2} \\  --region ${LOCATION_2} --format \u0026#34;value(ipAllocationPolicy.clusterIpv4CidrBlock)\u0026#34;) # 大阪リージョンの Pod アドレス範囲を、東京リージョンの承認済みネットワークに追加 gcloud container clusters update ${CLUSTER_1} \\  --region ${LOCATION_1} \\  --enable-master-authorized-networks \\  --master-authorized-networks \\  \u0026#34;${POD_IP_CIDR_2},$(dig +short myip.opendns.com @resolver1.opendns.com)/32\u0026#34; # 東京リージョンの Pod アドレス範囲を、大阪リージョンの承認済みネットワークに追加 gcloud container clusters update ${CLUSTER_2} \\  --region ${LOCATION_2} \\  --enable-master-authorized-networks \\  --master-authorized-networks \\  \u0026#34;${POD_IP_CIDR_1},$(dig +short myip.opendns.com @resolver1.opendns.com)/32\u0026#34; (3) クラスタ間サービスディスカバリの設定 # クラスタ間でサービスの自動検出ができるように asmcli を使って設定を行います。\n実行例）クラスタ間サービスディスカバリの設定\n./asmcli create-mesh ${PROJECT_ID} \\  ${PROJECT_ID}/${LOCATION_1}/${CLUSTER_1} \\  ${PROJECT_ID}/${LOCATION_2}/${CLUSTER_2} (4) シークレット情報の更新 (限定公開クラスタ時のみ) # 限定公開クラスタで構築した場合は、Anthos Service Mesh コントロールプレーンから他の GKE クラスタコントロールプレーンへプライベートネットワーク経由でアクセスできるようにシークレット情報を書き換えましょう。\n実行例）シークレット情報の更新(限定公開クラスタ時のみ)\n# 環境変数の設定 CLUSTER_1_PRIV_IP=$(gcloud container clusters describe \u0026#34;${CLUSTER_1}\u0026#34; \\  --region \u0026#34;${LOCATION_1}\u0026#34; --format \u0026#34;value(privateClusterConfig.privateEndpoint)\u0026#34;) CLUSTER_2_PRIV_IP=$(gcloud container clusters describe \u0026#34;${CLUSTER_2}\u0026#34; \\  --region \u0026#34;${LOCATION_2}\u0026#34; --format \u0026#34;value(privateClusterConfig.privateEndpoint)\u0026#34;) # プライベートエンドポイントに書き換えたシークレット情報の作成 (東京リージョン) ./${CLUSTER_1}/istioctl x create-remote-secret \\  --context=${CTX_1} --name=${CLUSTER_1} \\  --server=https://${CLUSTER_1_PRIV_IP} \u0026gt; ${CTX_1}.secret # プライベートエンドポイントに書き換えたシークレット情報の作成 (大阪リージョン) ./${CLUSTER_1}/istioctl x create-remote-secret \\  --context=${CTX_2} --name=${CLUSTER_2} \\  --server=https://${CLUSTER_2_PRIV_IP} \u0026gt; ${CTX_2}.secret # シークレット情報の更新 (東京リージョン) kubectl apply -f ${CTX_2}.secret --context=${CTX_1} # シークレット情報の更新 (大阪リージョン) kubectl apply -f ${CTX_1}.secret --context=${CTX_2} ここまで終わりましたらクラスタ間で Kubernetes サービスがロードバランシングされるようになります。\n(5) クラスタ間ロードバランシングの動作確認 # 構築はまだ続きますがいったんこの状態で、Anthos Service Mesh をインストールした際に \u0026ndash;output_dir で指定したディレクトリへ格納されているサンプルアプリケーションの中から HelloWorld と Sleep というアプリケーションを使用して、クラスタ間で負荷が分散されることを実際に確認していきたいと思います。サンプルアプリケーションの詳細につきましては次の URL をご参照ください。\n 現時点では何もルーティング設定をしていないため、次の図のように 50% ずつトラフィックが振り分けられる状態を確認できるかと思います。\n   (a) サンプルアプリケーションのデプロイ\nそれではサンプルアプリケーションをデプロイしていきましょう。まずは次のコマンドでサンプルアプリケーション用の Namespace を新たに作成します。\n実行例）サンプルアプリケーション用Namespaceの作成\n# 環境変数の設定 export SAMPLE_NAMESPACE=\u0026#34;sample\u0026#34; # 両クラスタにサンプルアプリケーション用 Namespace リソースの作成 for CTX in ${CTX_1} ${CTX_2} do kubectl create --context=${CTX} namespace ${SAMPLE_NAMESPACE} kubectl label --context=${CTX} namespace ${SAMPLE_NAMESPACE} \\  istio.io/rev=asm-managed-stable --overwrite done 次に HelloWorld および Sleep アプリケーションをデプロイしましょう。どちらのクラスタ上の Pod に振り分けられたかをわかりやすくするため、東京 GKE クラスタに HelloWorld アプリケーションの v1、大阪 GKE クラスタに v2 をデプロイしています。\n実行例）サンプルアプリケーションのデプロイ\n# 両クラスタに HelloWorld サービスのデプロイ for CTX in ${CTX_1} ${CTX_2} do kubectl apply --context=${CTX} -n ${SAMPLE_NAMESPACE} \\  -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\  -l service=\u0026#34;helloworld\u0026#34; done # 東京 GKE クラスタに HelloWorld アプリケーションの v1 をデプロイ kubectl apply --context=${CTX_1} -n ${SAMPLE_NAMESPACE} \\  -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\  -l version=\u0026#34;v1\u0026#34; # 大阪 GKE クラスタに HelloWorld アプリケーションの v2 をデプロイ kubectl apply --context=${CTX_2} -n ${SAMPLE_NAMESPACE} \\  -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\  -l version=\u0026#34;v2\u0026#34; # 両クラスタに Sleep サービス、アプリケーションのデプロイ for CTX in ${CTX_1} ${CTX_2} do kubectl apply --context=${CTX} -n ${SAMPLE_NAMESPACE} \\  -f ${CLUSTER_1}/istio-1.11.2-asm.17/samples/sleep/sleep.yaml done (b) サービス間通信の実行\nそれでは Sleep アプリケーションから HelloWorld アプリケーションへのサービス間通信をしてみましょう。次のコマンドでは各クラスタ上の Sleep アプリケーションからそれぞれ 10 回ずつ HelloWorld サービスへの通信を実施しています。\n実行例）サービス間通信の実行\nfor CTX in ${CTX_1} ${CTX_2} do echo \u0026#34;--- ${CTX}---\u0026#34; for x in `seq 1 10` do kubectl exec --context=\u0026#34;${CTX}\u0026#34; -n sample -c sleep \\  \u0026#34;$(kubectl get pod --context=\u0026#34;${CTX}\u0026#34; -n sample -l \\  app=sleep -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;)\u0026#34; \\  -- curl -sS helloworld.${SAMPLE_NAMESPACE}:5000/hello \\  -w \u0026#34;time_total:%{time_total}\\n\u0026#34; done done 次の出力例のように両クラスタから v1 と v2 の Pod へランダムで約 50% ずつトラフィックが振り分けられる状態を確認できるかと思います。また、応答時間については若干ですが東京からのアクセスは東京に配置される v1 の方が良く、大阪については v2 の方が良いことも確認できるかと思います。\n出力例）\n--- gke_${PROJECT_ID}_asia-northeast1_matt-tokyo-cluster-1 --- Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.135174 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.132497 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.175281 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.127934 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.157005 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.141171 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.132390 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.141507 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.129160 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.141614 --- gke_${PROJECT_ID}_asia-northeast2_matt-osaka-cluster-1 --- Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.171108 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.157021 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.137668 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.131152 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.135779 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.141066 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.135495 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.131946 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.135268 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.131380 以上でクラスタ間ロードバランシングの動作確認は完了です。\nStep5. ローカリティロードバランシングの設定 # 次にもう少し高度なクラスタ間ロードバランシングの設定をしていきましょう。ユースケースはいくつかありますが、今回は次の図のようにサービス間通信は基本的にリージョン内のアプリケーションへルーティングされるようにローカリティを意識した振り分け制御を行っていきたいと思います。\n   (1) Istio リソースのデプロイ # それでは設定していきましょう。まずは Istio DestinationRule リソース1の定義ファイルを作成しましょう。例のように DestinationRule リソースにて Outlier Detection(外れ値検知)の設定をすることでローカリティロードバランシングを有効化できます。\n作成例）helloworld-destinationrule.yaml\napiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:helloworldspec:host:helloworldtrafficPolicy:outlierDetection:consecutive5xxErrors:5interval:10sbaseEjectionTime:30s次のコマンドで両クラスタに Istio リソースをデプロイしましょう。これで設定は終わりです。\n実行例）Istioリソースのデプロイ\n# 両クラスタに DestinationRule リソースをデプロイ for CTX in ${CTX_1} ${CTX_2} do kubectl apply --context=${CTX} -n ${SAMPLE_NAMESPACE} \\  -f helloworld-destinationrule.yaml done for CTX in ${CTX_1} ${CTX_2} do kubectl delete --context=${CTX} -n ${SAMPLE_NAMESPACE} \\  -f helloworld-destinationrule.yaml done (2) ローカリティロードバランシングの動作確認 # 構築はまだ続きますがいったんこの状態で、クラスタ間で負荷が設定どおりの比重で分散されることを実際に確認していきたいと思います。クラスタ間ロードバランシングの動作確認と同様に Sleep アプリケーションから HelloWorld アプリケーションへのサービス間通信をしてみましょう。\n実行例）サービス間通信の実行\nfor CTX in ${CTX_1} ${CTX_2} do echo \u0026#34;--- ${CTX}---\u0026#34; for x in `seq 1 5` do kubectl exec --context=\u0026#34;${CTX}\u0026#34; -n sample -c sleep \\  \u0026#34;$(kubectl get pod --context=\u0026#34;${CTX}\u0026#34; -n sample -l \\  app=sleep -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;)\u0026#34; \\  -- curl -sS helloworld.${SAMPLE_NAMESPACE}:5000/hello \\  -w \u0026#34;time_total:%{time_total}\\n\u0026#34; done done 出力例のように東京リージョンの Sleep アプリケーションからのアクセスは v1 のみ、大阪リージョンからのアクセスは v2 のみに振り分けられることが確認できるかと思います。応答時間についてはあまり大きな差は見られないものの、ローカリティを意識しない場合と比較すると若干バラツキが少なくなったように感じます。\n出力例）\n--- gke_${PROJECT_ID}_asia-northeast1_matt-tokyo-cluster-1 --- Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.132195 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.134340 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.127857 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.142889 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.128125 --- gke_${PROJECT_ID}_asia-northeast2_matt-osaka-cluster-1 --- Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.136323 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.131857 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.130870 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.130531 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.131932 以上でローカリティロードバランシング設定の動作確認もできました。\nStep6. Ingress ゲートウェイの設定 # さてここからは話題がガラッと変わり、メッシュの外からの通信を受け入れるための Ingress ゲートウェイの設定をしていきたいと思います。今回は次の図のように各クラスタに配置された Ingress ゲートウェイアプリケーションを束ねるようにマルチクラスタ Ingress およびマルチクラスタ Service を配置する構成を作っていきます。\n   (1) マルチクラスタ Ingress 機能の有効化 # 最初に次のコマンドを実行し、マルチクラスタ Ingress 機能を有効化しましょう。なお、今回はマルチクラスタ Ingress の設定を行うメインの GKE クラスタ(=構成クラスタ)として、GKE クラスタ #1 を登録しています。\n実行例）マルチクラスタIngress機能の有効化\ngcloud container hub ingress enable \\  --config-membership=${CLUSTER_1} (2) Ingress ゲートウェイ定義ファイルの作成 # 今回は Anthos Service Mesh をインストールした際に --output_dir で指定したディレクトリへ Ingress ゲートウェイのサンプル定義ファイルが配置されているのでこちらをベースに作成していきたいと思います。まずはサンプル定義ファイルを複製し、マルチクラスタ Ingress 構成向けに MultiClusterService2、BackendConfig3、MultiClusterIngress4 の 3 種類のリソース定義ファイルを追加していきましょう。\n実行例）Ingressゲートウェイ定義ファイルの作成準備\n# サンプル定義ファイルを複製 cp -r ${CLUSTER_1}/samples/gateways/istio-ingressgateway . # マルチクラスタ Ingress 定義ファイルを格納するディレクトリを作成 mkdir -p istio-ingressgateway/multicluster # Service リソース定義から MultiClusterService リソース定義ファイルに書き換え mv istio-ingressgateway/service.yaml istio-ingressgateway/multicluster/multiclusterservice.yaml # 新たな定義ファイルを 2 種類作成 touch istio-ingressgateway/multicluster/backendconfig.yaml touch istio-ingressgateway/multicluster/multiclusteringress.yaml それでは MultiClusterService2、BackendConfig3、MultiClusterIngress4 の 3 種類のリソース定義ファイルを編集していきましょう。MultiClusterService は Service リソースをマルチクラスタに対応させたリソースという位置づけのため、基本的に Service リソースの設定値とほぼ変わりません。今回は Ingress をフロントに配置するので LoadBalancer タイプの定義を削除し、デフォルトの Cluster IP に変更しています。\n作成例）istio-ingressgateway/multicluster/multiclusterservice.yaml（差分）\n- apiVersion: v1 + apiVersion: networking.gke.io/v1 - kind: Service + kind: MultiClusterService  metadata: name: istio-ingressgateway + annotations: + cloud.google.com/backend-config: \u0026#39;{\u0026#34;default\u0026#34;: \u0026#34;ingress-backendconfig\u0026#34;}\u0026#39;  labels: app: istio-ingressgateway istio: ingressgateway spec: - ports: - # status-port exposes a /healthz/ready endpoint that can be used with GKE Ingress health checks - - name: status-port - port: 15021 - protocol: TCP - targetPort: 15021 - # Any ports exposed in Gateway resources should be exposed here. - - name: http2 - port: 80 - - name: https - port: 443 - selector: - istio: ingressgateway - app: istio-ingressgateway - type: LoadBalancer + template: + spec: + ports: + # status-port exposes a /healthz/ready endpoint that can be used with GKE Ingress health checks + - name: status-port + port: 15021 + protocol: TCP + targetPort: 15021 + # Any ports exposed in Gateway resources should be exposed here. + - name: http2 + port: 80 + - name: https + port: 443 + selector: + istio: ingressgateway + app: istio-ingressgateway BackendConfig リソースではバックエンドサービスである Ingress ゲートウェイアプリケーションのヘルスチェックに関する定義を記載します。Ingress ゲートウェイはヘルスチェック用パスとして /healthz/ready:15021 を用意しているため、こちらを設定しましょう。\n作成例）istio-ingressgateway/multicluster/backendconfig.yaml（差分）\n+ apiVersion: cloud.google.com/v1 + kind: BackendConfig + metadata: + name: ingress-backendconfig + spec: + healthCheck: + requestPath: /healthz/ready + port: 15021 + type: HTTP MultiClusterIngress は Ingress リソースをマルチクラスタに対応させたリソースという位置づけであり、基本的に Ingress リソースを定義するときと設定値はほぼ同じです。\n作成例）istio-ingressgateway/multicluster/multiclusteringress.yaml（差分）\n+ apiVersion: networking.gke.io/v1beta1 + kind: MultiClusterIngress + metadata: + name: istio-ingressgateway + labels: + app: istio-ingressgateway + istio: ingressgateway + spec: + template: + spec: + backend: + serviceName: istio-ingressgateway + servicePort: 80 (3) Ingress ゲートウェイのデプロイ # まずは Ingress ゲートウェイリソースをデプロイする Namespace を新たに作成します。今回の例では istio-gateway という名前の Namespace を作成しています。\n実行例）Ingress Gateway用のNamespace作成\n# 環境変数の設定 export GATEWAY_NAMESPACE=\u0026#34;istio-gateway\u0026#34; # 両クラスタにサンプルアプリケーション用 Namespace リソースの作成 for CTX in ${CTX_1} ${CTX_2} do kubectl create --context=${CTX} namespace ${GATEWAY_NAMESPACE} kubectl label --context=${CTX} namespace ${GATEWAY_NAMESPACE} \\  istio.io/rev=asm-managed-stable --overwrite done 次のコマンドを実行して Ingress ゲートウェイアプリケーションを両クラスタにデプロイしましょう。\n実行例）Ingress Gatewayアプリケーションのデプロイ\nfor CTX in ${CTX_1} ${CTX_2} do kubectl apply -n ${GATEWAY_NAMESPACE} --context=${CTX} \\  -f istio-ingressgateway done 最後にマルチクラスタ Ingress リソースを構成クラスタである GKE クラスタ #1 に対してデプロイをしましょう。\n実行例）Ingress Gatewayアプリケーションのデプロイ\nkubectl apply -n ${GATEWAY_NAMESPACE} --context=${CTX_1} \\  -f istio-ingressgateway/multicluster 以上で Ingress ゲートウェイのデプロイは終わりです。\n(4) Istio リソースのデプロイ # Ingress ゲートウェイを通じてメッシュの外から HelloWorld アプリケーションへ通信ができるように Istio リソースの定義を行っていきたいと思います。まずは Istio Gateway リソース5および Istio VirtualService リソース1の定義ファイルを作成しましょう。例のように Gateway リソースにメッシュ外から受け付けるポートとプロトコルを定義し、VirtualService リソースには Gateway に入ってきた通信のパターンマッチ条件と振り分け先バックエンドの指定をします。\n作成例）helloworld-gateway.yaml\napiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:helloworld-gatewayspec:selector:istio:ingressgatewayservers:- port:number:80name:httpprotocol:HTTPhosts:- \u0026#34;*\u0026#34;---apiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:helloworld-gatewayspec:hosts:- \u0026#34;*\u0026#34;gateways:- helloworld-gatewayhttp:- match:- uri:exact:/helloroute:- destination:host:helloworldport:number:5000次のコマンドで両クラスタに Istio リソースをデプロイし、アプリケーションへのインバウンド通信ができるように設定しましょう。\n実行例）Istioリソースのデプロイ\nfor CTX in ${CTX_1} ${CTX_2} do kubectl apply -n ${SAMPLE_NAMESPACE} --context=${CTX} \\  -f helloworld-gateway.yaml done 以上でメッシュ外からのアプリケーションへのインバウンド通信もできるようになりました。\n(5) インバウンド通信の動作確認 # それではメッシュ外からのアプリケーションへのインバウンド通信ができることを確認していきましょう。ローカリティロードバランシングの設定は「Step5. ローカリティロードバランシングの設定」にて実施済みですので次のような挙動となることが想定されます。\n   (a) パブリックエンドポイントの取得\nまずは Ingress ゲートウェイの外部 IP アドレスを取得しましょう。\n実行例）Ingressゲートウェイの外部IPアドレスの取得\nkubectl --context=${CTX_1} \\  -n ${GATEWAY_NAMESPACE} get MultiClusterIngress \\  -o custom-columns=VIP:status.VIP --no-headers (b) インバウンド通信の実行\nでは東京リージョン上に仮想マシンなどを立てて Ingress ゲートウェイの外部 IP アドレスに対して curl コマンドを実行し、アクセスをしてみましょう。実行例のように v1 に振り分けが 100% されることを確認できるかと思います。\n出力例）ローカリティロードバランシング有効時\n$ for x in `seq 1 10`; do curl http://\u0026lt;Ingress Gateway\u0026#39;s External IP\u0026gt;/hello -w \u0026#34;time_total:%{time_total}\\n\u0026#34;; done Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.139677 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.139758 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.138518 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.142124 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.137289 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.151358 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.133507 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.139144 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.133560 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.135621 次に大阪リージョン上に仮想マシンなどを立てて Ingress ゲートウェイの外部 IP アドレスに対して curl コマンドを実行し、アクセスをしてみましょう。先ほどとは異なり、実行例のように v2 に振り分けが 100% されることを確認できるかと思います。\n出力例）ローカリティロードバランシング有効時\n$ for x in `seq 1 10`; do curl http://\u0026lt;Ingress Gateway\u0026#39;s External IP\u0026gt;/hello -w \u0026#34;time_total:%{time_total}\\n\u0026#34;; done Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.139 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.133 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.134 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.130 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.129 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.141 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.133 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.176 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.137 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.138 ご参考、ローカリティロードバランシング無効時の動作\nローカリティロードバランシングを無効(=DestinationRule リソースを削除)にした状態で、各リージョンからアクセスした結果も取得してみました。メッシュ内の通信では差が見えにくかったのですが、インバウンド通信にするとローカリティによるレスポンスの改善がはっきりとわかる結果になりました。\n出力例）ローカリティロードバランシング無効時\n$ for x in `seq 1 10`; do curl http://\u0026lt;Ingress Gateway\u0026#39;s External IP\u0026gt;/hello -w \u0026#34;time_total:%{time_total}\\n\u0026#34;; done Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.221164 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.142753 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.224596 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.160720 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.188251 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.145336 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.173262 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.138860 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t time_total:0.139937 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 time_total:0.170250 出力例）ローカリティロードバランシング無効時\n$ for x in `seq 1 10`; do curl http://\u0026lt;Ingress Gateway\u0026#39;s External IP\u0026gt;/hello -w \u0026#34;time_total:%{time_total}\\n\u0026#34;; done Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 response_time:0.147 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t response_time:0.225 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t response_time:0.211 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 response_time:0.141 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 response_time:0.134 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t response_time:0.182 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 response_time:0.133 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 response_time:0.134 Hello version: v2, instance: helloworld-v2-54df5f84b-jqpm4 response_time:0.143 Hello version: v1, instance: helloworld-v1-776f57d5f6-6kw7t response_time:0.187 以上でメッシュの外からアプリケーションへのインバウンド通信に対する動作確認も終了です。お疲れ様でした。\n終わりに # 今回は複数リージョンに展開した複数 GKE クラスタを単一の Anthos Service Mesh 環境に追加し、GKE クラスタ間で負荷分散を行う方法についてご紹介でしたがいかがだったでしょうか。\n複数リージョンの GKE クラスタでマルチクラスタメッシュを構築することにより、予期しない大規模災害の発生にも耐えうる高い可用性と回復力の実現、エンドユーザからより近い位置への振り分けによるレイテンシの改善といったことが期待できるようになる見込みです。もしこれから Anthos Service Mesh 環境の利用を検討している方はマルチクラスタメッシュ構成についても検討してみてはいかがでしょうか。\n  Google Cloud は、Google LLC の商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。    DestinationRule は転送先サービスのサブセット化や各種トラフィックポリシーを定義する Istio リソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n MultiClusterService は Service リソースを複数のクラスタ上に展開する GKE 独自のカスタムリソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n BackendConfig はバックエンドサービスのヘルスチェックを定義する GKE 独自のカスタムリソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n MultiClusterIngress はマルチクラスタに対応した Ingress リソースを定義する GKE 独自のカスタムリソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Gateway は Ingress/Egress ゲートウェイで受け付けるポート、プロトコルを定義する Istio リソース\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":"December 9, 2021","permalink":"/posts/2021/12/gcp-multi-region-asm-cluster/","section":"記事一覧","summary":"みなさん、こんにちは。今回は複数のリージョンに展開する各 GKE クラスタを単一の Anthos Service Mesh 環境に追加し、GKE クラスタ間で負荷分散を行う方法についてご紹介していきたいと思います。","title":"複数リージョンの GKE クラスタと Anthos Service Mesh でマルチクラスタメッシュ環境を構築してみた"},{"content":"みなさん、こんにちは。今回は GitHub Enterprise Cloud(GHEC) の監査ログ(Audit Log) の取得方法についてのお話です。\nGHEC 監査ログの取得方法としてはいくつか方法はあるのですが、この記事では GHEC の監査ログを取得するためのコマンドラインインタフェースである GHEC Audit Log CLI を使った方法をご紹介していきたいと思います。\n 今回は Linux(AWS CloudShell) 上に環境を作って試しに実行してみるところからはじめて、定期的に監査ログ取得を行う自動化フローの構築まで紹介していきたいと思います。\nまずは手動で GHEC Audit Log CLI を実行してみよう # Step1. 前提パッケージのインストール # GHEC Audit Log CLI の前提パッケージとして Node.js が必要となります。GitHub からソースコードを入手する必要があるため、git コマンドと併せてインストールしましょう。\n実行例）前提パッケージの導入\n$ curl --silent --location https://rpm.nodesource.com/setup_16.x | sudo bash - $ sudo yum install -y nodejs git Step2. GHEC Audit Log CLI のインストール # GitHub からソースコードを取得し、npm コマンドを使って GHEC Audit Log CLI をインストールします。最後の ghce-audit-log-cli -v コマンドにてバージョン情報が出力されれば CLI のインストールは完了です。\n実行例）ソースコード取得とインストール\n$ git clone https://github.com/github/ghec-audit-log-cli.git $ cd ghec-audit-log-cli $ sudo npm link $ ghec-audit-log-cli -v 2.1.2 $ ghec-audit-log-cli --help Usage: ghec-audit-log-cli [options] Options: -v, --version Output the current version -t, --token \u0026lt;string\u0026gt; the token to access the API (mandatory) -o, --org \u0026lt;string\u0026gt; the organization we want to extract the audit log from -cfg, --config \u0026lt;string\u0026gt; location for the config yaml file. Default \u0026#34;.ghec-audit-log\u0026#34; (default: \u0026#34;./.ghec-audit-log\u0026#34;) -p, --pretty prints the json data in a readable format (default: false) -l, --limit \u0026lt;number\u0026gt; a maximum limit on the number of items retrieved -f, --file \u0026lt;string\u0026gt; the output file where the result should be printed -a, --api \u0026lt;string\u0026gt; the version of GitHub API to call (default: \u0026#34;v4\u0026#34;) -at, --api-type \u0026lt;string\u0026gt; Only if -a is v3. API type to bring, either all, web or git (default: \u0026#34;all\u0026#34;) -c, --cursor \u0026lt;string\u0026gt; if provided, this cursor will be used to query the newest entries from the cursor provided. If not present, the result will contain all the audit log from the org -s, --source \u0026lt;string\u0026gt; the source of the audit log. The source can be either a GitHub Enterprise or a GitHub Enterprise Organization. Accepts the following values: org | enterprise. Defaults to org (default: \u0026#34;org\u0026#34;) -h, --help display help for command Step3. GitHub アクセストークンの取得 # 次に GitHub のサイトへ移り、GitHub API へアクセスする際に利用するアクセストークンを作成します。Organization の Owner 権限を持つユーザで、Settings \u0026gt; Developer settings \u0026gt; Personal access tokens \u0026gt; Generate new tokens から作成しましょう。付与するスコープについては筆者の環境では次の 4 つの権限を付与することで動作を確認することができました。\n public_repo admin:org read:user admin:enterprise  Step4. GHEC Audit Log CLI の動作確認 # ここまで来たら実際にコマンドを実行して監査ログが取得できるか試してみましょう。次のコマンドを実行して監査ログが出力されれば成功です。\n実行例）監査ログの取得\n$ export ORG_NAME=\u0026#34;Your GitHub organization account name\u0026#34; $ export AUDIT_LOG_TOKEN=\u0026#34;Your GitHub access token\u0026#34; $ ghec-audit-log-cli --org ${ORG_NAME} --token ${AUDIT_LOG_TOKEN} --api \u0026#34;v3\u0026#34; --pretty なお、GitHub アクセストークンに付与した権限が足りない場合は次のようなメッセージを出力してコマンドがエラー終了します。エラーメッセージに不足している権限についての情報がありますので Step3 へ戻り、アクセストークンへ不足する権限を追加して再度 CLI を実行してください。\n出力例）コマンド失敗時\nGraphqlError: Your token has not been granted the required scopes to execute this query. The \u0026#39;organizationBillingEmail\u0026#39; field requires one of the following scopes: [\u0026#39;admin:org\u0026#39;], but your token has only been granted the: [\u0026#39;admin:enterprise\u0026#39;, \u0026#39;read:org\u0026#39;] scopes. Please modify your token\u0026#39;s scopes at: https://github.com/settings/tokens. 以上で無事にローカル環境で GHEC Audit Log CLI を実行することができました。\n監査ログを取得するフローを自動化しよう # 次は定期的に監査ログ取得を行う自動化フローの構築を行っていきたいと思います。今回はサンプルとして用意されているワークフロー定義を少し改造し、「1 時間ごと GHEC Audit Log CLI を使って監査ログを取得し、監査ログ格納用のリポジトリに追加する」といったフローを GitHub Actions を使って自動化していきたいと思います。\nStep1. 監査ログ格納用リポジトリの作成 # まずは対象 Organization に監査ログを取得、格納するためのリポジトリを作成しましょう。今回は ghec-audit-log-cli という名前のリポジトリを作成しています。\n   Step2. シークレットの登録 # 次にアクセストークンなどの機密性の高い情報をワークフローへ渡すために Organization 設定にてシークレットの登録を行います。シークレットは次の 3 種類を登録しましょう。\n   シークレット名 説明     ORG_NAME 監査ログを取得する対象の Organization アカウント名を指定   AUDIT_LOG_TOKEN GitHub Audit Log API へアクセスする際に利用するアクセストークンを指定   COMMITTER_EMAIL 監査ログなどをリポジトリに Push するアカウントのメールアドレスを指定       Step3. GHEC Audit Log CLI のソースコード登録 # GHEC Audit Log CLI のソースコードを取得し、新しく作成したリポジトリへソースコードを登録をしましょう。\n実行例）ソースコードの登録\n$ export ORG_NAME=\u0026#34;Your GitHub organization account name\u0026#34; $ git clone https://github.com/github/ghec-audit-log-cli.git $ cd ghec-audit-log-cli $ git remote add logging https://github.com/${ORG_NAME}/ghec-audit-log-cli.git $ git push -u logging main Step4. ワークフロー定義の作成 # 次にサンプルのワークフロー定義が workflows ディレクトリに用意されていますので、こちらをベースにワークフロー定義を作成していきたいと思います。なお、v3 と v4 の 2 種類のサンプルが用意されていますが記事の執筆時点では ​v4 に不具合があったため v3 を利用しています。\nまずはサンプル定義ファイルを GitHub Actions 所定のディレクトリ .github/workflows へ複製します。\n実行例）サンプルワークフローを複製\n$ cp workflows/forward-v3-workflow.yml .github/workflows/ghec-audit-log.yml 次に複製した .github/workflows/ghec-audit-log.yml ファイルを編集します。サンプルでは取得した監査ログを指定した URL へ POST するように定義されていますが、今回はこちらのリポジトリへコミットするように書き換えています。\n変更例）.github/workflows/ghec-audit-log.yml（差分）\n############################################ # Github Action Workflow to poll and aggregate logs # ############################################ name: POLL/POST Audit Log Data from v3 API ############################################## # Run once an hour and when pushed to main # ############################################## on: push: branches: main schedule: - cron: \u0026#39;59 * * * *\u0026#39; ################# # Build the job # ################# jobs: poll: runs-on: ubuntu-latest strategy: matrix: node-version: [12.x] steps: # Clone source code - name: Checkout source code uses: actions/checkout@v2 # Install congiure NodeJS - name: Use Node.js ${{ matrix.node-version }} uses: actions/setup-node@v1 with: node-version: ${{ matrix.node-version }} # Need to install NPM - name: NPM Install run: npm install # If this is the first time we poll, then do a fresh poll. If not, poll from latest cursor. - name: Poll from Cursor run: | + FILE_SUFFIIX=$(date +%Y%m%d-%H%M) + mkdir -p audit-logs  if [ -f \u0026#34;.last-v3-cursor-update\u0026#34; ]; then LAST_CURSOR=$(cat .last-v3-cursor-update) fi if [ -z \u0026#34;$LAST_CURSOR\u0026#34; ]; then echo \u0026#34;FIRST TIME RUNNING AUDIT LOG POLL\u0026#34; - npm start -- --token ${{secrets.AUDIT_LOG_TOKEN}} --org ${{secrets.ORG_NAME}} --api \u0026#39;v3\u0026#39; --api-type \u0026#39;all\u0026#39; --file \u0026#34;audit-log-output.json\u0026#34; + npm start -- --token ${{secrets.AUDIT_LOG_TOKEN}} \\ + --org ${{secrets.ORG_NAME}} --api \u0026#39;v3\u0026#39; --api-type \u0026#39;all\u0026#39; \\ + --file \u0026#34;audit-logs/audit-log-output-${FILE_SUFFIIX}.json\u0026#34; --pretty  else echo \u0026#34;RUNNING AUDIT LOG POLL FROM $LAST_CURSOR\u0026#34; - npm start -- --token ${{secrets.AUDIT_LOG_TOKEN}} --org ${{secrets.ORG_NAME}} --api \u0026#39;v3\u0026#39; --api-type \u0026#39;all\u0026#39; --cursor $LAST_CURSOR --file \u0026#34;audit-log-output.json\u0026#34; + npm start -- --token ${{secrets.AUDIT_LOG_TOKEN}} \\ + --org ${{secrets.ORG_NAME}} --api \u0026#39;v3\u0026#39; --api-type \u0026#39;all\u0026#39; \\ + --cursor $LAST_CURSOR \\ + --file \u0026#34;audit-logs/audit-log-output-${FILE_SUFFIIX}.json\u0026#34; --pretty  fi - curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d @audit-log-output.json ${{secrets.WEBHOOK_URL}}  # Commit the cursor back to source - name: Commit cursor uses: EndBug/add-and-commit@v5 with: author_name: Audit Log Integration author_email: ${{ secrets.COMMITTER_EMAIL }} message: \u0026#34;Updating cursor for audit log\u0026#34; add: \u0026#34;.last-v3-cursor-update --force\u0026#34; env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} + - name: Commit audit log + uses: EndBug/add-and-commit@v5 + with: + author_name: Audit Log Integration + author_email: ${{ secrets.COMMITTER_EMAIL }} + message: \u0026#34;Adding audit log\u0026#34; + add: \u0026#34;audit-logs/audit-log-output-*.json --force\u0026#34; + env: + GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} 最後に定義ファイルをコミットして、リモートリポジトリへ登録しましょう。\n実行例）リポジトリにワークフローの登録\n$ git add .github/workflows/ghec-audit-log.yml $ git commit -m \u0026#34;add ghec-audit-log workflow\u0026#34; $ git push -u logging main このリポジトリへの Push をトリガーに GitHub Actions が起動しますので、この後はワークフローが期待通り動作しているかを確認していきましょう。\nStep5. ワークフローの動作確認 # まずは GitHub Actions 画面へ遷移し、ワークフローの起動および処理が成功していることを確認します。出力例のように GitHub Actions のワークフロー実行履歴にて成功が記録されていれば OK です。\n   次に監査ログが正しく格納されているか確認しましょう。出力例のように監査ログの格納が確認できれば OK です。\n   以上でワークフローの設定も完了です。お疲れ様でした。以降はワークフローで定義したスケジュールに沿って 1 時間ごとに監査ログがエクスポートされるようになるかと思います。\n終わりに # 今回は GitHub Enterprise Cloud (GHEC) の監査ログを取得する方法でした。GHEC の監査ログを長期(90 日以上)保存したい方はエクスポートが必須となってきますので参考にしてみてはいかがでしょうか。\n  GitHub は、GitHub Inc. の商標または登録商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。  ","date":"December 7, 2021","permalink":"/posts/2021/12/ghec-audit-log-cli/","section":"記事一覧","summary":"みなさん、こんにちは。今回は GitHub Enterprise Cloud(GHEC) の監査ログ(Audit Log) の取得方法についてのお話です。","title":"GHEC Audit Log CLI を使って GitHub Enterprise Cloud の監査ログを取得してみた"},{"content":"みなさん、こんにちは。今回は Google Cloud が提供するマネージドサービスメッシュサービスの Anthos Service Mesh に関するお話です。\nAnthos Service Mesh はここ半年で「マネージドコントロールプレーン機能の一般公開」、「マネージドデータプレーン機能のプレビュー公開」と Google マネージドの範囲を徐々に広げてきましたが、2021 年 11 月 19 日の更新でプレビュー段階ではありますが「Google Kubernetes Engine(GKE) Autopilot 上でも Anthos Service Mesh を利用できる」ようになりました。\n今回はそんなプレビュー公開されたばかりの GKE Autopilot と Anthos Service Mesh(ASM) を使った Kubernetes 部分も含めてフルマネージドなサービスメッシュ環境を構築していきたいと思います。\n構築するシステムについて # 次の図に示すように限定公開クラスタおよび承認済みネットワーク機能を有効化した GKE Autopilot クラスタに対して Anthos Service Mesh を導入し、サービスメッシュ上でサンプルアプリケーションを動かしていきたいと思います。\n   それでは構築していきましょう # いつも通り公式ドキュメントを参考にしつつ、公式ドキュメントに書かれていない部分を補足しながら構築をしていきたいと思います。\n Step1. VPC ネットワークの作成 # まずは GKE ノードを配置する VPC ネットワークおよび東京リージョンにサブネットを作成します。今回の例では GKE ノードからプライベートネットワーク経由で Artifact Registry などの他のマネージドサービスへアクセスできるように限定公開の Google アクセスをオンにしています。\n   プライベートネットワークからインターネット上の Docker Hub などへ接続できるよう Cloud NAT リソースも作成しておきたいと思います。\n   Step2. GKE Autopilot クラスタの作成 # 次に GKE Autopilot クラスタを作成していきましょう。Google Cloud コンソールから操作する場合は GKE クラスタ画面の「+作成」ボタンをクリックし、GKE Autopilot クラスタの作成画面へと遷移します。\n   Autopilot クラスタの作成画面では、クラスタを展開するネットワークの選択とリリースチャンネルの選択をしていきましょう。今回はセキュリティの観点から限定公開クラスタに設定し、GKE Autopilot リリースチャンネルには Anthos Service Mesh が現時点で唯一サポート対象としている Rapid チャンネルを指定しています。作成ボタンを押してからクラスタの作成が完了するまで 5 分程度かかりました。\n   なお、Anthos Service Mesh がサポートする GKE Autopilot リリースチャンネルについては近い将来変更が生じる可能性が高いため、構築を行う際には最新のサポート状況を公式ドキュメントから確認するようにしましょう。\n Step3. Anthos Service Mesh のインストール # ここからは Cloud Shell から操作を実施していきたいと思います。まずは Kubernetes API へ接続できるように GKE コントロールプレーンの承認済みネットワークに Cloud Shell の IP アドレスを登録し、kubectl を実行できるようにクラスタ認証情報を取得します。\n実行例）クラスタ認証情報の取得\nexport PROJECT_ID=`gcloud config list --format \u0026#34;value(core.project)\u0026#34;` export CLUSTER_NAME=\u0026#34;matt-tokyo-autopilot-cluster-001\u0026#34; export CLUSTER_LOCATION=\u0026#34;asia-northeast1\u0026#34; # CloudShellの承認済みネットワーク登録 gcloud container clusters update ${CLUSTER_NAME} \\  --region ${CLUSTER_LOCATION} \\  --enable-master-authorized-networks \\  --master-authorized-networks \\  \u0026#34;`dig +short myip.opendns.com @resolver1.opendns.com`/32\u0026#34; # クラスタ認証情報の取得 gcloud container clusters get-credentials ${CLUSTER_NAME} \\  --region ${CLUSTER_LOCATION} 次に Anthos Service Mesh v1.11 から正式な管理ツールとなった asmcli をダウンロードします。\n実行例）asmcliツールのダウンロード\ncurl https://storage.googleapis.com/csm-artifacts/asm/asmcli_1.11 \u0026gt; asmcli # 実行権限の付与 chmod +x asmcli asmcli を使って GKE Autopilot クラスタに Rapid チャンネル(※1)の Anthos Service Mesh をインストールします。コマンドが完了するまでおおよそ 5 分程度かかりました。\n※1: 現時点では GKE Autopilot は Anthos Service Mesh の Rapid チャンネルでのみサポートされているため\n実行例）Anthos Service Meshのインストール\n./asmcli x install \\  --project_id ${PROJECT_ID} \\  --cluster_location ${CLUSTER_LOCATION} \\  --cluster_name ${CLUSTER_NAME} \\  --managed \\  --use_managed_cni \\  --channel \u0026#34;rapid\u0026#34; \\  --enable-all \\  --output_dir ${CLUSTER_NAME} インストールに成功した場合は次のようなメッセージが出力されます。\n出力例）\nasmcli: Successfully installed ASM. Step4. ファイアウォールルールの更新 (限定公開クラスタ時のみ) # 限定公開クラスタに Anthos Service Mesh をインストールした場合は、コントロールプレーンからのポート 15017 による通信を追加で許可する必要があります。まず次のコマンドを実行し、既存のファイアウォールルール名を取得します。\n実行例）ファイアウォールルールの確認\ngcloud compute firewall-rules list --filter=\u0026#34;name~${CLUSTER_NAME}-.*-master\u0026#34; 次のコマンドのファイアウォールルール名 ${FIREWALL_RULE_NAME} を前のコマンドで取得した名前に置き替え、コマンド実行してコントロールプレーンからのポート 15017 による通信を許可します。\n実行例）ファイアウォールルールの更新\ngcloud compute firewall-rules update ${FIREWALL_RULE_NAME} \\  --allow tcp:10250,tcp:443,tcp:15017 Step5. マネージドデータプレーンの有効化 (Namespace 毎に設定) # マネージドデータプレーンは Kubernetes Namespace リソースごとにアノテーションを設定して有効化を行います。次のコマンドの ${NAMESPACE_NAME} を設定対象の Namespace 名に置き替え、コマンド実行してマネージドデータプレーンを有効化します。\n実行例）マネージドデータプレーンの有効化\nkubectl annotate --overwrite namespace ${NAMESPACE_NAME} \\  mesh.cloud.google.com/proxy=\u0026#39;{\u0026#34;managed\u0026#34;:\u0026#34;true\u0026#34;}\u0026#39; Step6. Ingress Gateway のデプロイ # asmcli でインストールした場合は自動で Ingress Gateway はデプロイされないため、メッシュ外からのアクセスをさせるためには Ingress Gateway をデプロイする必要があります。まず次のコマンドで istio-gateway Namespace を新たに作成します。\nexport GATEWAY_NAMESPACE=\u0026ldquo;istio-gateway\u0026rdquo;\nkubectl apply -f - \u0026laquo;EOF kind: Namespace metadata: name: ${GATEWAY_NAMESPACE} annotations: # マネージドデータプレーン有効化の設定 mesh.cloud.google.com/proxy: \u0026lsquo;{\u0026ldquo;managed\u0026rdquo;:\u0026ldquo;true\u0026rdquo;}\u0026rsquo; labels: # 自動サイドカーインジェクション(Rapid)有効化の設定 istio.io/rev: asm-managed-rapid EOF\n 次のコマンドを実行して Ingress Gateway をデプロイします。なお、今回は Anthos Service Mesh をインストールした際に `--output_dir` で指定したディレクトリに Ingress Gateway の定義ファイルが配置されているのでこちらをそのまま利用しています。 **実行例）Ingress Gatewayのデプロイ** ```bash kubectl apply -n ${GATEWAY_NAMESPACE} \\ -f ${CLUSTER_NAME}/samples/gateways/istio-ingressgateway なお、今回利用した Ingress Gateway の詳細については次の URL をご参照ください。\n 以上でアプリケーションをデプロイする準備が整いました。\nStep7. サンプルアプリケーションのデプロイ # 最後にサンプルアプリケーションをデプロイし、環境に問題がないかを確認していきましょう。まず次のコマンドでサンプルアプリケーション用の Namespace を新たに作成します。\n実行例）Namespaceの作成\nexport SAMPLE_NAMESPACE=\u0026#34;sample\u0026#34; # サンプルアプリケーション用 Namespace リソースの作成 kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Namespace metadata: annotations: # マネージドデータプレーン有効化の設定 mesh.cloud.google.com/proxy: \u0026#39;{\u0026#34;managed\u0026#34;:\u0026#34;true\u0026#34;}\u0026#39; labels: # 自動サイドカーインジェクション(Rapid)有効化の設定 istio.io/rev: asm-managed-rapid EOF サンプルアプリケーションをデプロイします。今回は Anthos Service Mesh をインストールした際に --output_dir で指定したディレクトリへ格納されているサンプルアプリケーションの中から HelloWorld というサンプルアプリケーションを使用しています。\n実行例）HelloWorldアプリケーションのデプロイ\n# Kubernetes Service リソースのデプロイ kubectl apply -n ${SAMPLE_NAMESPACE} \\  -f ${CLUSTER_NAME}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\  -l service=helloworld # Kubernetes Deployment リソースのデプロイ kubectl apply -n ${SAMPLE_NAMESPACE} \\  -f ${CLUSTER_NAME}/istio-1.11.2-asm.17/samples/helloworld/helloworld.yaml \\  -l version=v1 # Istio Gateway/VirtualService リソースのデプロイ kubectl apply -n ${SAMPLE_NAMESPACE} \\  -f ${CLUSTER_NAME}/istio-1.11.2-asm.17/samples/helloworld/helloworld-gateway.yaml アプリケーションのデプロイが終わったので Ingress Gateway 経由でアプリケーションにアクセスできるかを確認していきましょう。まず次のコマンドを実行して Ingress Gateway の External IP アドレスを取得します。\n実行例）IngressGatewayの設定\nkubectl -n ${GATEWAY_NAMESPACE} get service istio-ingressgateway External IP アドレスが取得できたら curl コマンドなどで http://\u0026lt;EXTERNAL_IP\u0026gt;/hello にアクセスしてみましょう。次のようなメッセージが表示されていれば成功です。\n出力例）\nHello version: v1, instance: helloworld-v1-xxxxxxxxxx-xxxxx 以上で構築は終わりです。お疲れ様でした。\n終わりに # さて今回はプレビュー公開されたばかりの Google Kubernetes Engine(GKE) Autopilot と Anthos Service Mesh(ASM) を使ったフルマネージドなサービスメッシュ環境を構築する方法のご紹介でした。いかがだったでしょうか。\nこれで晴れて Kubernetes 部分も含めフルマネージドなサービスメッシュ環境が実現できるようになり、こちらの活用により以前よりも運用負荷を大きく軽減できる未来が近づいてきましたね。\nもちろん安定性、保守性といった観点からプロダクション用途へのプレビュー段階の機能の適用は、現時点ではオススメできませんが、もしこれから検証目的で試してみたいという方は参考にしてみてはいかがでしょうか。\n  Google Cloud は、Google LLC の商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。  ","date":"December 1, 2021","permalink":"/posts/2021/12/gcp-asm-with-gke-autopilot/","section":"記事一覧","summary":"みなさん、こんにちは。今回は Google Cloud が提供するマネージドサービスメッシュサービスの Anthos Service Mesh に関するお話です。","title":"GKE Autopilot と Anthos Service Mesh を使ってフルマネージドなサービスメッシュ環境を構築してみた"},{"content":"みなさん、こんにちは。今回は GitHub Enterprise Cloud(GHEC) の各種ログを SIEM1 マネージドサービスである Microsoft Sentinel (旧称 Azure Sentinel) に集約し、サイバー攻撃の兆候を検知できるようにする方法をご紹介していきたいと思います。\nMicrosoft Sentinel と GHEC との連携方法はいくつかあるのですが、この記事では Microsoft Sentinel コミュニティで開発している Azure Logic Apps(ロジックアプリ)、Azure Functions(関数アプリ)の 2 種類のカスタムデータコネクタの内、「Azure Logic Apps コネクタ」を使った方法をご紹介していきたいと思います。\nこれから GHEC の利用を検討している方や、GHEC は利用しているけれど SIEM システムの導入まではしていないという方は、セキュリティ強化策のひとつとして参考にしてみてはいかがでしょうか。\n構築するシステムについて # 今回は Azure Logic Apps(ロジックアプリ)を定期的に起動し、GHEC から監査ログなどを取得して Microsoft Sentinel ワークスペースへ格納、格納されたログに対して Microsoft Sentinel が自動的に相関分析をかけていく、といった流れで処理を行うシステムを構築していきたいと思います。\n   Azure Sentinel とのコネクタについては、今回は Microsoft Sentinel コミュニティで公開されている次のカスタムデータコネクタを利用していきましょう。\n 本カスタムコネクタをデプロイすると、次の 3 種類の Azure Logic Apps リソースが動作するようになります。\n   リソース種別 説明     Audit Playbook 監査ログを定期的に収集する自動ワークフロー (デフォルト 5 分間隔)   Repo Playbook 各リポジトリに対するフォーク、クローン、コミットなどの操作ログを定期的に収集する自動ワークフロー (デフォルト 1 時間間隔)   Vulnerability Alert Playbook 各リポジトリに対するセキュリティ脆弱性診断ログを定期的に収集する自動ワークフロー (デフォルト 1 日間隔)    また、本カスタムコネクタで取得した各種ログデータについては、Log Analytics ワークスペースの次のカスタムテーブルへ格納されるようになります。\n   テーブル名 説明     GitHub_CL 監査ログのデータを格納するテーブル   GitHubRepoLogs_CL 各リポジトリに対するフォーク、クローン、コミットなどの操作ログやリポジトリに対するセキュリティ脆弱性診断ログのデータを格納するテーブル    それでは構築していきましょう # 今回は GitHub Enterprise Cloud → Microsoft Sentinel ワークスペース → カスタムコネクタ → Microsoft Sentinel の順で設定していきましょう。\nStep1. GitHub Enterprise Cloud を設定しよう # ここではログを収集する先の GitHub Organization の作成と、Azure Logic Apps から GitHub API へアクセスする際に利用するアクセストークンの作成を実施していきましょう。\n(1) Organization(Enterprise)の作成 # 監査ログなどを取得する対象の Organization を作成しましょう。今回は GitHub Enterprise Cloud とするため課金プランは「Enterprise」を選択します。\n   (2) アクセストークンの作成 # 次に Azure Logic Apps から GitHub API へアクセスする際に利用するアクセストークンを作成します。Organization の Owner 権限を持つユーザで、Settings \u0026gt; Developer settings \u0026gt; Personal access tokens \u0026gt; Generate new tokens から作成しましょう。\n付与するスコープについては Microsoft Sentinel コミュニティでは言及されておりませんが、筆者の環境では必要最低限のスコープとして次の 5 つの権限を付与することで動作を確認することができました。\n public_repo admin:org read:user user:email admin:enterprise  作成後はデータコネクタをデプロイする際に利用するためトークン情報をコピーしておきます。\n      トークン情報が漏れると大変なことになりますので絶対に漏らさないように注意しましょう。  Step2. Microsoft Sentinel ワークスペースを設定しよう # 次に Microsoft Sentinel の設定です。ここでは Microsoft Sentinel 用の Log Analytics ワークスペースの作成および Microsoft Sentinel リソースの作成を実施していきましょう。\n(1) ワークスペースの作成 # Azure ポータルなどから Microsoft Sentinel および Microsoft Sentinel 用の Log Analytics ワークスペースを作成しましょう。\n   (2) ワークスペース情報の取得 # Microsoft Sentinel リソースの作成後は、データコネクタをデプロイする際に利用するため Log Analytics ワークスペース画面のエージェント管理などから「ワークスペース ID」と「主キー」をコピーしておきます。\n   Step3. カスタムコネクタを設定しよう # ここでは GitHub 用カスタムデータコネクタを Azure Resource Manager(ARM) テンプレートからデプロイして、対象 GitHub Organization からログデータを取得できるようにデータコネクタの諸設定まで実施していきましょう。\n(1) ARM テンプレートのデプロイ # それでは GitHub 用カスタムデータコネクタをデプロイしていきましょう。データコネクタの「Readme」に記載されている Deploy to Azureボタンをクリックします。\n   Azure のカスタムテンプレートのデプロイ画面へ遷移したらパラメータ値を入力して作成をしましょう。\n   パラメータ 説明     Personal Access Token GitHub のアクセストークンを指定する   User Name GitHub アクセストークンなどを格納する Azure Key Vault キーコンテナへのアクセスを許可する Azure AD ユーザ名を指定する   Principal Id 上記 Azure AD ユーザのオブジェクト ID を指定する   Workspace Id Microsoft Sentinel ワークスペースのワークスペース ID を指定する   Workspace Key Microsoft Sentinel ワークスペースの主キーを指定する       デプロイにかかる時間は環境により誤差はあると思いますがおおよそ 2 分でした。デプロイが終わると Azure Logic Apps 以外にもストレージアカウントや Azure Key Vault などのリソースも作成されていることがわかります。\n   (2) Azure Key Vault API 接続の設定 # Azure Logic Apps から Azure Key Vault キーコンテナに格納された GitHub アクセストークンへのアクセスを可能とするため、keyvault-GitHubPlaybooks リソースの設定画面から API 接続への承認処理を行います。\n   (3) 設定ファイルの作成 # データコネクタの 2 種類の設定ファイル ORGS.json と lastrun-Audit.json を作成します。各ファイルの概要およびフォーマットは次の通りです。\n   ファイル名 説明     ORGS.json ログ取得対象の GitHub Organization を定義するファイル   lastrun-Audit.json 最終実行時刻を管理するファイル(新しいレコードのみを収集するために利用される)    作成例）ORGS.json\n[ {​​​​​​​ \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your 1st organization account name\u0026gt;\u0026#34; }​​​​​​​, {​​​​​​​​​​​​​​​​​​​​​ \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your 2nd organization account name\u0026gt;\u0026#34; }​​​​​​​​​​​​​​​​​​​​​, {​​​​​​​​​​​​​​​​​​​​​ \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your Nth organization account name\u0026gt;\u0026#34; }​​​​​​​​​​​​​​​​​​​​​ ] 作成例）lastrun-Audit.json\n{​​​​​​​​​​​​​​​​​​​​​ \u0026#34;lastContext\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;lastRun\u0026#34;: \u0026#34;\u0026#34; }​​​​​​​​​​​​​​​​​​​​​ ORGS.json ファイルを編集し、org パラメータ値をログ取得対象の GitHub Organization アカウント名に置きかえます。フォーマットのように、複数の GitHub Organization を指定することもできますので、実際の GitHub Organization の数にあわせて増減させてください。\nlastrun-Audit.json ファイルの各パラメータ値については、カスタムコネクタが動作すると自動的に値が更新されるためユーザ側で意識する必要はありません。\n(4) 設定ファイルの配置 # 2 種類のファイルを作成したら、ARM テンプレートから作成されたストレージアカウントの githublogicapp コンテナにアップロードしましょう。\n   (5) Azure Blob Storage API 接続の設定 # Azure Logic Apps からストレージアカウントに設定ファイルへのアクセスを可能とするため、ストレージアカウントのアクセスキー情報を取得し、azureblob-GitHubPlaybooks リソースの設定画面から API 接続を行います。\n   (6) Azure Logic Apps の有効化 # 最後に ARM テンプレートで作成された 3 つの Azure Logic Apps を有効化していきましょう。\n   ここまででひととおりの設定は終わりましたので、あとはスケジュールに沿ってログの取得が行われるようになっていることを確認していきましょう。\n(7) 動作確認 # まずは Azure Logic Apps から見ていきましょう。Azure Logic Apps では各アプリケーションが正しくスケジュール実行されていること、実行されたアプリケーションが正常終了していることを確認します。出力例のように実行の履歴にて成功と記録されていることが確認できれば OK です。\n   次に Log Analytics ワークスペースを確認していきましょう。ここではカスタムログに GitHub Enterprise Cloud から取得したログが格納されていることを確認します。出力例のように「GitHub_CL」と「GitHubRepoLogs_CL」の 2 種類のカスタムテーブルが作成されていれば OK です。\n   最後に Microsoft Sentinel を確認してみましょう。出力例のように「GitHub_CL」および「GitHubRepoLogs_CL」からのイベントが記録されていれば OK です。\n   以上でカスタムコネクタのデプロイは終わりです。お疲れ様でした。\nStep4. Microsoft Sentinel を設定しよう # さてログデータが収集できるようになりましたので、ここからは収集したログデータを自動で分析して脅威を検出できるように Microsoft Sentinel の設定をしていきましょう。今回は Microsoft Sentinel が標準で用意している GitHub 固有の分析ルールを有効化し、次のイベントを自動で検出できるようにしていきたいと思います。\n GitHub アカウントへのブルートフォース攻撃 (※1) 複数の異なるロケーションからのサインインバースト (※1) 新しい国からのアクティビティ TI インジケータに登録した IP アドレスからのアクセス (※2) 二要素認証の無効化 リポジトリ内にセキュリティ脆弱性  ※1: Azure Active Directory(Azure AD) とのシングルサインオン(SSO)設定、Azure AD ログの収集が必要\n※2: Microsoft Sentinel の脅威インテリジェンス設定にて TI インジケータの事前登録が必要\n(1) データの解析および正規化 # まずは GitHub_CL テーブルおよび GitHubRepoLogs_CL テーブルに格納されているデータを、Microsoft Sentinel で分析しやすいように加工をしていきましょう。なお、Microsoft Sentinel コミュニティにてデータ加工用のパーサ関数(GitHubAudit 関数、GitHubRepo 関数)が公開されておりますので、今回はこちらを利用していきましょう。\n 作成例）GitHubAudit関数\nGitHub_CL | project TimeGenerated=node_createdAt_t, Organization=columnifexists(\u0026#39;node_organizationName_s\u0026#39;, \u0026#34;\u0026#34;), Action=node_action_s, OperationType=node_operationType_s, Repository=columnifexists(\u0026#39;node_repositoryName_s\u0026#39;,\u0026#34;\u0026#34;), Actor=columnifexists(\u0026#39;node_actorLogin_s\u0026#39;, \u0026#34;\u0026#34;), IPaddress=columnifexists(\u0026#39;node_actorIp_s\u0026#39;, \u0026#34;\u0026#34;), City=columnifexists(\u0026#39;node_actorLocation_city_s\u0026#39;, \u0026#34;\u0026#34;), Country=columnifexists(\u0026#39;node_actorLocation_country_s\u0026#39;, \u0026#34;\u0026#34;), ImpactedUser=columnifexists(\u0026#39;node_userLogin_s\u0026#39;, \u0026#34;\u0026#34;), ImpactedUserEmail=columnifexists(\u0026#39;node_user_email_s\u0026#39;, \u0026#34;\u0026#34;), InvitedUserPermission=columnifexists(\u0026#39;node_permission_s\u0026#39;, \u0026#34;\u0026#34;), Visibility=columnifexists(\u0026#39;node_visibility_s\u0026#39;, \u0026#34;\u0026#34;), OauthApplication=columnifexists(\u0026#39;node_oauthApplicationName_s\u0026#39;, \u0026#34;\u0026#34;), OauthApplicationUrl=columnifexists(\u0026#39;node_applicationUrl_s\u0026#39;, \u0026#34;\u0026#34;), OauthApplicationState=columnifexists(\u0026#39;node_state_s\u0026#39;, \u0026#34;\u0026#34;), UserCanInviteCollaborators=columnifexists(\u0026#39;node_canInviteOutsideCollaboratorsToRepositories_b\u0026#39;, \u0026#34;\u0026#34;), MembershipType=columnifexists(\u0026#39;node_membershipTypes_s\u0026#39;, \u0026#34;\u0026#34;), CurrentPermission=columnifexists(\u0026#39;node_permission_s\u0026#39;, \u0026#34;\u0026#34;), PreviousPermission=columnifexists(\u0026#39;node_permissionWas_s\u0026#39;, \u0026#34;\u0026#34;), TeamName=columnifexists(\u0026#39;node_teamName_s\u0026#39;, \u0026#34;\u0026#34;), Reason=columnifexists(\u0026#39;node_reason_s\u0026#39;, \u0026#34;\u0026#34;), BlockedUser=columnifexists(\u0026#39;node_blockedUserName_s\u0026#39;, \u0026#34;\u0026#34;), CanCreateRepositories=columnifexists(\u0026#39;canCreateRepositories_b\u0026#39;, \u0026#34;\u0026#34;) 作成例）GitHubRepo関数\nGitHubRepoLogs_CL | project TimeGenerated = columnifexists(\u0026#39;DateTime_t\u0026#39;, \u0026#34;\u0026#34;), Organization=columnifexists(\u0026#39;Organization_s\u0026#39;, \u0026#34;\u0026#34;), Repository=columnifexists(\u0026#39;Repository_s\u0026#39;,\u0026#34;\u0026#34;), Action=columnifexists(\u0026#39;LogType_s\u0026#39;,\u0026#34;\u0026#34;), Actor=coalesce(login_s, owner_login_s), ActorType=coalesce(owner_type_s, type_s), IsPrivate=columnifexists(\u0026#39;private_b\u0026#39;,\u0026#34;\u0026#34;), ForksUrl=columnifexists(\u0026#39;forks_url_s\u0026#39;,\u0026#34;\u0026#34;), PushedAt=columnifexists(\u0026#39;pushed_at_t\u0026#39;,\u0026#34;\u0026#34;), IsDisabled=columnifexists(\u0026#39;disabled_b\u0026#39;,\u0026#34;\u0026#34;), AdminPermissions=columnifexists(\u0026#39;permissions_admin_b\u0026#39;,\u0026#34;\u0026#34;), PushPermissions=columnifexists(\u0026#39;permissions_push_b\u0026#39;,\u0026#34;\u0026#34;), PullPermissions=columnifexists(\u0026#39;permissions_pull_b\u0026#39;,\u0026#34;\u0026#34;), ForkCount=columnifexists(\u0026#39;forks_count_d\u0026#39;,\u0026#34;\u0026#34;), Count=columnifexists(\u0026#39;count_d,\u0026#39;,\u0026#34;\u0026#34;), UniqueUsersCount=columnifexists(\u0026#39;uniques_d\u0026#39;,\u0026#34;\u0026#34;), DismmisedAt=columnifexists(\u0026#39;dismissedAt_t\u0026#39;,\u0026#34;\u0026#34;), Reason=columnifexists(\u0026#39;dismissReason_s\u0026#39;,\u0026#34;\u0026#34;), vulnerableManifestFilename = columnifexists(\u0026#39;vulnerableManifestFilename_s\u0026#39;,\u0026#34;\u0026#34;), Description=columnifexists(\u0026#39;securityAdvisory_description_s\u0026#39;,\u0026#34;\u0026#34;), Link=columnifexists(\u0026#39;securityAdvisory_permalink_s\u0026#39;,\u0026#34;\u0026#34;), PublishedAt=columnifexists(\u0026#39;securityAdvisory_publishedAt_t \u0026#39;,\u0026#34;\u0026#34;), Severity=columnifexists(\u0026#39;securityAdvisory_severity_s\u0026#39;,\u0026#34;\u0026#34;), Summary=columnifexists(\u0026#39;securityAdvisory_summary_s\u0026#39;,\u0026#34;\u0026#34;) 次の画像を参考に Microsoft Sentinel のログ画面から 2 つのパーサ関数を登録しましょう。なお、「従来のカテゴリ」の値については任意の文字列で大丈夫です。\n   (2) 分析ルールの追加 # 次に GitHub 固有の脅威を検出できるように分析ルールをテンプレートから追加していきしょう。Microsoft Sentinel では GitHub 固有の脅威に対する分析ルールのテンプレートが用意されていますので、今回はこちらを活用して分析ルールの追加をしてきたいと思います。出力例のように検索キーワードに「github」と入力することで目的のテンプレートを見つけることができます。\n   なお、2021 年 11 月時点で用意されているテンプレートは次の 6 種類となります。\n   No. テンプレート名 説明     1 GitHub Two Factor Auth Disable 二要素認証の無効化イベントを検知するルール、デフォルトでは 1 日ごとにクエリを実行   2 GitHub Activites from a New Country 新しい国からのアクティビティを検知するルール、デフォルトでは過去 7 日分を学習データに利用して 1 日ごとにクエリを実行   3 Brute Force Attack against GitHub Account GitHub アカウントへのブルートフォース攻撃を検知するルール、デフォルトでは 1 日ごとにクエリを実行   4 GitHub Signin Burst from Multiple Locations 複数の異なるロケーションからのサインインバーストを検知するルール、デフォルトでは 1 時間ごとにクエリを実行   5 TI map IP entity to GitHub_CL TI インジケータに登録した IP アドレスからのアクセスを検知するルール、デフォルトでは 1 時間ごとにクエリを実行   6 GitHub Security Vulnerability in Repository リポジトリ内にセキュリティ脆弱性が含まれていることを検知するルール、デフォルトでは 1 時間ごとにクエリを実行    それではテンプレートを用いた分析ルールの追加をしていきましょう。まず追加したいルールを選択し、「ルールの作成」ボタンをクリックします。\n   ルールの追加画面ではウィザードにしたがってルールの作成を行っていきます。各パラメータにはテンプレートによってデフォルト値が入力されていますのでとくに値を変更する要件がなければそのまま作成していきましょう。\n   分析ルールが追加されたテンプレートについては次の出力例のように「使用中」アイコンが付与されます。先ほどと同様の手順を繰り返し、他のテンプレートに対しても分析ルールの追加をしていきましょう。出力例のように 6 種類すべてに使用中アイコンが付与されれば脅威を自動検知するルールの設定も終わりです。お疲れ様でした。\n   補足、もう一歩踏み込んだ脅威分析を行うには # Microsoft Sentinel では今回紹介した標準の分析ルールを利用する以外に、カスタム分析ルールを自作してより高度なイベントの検知をすることが可能です。サンプルとして次のようなカスタム分析ルールが Microsoft 技術ブログでも紹介されていますのでぜひ参考にしてみてください。\n リポジトリに対する異常数のクローン操作 リポジトリの一括削除 リポジトリをプライベートからパブリックに変更 リポジトリに対する部外者からのフォーク操作 GitHub Organization へのユーザ招待およびユーザ追加 ユーザへのアクセス許可の追加付与　など   終わりに # 今回は GitHub Enterprise Cloud の各種ログを SIEM マネージドサービスである Microsoft Sentinel (旧称 Azure Sentinel) に集約し、サイバー攻撃の兆候を検知できるようにする方法のご紹介でしたがいかがだったでしょうか？\nGitHub Enterprise Cloud に限らずさまざまな監査ログを収集し、長期保存されている方は多いと思います。ただ、せっかく取得するのであればただ集めて長期保存しておくだけではなく、Microsoft Sentinel を活用してもう一歩進んだセキュアな環境を実現してみる、というのも \u0026ldquo;有り\u0026rdquo; なのではないでしょうか。\n以上、「Microsoft Sentinel を使って GitHub Enterprise Cloud のセキュリティを強化しよう (Azure Logic Apps 編)」でした。\n  Microsoft Azure は，Microsoft Corporation の商標または登録商標です。 GitHub は、GitHub Inc. の商標または登録商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。    SIEM(Security Information and Event Management) は、さまざまなログを一元的に集約、相関分析をしてサイバー攻撃などの異常を自動的に検出するソリューションです。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":"November 26, 2021","permalink":"/posts/2021/11/azure-sentinel-logicapps-data-connector-for-ghec/","section":"記事一覧","summary":"みなさん、こんにちは。今回は GitHub Enterprise Cloud(GHEC) の各種ログを SIEM1 マネージドサービスである Microsoft Sentinel (旧称 Azure Sentinel) に集約し、サイバー攻撃の兆候を検知できるようにする方法をご紹介していきたいと思います。","title":"Microsoft Sentinel を使って GitHub Enterprise Cloud のセキュリティを強化しよう (Azure Logic Apps コネクタ編)"},{"content":"みなさん、こんにちは。今回は GitHub Enterprise Cloud(GHEC) の各種ログを SIEM1 マネージドサービスである Microsoft Sentinel (旧称 Azure Sentinel) に集約し、サイバー攻撃の兆候を検知できるようにする方法をご紹介していきたいと思います。\nMicrosoft Sentinel と GHEC との連携方法はいくつかあるのですが、この記事では Microsoft Sentinel コミュニティで開発している Azure Logic Apps(ロジックアプリ)、Azure Functions(関数アプリ)の 2 種類のカスタムデータコネクタの内、「Azure Functions コネクタ」を使った方法をご紹介していきたいと思います。\nこれから GHEC の利用を検討している方や、GHEC は利用しているけれど SIEM システムの導入まではしていないという方は、セキュリティ強化策のひとつとして参考にしてみてはいかがでしょうか。\n構築するシステムについて # 今回は Azure Functions(関数アプリ)を定期的に起動し、GHEC から監査ログなどを取得して Microsoft Sentinel ワークスペースへ格納、格納されたログに対して Microsoft Sentinel が自動的に相関分析をかけていく、といった流れで処理を行うシステムを構築していきたいと思います。\n   Azure Sentinel とのコネクタについては、今回は Microsoft Sentinel コミュニティで公開されている次のカスタムデータコネクタを利用していきたいと思います。\n 本カスタムコネクタで取得した各種ログデータについては、Log Analytics ワークスペースの次のカスタムテーブルへ格納されるようになります。\n   テーブル名 説明     GitHub_CL 監査ログのデータを格納するテーブル   GitHubRepoLogs_CL 各リポジトリに対するフォーク、クローン、コミットなどの操作ログやリポジトリに対するセキュリティ脆弱性診断ログのデータを格納するテーブル    それでは構築していきましょう # 今回は GitHub Enterprise Cloud → Microsoft Sentinel ワークスペース → カスタムコネクタ → Microsoft Sentinel の順で設定していきましょう。\nStep1. GitHub Enterprise Cloud を設定しよう # ここではログを収集する先の GitHub Organization の作成と、Azure Functions から GitHub API へアクセスする際に利用するアクセストークンの作成を実施していきましょう。\n(1) Organization(Enterprise) の作成 # 監査ログなどを取得する対象の Organization を作成しましょう。今回は GitHub Enterprise Cloud とするため課金プランは「Enterprise」を選択します。\n   (2) アクセストークンの作成 # 次に Azure Logic Apps から GitHub API へアクセスする際に利用するアクセストークンを作成します。Organization の Owner 権限を持つユーザで、Settings \u0026gt; Developer settings \u0026gt; Personal access tokens \u0026gt; Generate new tokens から作成しましょう。\n付与するスコープについては Microsoft Sentinel コミュニティでは言及されておりませんが、筆者の環境では必要最低限のスコープとして次の 5 つの権限を付与することで動作を確認することができました。\n public_repo admin:org read:user user:email admin:enterprise  作成後はデータコネクタをデプロイする際に利用するためトークン情報をコピーしておきます。      トークン情報が漏れると大変なことになりますので絶対に漏らさないように注意しましょう。  Step2. Microsoft Sentinel ワークスペースを設定しよう # 次に Microsoft Sentinel ワークスペースの設定です。ここでは Log Analytics ワークスペースの作成および Log Analytics ワークスペースへの Microsoft Sentinel 機能の追加を実施していきましょう。\n(1) ワークスペースの作成 # Azure ポータルなどから Log Analytics ワークスペースの作成および作成したワークスペースへの Microsoft Sentinel 機能の追加を次の例のように実施していきましょう。\n   (2) ワークスペース情報の取得 # Microsoft Sentinel ワークスペースの作成後は、データコネクタをデプロイする際に利用するため Log Analytics ワークスペース画面のエージェント管理などから「ワークスペース ID」と「主キー」をコピーしておきます。\n   Step3. カスタムコネクタを設定しよう # ここでは GitHub 用カスタムデータコネクタを ARM テンプレートからデプロイして、対象 GitHub Organization からログデータを取得できるようにデータコネクタの諸設定まで実施していきましょう。\n(1) ARM テンプレートのデプロイ # それでは GitHub 用カスタムデータコネクタをデプロイしていきましょう。データコネクタの「Readme」に記載されている Deploy to Azureボタンをクリックします。\n   Azure のカスタムテンプレートのデプロイ画面へ遷移したらパラメータ値を入力して作成をしましょう。\n   パラメータ 説明     Personal Access Token GitHub のアクセストークンを指定する   Workspace Id Microsoft Sentinel ワークスペースのワークスペース ID を指定する   Workspace Key Microsoft Sentinel ワークスペースの主キーを指定する   Function Schedule Azure Functions の起動スケジュールを「\u0026ldquo;秒\u0026rdquo; \u0026ldquo;分\u0026rdquo; \u0026ldquo;時\u0026rdquo; \u0026ldquo;日付\u0026rdquo; \u0026ldquo;月\u0026rdquo; \u0026ldquo;曜日\u0026rdquo;」で指定する(デフォルト 10 分間隔)       デプロイにかかる時間は環境により誤差はあると思いますがおおよそ 3 分でした。デプロイが終わると Azure Functions 以外にもストレージアカウントや Azure Key Vault などのリソースも作成されていることがわかります。\n   (2) 設定ファイルの作成 # データコネクタの 2 種類の設定ファイル ORGS.json と lastrun-Audit.json を作成します。各ファイルの概要およびフォーマットは次の通りです。\n   ファイル名 説明     ORGS.json ログ取得対象の GitHub Organization を定義するファイル   lastrun-Audit.json 最終実行時刻を管理するファイル(新しいレコードのみを収集するために利用される)    作成例）ORGS.json\n[ { \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your 1st organization account name\u0026gt;\u0026#34; }, { \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your 2nd organization account name\u0026gt;\u0026#34; }, { \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your Nth organization account name\u0026gt;\u0026#34; } ] 作成例）lastrun-Audit.json\n[ { \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your 1st organization account name\u0026gt;\u0026#34; \u0026#34;lastContext\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;lastRun\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your 2nd organization account name\u0026gt;\u0026#34; \u0026#34;lastContext\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;lastRun\u0026#34;: \u0026#34;\u0026#34; }, { \u0026#34;org\u0026#34;: \u0026#34;\u0026lt;Your Nth organization account name\u0026gt;\u0026#34; \u0026#34;lastContext\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;lastRun\u0026#34;: \u0026#34;\u0026#34; } ] 各ファイルを編集し、org パラメータ値をログ取得対象の GitHub Organization アカウント名に置きかえます。フォーマットのように、複数の GitHub Organization を指定することもできますので、実際の GitHub Organization の数にあわせて増減させてください。\nなお、lastrun-Audit.json ファイルには org 以外のパラメータもありますが、これらはカスタムコネクタが動作すると自動的に値が更新されるため、ユーザ側で意識する必要はありません。\n(3) 設定ファイルの配置 # 2 種類のファイルを作成したら、ARM テンプレートから作成されたストレージアカウントの github-repo-logs コンテナにアップロードしましょう。\n   ここまででひととおりの設定は終わりましたので、あとはスケジュールに沿ってログの取得が行われるようになっていることを確認していきましょう。\n(4) 動作確認 # まずは Azure Functions から見ていきましょう。Azure Functions では、関数が正しくスケジュール実行されていること、実行された関数が正常終了していることを確認します。出力例のように実行回数と成功回数がカウントされていれば OK です。\n   次に Log Analytics ワークスペースを確認していきましょう。ここではカスタムログに GitHub Enterprise Cloud から取得したログが格納されていることを確認します。出力例のように「GitHub_CL」と「GitHubRepoLogs_CL」の 2 種類のカスタムテーブルが作成されていれば OK です。\n   最後に Microsoft Sentinel を確認してみましょう。出力例のように「GitHub_CL」および「GitHubRepoLogs_CL」からのイベントが記録されていれば OK です。\n   以上でカスタムコネクタのデプロイは終わりです。お疲れ様でした。\n 複数の異なるロケーションからのサインインバースト (※1) 新しい国からのアクティビティ TI インジケータに登録した IP アドレスからのアクセス (※2) 二要素認証の無効化 リポジトリ内にセキュリティ脆弱性  ※1: Azure Active Directory(Azure AD) とのシングルサインオン(SSO)設定、Azure AD ログの収集が必要\n※2: Microsoft Sentinel の脅威インテリジェンス設定にて TI インジケータの事前登録が必要\n(1) データの解析および正規化 # まずは GitHub_CL テーブルおよび GitHubRepoLogs_CL テーブルに格納されているデータを、Microsoft Sentinel で分析しやすいように加工をしていきましょう。なお、Microsoft Sentinel コミュニティにてデータ加工用のパーサ関数(GitHubAudit 関数、GitHubRepo 関数)が公開されておりますので、今回はこちらを利用していきましょう。\n 作成例）GitHubAudit関数\nGitHub_CL | project TimeGenerated=node_createdAt_t, Organization=columnifexists(\u0026#39;node_organizationName_s\u0026#39;, \u0026#34;\u0026#34;), Action=node_action_s, OperationType=node_operationType_s, Repository=columnifexists(\u0026#39;node_repositoryName_s\u0026#39;,\u0026#34;\u0026#34;), Actor=columnifexists(\u0026#39;node_actorLogin_s\u0026#39;, \u0026#34;\u0026#34;), IPaddress=columnifexists(\u0026#39;node_actorIp_s\u0026#39;, \u0026#34;\u0026#34;), City=columnifexists(\u0026#39;node_actorLocation_city_s\u0026#39;, \u0026#34;\u0026#34;), Country=columnifexists(\u0026#39;node_actorLocation_country_s\u0026#39;, \u0026#34;\u0026#34;), ImpactedUser=columnifexists(\u0026#39;node_userLogin_s\u0026#39;, \u0026#34;\u0026#34;), ImpactedUserEmail=columnifexists(\u0026#39;node_user_email_s\u0026#39;, \u0026#34;\u0026#34;), InvitedUserPermission=columnifexists(\u0026#39;node_permission_s\u0026#39;, \u0026#34;\u0026#34;), Visibility=columnifexists(\u0026#39;node_visibility_s\u0026#39;, \u0026#34;\u0026#34;), OauthApplication=columnifexists(\u0026#39;node_oauthApplicationName_s\u0026#39;, \u0026#34;\u0026#34;), OauthApplicationUrl=columnifexists(\u0026#39;node_applicationUrl_s\u0026#39;, \u0026#34;\u0026#34;), OauthApplicationState=columnifexists(\u0026#39;node_state_s\u0026#39;, \u0026#34;\u0026#34;), UserCanInviteCollaborators=columnifexists(\u0026#39;node_canInviteOutsideCollaboratorsToRepositories_b\u0026#39;, \u0026#34;\u0026#34;), MembershipType=columnifexists(\u0026#39;node_membershipTypes_s\u0026#39;, \u0026#34;\u0026#34;), CurrentPermission=columnifexists(\u0026#39;node_permission_s\u0026#39;, \u0026#34;\u0026#34;), PreviousPermission=columnifexists(\u0026#39;node_permissionWas_s\u0026#39;, \u0026#34;\u0026#34;), TeamName=columnifexists(\u0026#39;node_teamName_s\u0026#39;, \u0026#34;\u0026#34;), Reason=columnifexists(\u0026#39;node_reason_s\u0026#39;, \u0026#34;\u0026#34;), BlockedUser=columnifexists(\u0026#39;node_blockedUserName_s\u0026#39;, \u0026#34;\u0026#34;), CanCreateRepositories=columnifexists(\u0026#39;canCreateRepositories_b\u0026#39;, \u0026#34;\u0026#34;) 作成例）GitHubRepo関数\nGitHubRepoLogs_CL | project TimeGenerated = columnifexists(\u0026#39;DateTime_t\u0026#39;, \u0026#34;\u0026#34;), Organization=columnifexists(\u0026#39;Organization_s\u0026#39;, \u0026#34;\u0026#34;), Repository=columnifexists(\u0026#39;Repository_s\u0026#39;,\u0026#34;\u0026#34;), Action=columnifexists(\u0026#39;LogType_s\u0026#39;,\u0026#34;\u0026#34;), Actor=coalesce(login_s, owner_login_s), ActorType=coalesce(owner_type_s, type_s), IsPrivate=columnifexists(\u0026#39;private_b\u0026#39;,\u0026#34;\u0026#34;), ForksUrl=columnifexists(\u0026#39;forks_url_s\u0026#39;,\u0026#34;\u0026#34;), PushedAt=columnifexists(\u0026#39;pushed_at_t\u0026#39;,\u0026#34;\u0026#34;), IsDisabled=columnifexists(\u0026#39;disabled_b\u0026#39;,\u0026#34;\u0026#34;), AdminPermissions=columnifexists(\u0026#39;permissions_admin_b\u0026#39;,\u0026#34;\u0026#34;), PushPermissions=columnifexists(\u0026#39;permissions_push_b\u0026#39;,\u0026#34;\u0026#34;), PullPermissions=columnifexists(\u0026#39;permissions_pull_b\u0026#39;,\u0026#34;\u0026#34;), ForkCount=columnifexists(\u0026#39;forks_count_d\u0026#39;,\u0026#34;\u0026#34;), Count=columnifexists(\u0026#39;count_d,\u0026#39;,\u0026#34;\u0026#34;), UniqueUsersCount=columnifexists(\u0026#39;uniques_d\u0026#39;,\u0026#34;\u0026#34;), DismmisedAt=columnifexists(\u0026#39;dismissedAt_t\u0026#39;,\u0026#34;\u0026#34;), Reason=columnifexists(\u0026#39;dismissReason_s\u0026#39;,\u0026#34;\u0026#34;), vulnerableManifestFilename = columnifexists(\u0026#39;vulnerableManifestFilename_s\u0026#39;,\u0026#34;\u0026#34;), Description=columnifexists(\u0026#39;securityAdvisory_description_s\u0026#39;,\u0026#34;\u0026#34;), Link=columnifexists(\u0026#39;securityAdvisory_permalink_s\u0026#39;,\u0026#34;\u0026#34;), PublishedAt=columnifexists(\u0026#39;securityAdvisory_publishedAt_t \u0026#39;,\u0026#34;\u0026#34;), Severity=columnifexists(\u0026#39;securityAdvisory_severity_s\u0026#39;,\u0026#34;\u0026#34;), Summary=columnifexists(\u0026#39;securityAdvisory_summary_s\u0026#39;,\u0026#34;\u0026#34;) 次の画像を参考に Microsoft Sentinel のログ画面から 2 つのパーサ関数を登録しましょう。なお、「従来のカテゴリ」の値については任意の文字列で大丈夫です。\n   (2) 分析ルールの追加 # 次に GitHub 固有の脅威を検出できるように分析ルールをテンプレートから追加していきしょう。Microsoft Sentinel では GitHub 固有の脅威に対する分析ルールのテンプレートが用意されていますので、今回はこちらを活用して分析ルールの追加をしてきたいと思います。出力例のように検索キーワードに「github」と入力することで目的のテンプレートを見つけることができます。\n   なお、2021 年 11 月時点で用意されているテンプレートは次の 6 種類となります。\n| No. | テンプレート名 | 説明 | | 3 | Brute Force Attack against GitHub Account | GitHub アカウントへのブルートフォース攻撃を検知するルール、デフォルトでは 1 日ごとにクエリを実行 | | 5 | TI map IP entity to GitHub_CL | TI インジケータに登録した IP アドレスからのアクセスを検知するルール、デフォルトでは 1 時間ごとにクエリを実行 | | 6 | GitHub Security Vulnerability in Repository | リポジトリ内にセキュリティ脆弱性が含まれていることを検知するルール、デフォルトでは 1 時間ごとにクエリを実行 |\nそれではテンプレートを用いた分析ルールの追加をしていきましょう。まず追加したいルールを選択し、「ルールの作成」ボタンをクリックします。\n   ルールの追加画面ではウィザードにしたがってルールの作成を行っていきます。各パラメータにはテンプレートによってデフォルト値が入力されていますのでとくに値を変更する要件がなければそのまま作成していきましょう。\n   分析ルールが追加されたテンプレートについては次の出力例のように「使用中」アイコンが付与されます。先ほどと同様の手順を繰り返し、他のテンプレートに対しても分析ルールの追加をしていきましょう。出力例のように 6 種類すべてに使用中アイコンが付与されれば脅威を自動検知するルールの設定も終わりです。お疲れ様でした。\n   補足、もう一歩踏み込んだ脅威分析を行うには # Microsoft Sentinel では今回紹介した標準の分析ルールを利用する以外に、カスタム分析ルールを自作してより高度なイベントの検知をすることが可能です。サンプルとして次のようなカスタム分析ルールが Microsoft 技術ブログでも紹介されていますのでぜひ参考にしてみてください。\n リポジトリに対する異常数のクローン操作 リポジトリの一括削除 リポジトリをプライベートからパブリックに変更 リポジトリに対する部外者からのフォーク操作 GitHub Organization へのユーザ招待およびユーザ追加 ユーザへのアクセス許可の追加付与　など   終わりに # 今回は GitHub Enterprise Cloud の各種ログを SIEM マネージドサービスである Microsoft Sentinel (旧称 Azure Sentinel) に集約し、サイバー攻撃の兆候を検知できるようにする方法のご紹介でしたがいかがだったでしょうか？\nGitHub Enterprise Cloud に限らずさまざまな監査ログを収集し、長期保存されている方は多いと思います。ただ、せっかく取得するのであればただ集めて長期保存しておくだけではなく、Microsoft Sentinel を活用してもう一歩進んだセキュアな環境を実現してみる、というのも \u0026ldquo;有り\u0026rdquo; なのではないでしょうか。\n以上、「Microsoft Sentinel を使って GitHub Enterprise Cloud のセキュリティを強化しよう (Azure Functions コネクタ編)」でした。\n  Microsoft Azure は，Microsoft Corporation の商標または登録商標です。 GitHub は、GitHub Inc. の商標または登録商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。    SIEM(Security Information and Event Management) は、さまざまなログを一元的に集約、相関分析をしてサイバー攻撃などの異常を自動的に検出するソリューションです。\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","date":"November 25, 2021","permalink":"/posts/2021/11/azure-sentinel-functions-data-connector-for-ghec/","section":"記事一覧","summary":"みなさん、こんにちは。今回は GitHub Enterprise Cloud(GHEC) の各種ログを SIEM1 マネージドサービスである Microsoft Sentinel (旧称 Azure Sentinel) に集約し、サイバー攻撃の兆候を検知できるようにする方法をご紹介していきたいと思います。","title":"Microsoft Sentinel を使って GitHub Enterprise Cloud のセキュリティを強化しよう (Azure Functions コネクタ編)"},{"content":"みなさん、こんにちは。今回は TaskCat というオープンソースを利用した AWS CloudFormation (CFn) テンプレートの自動テストについてのお話です。\nCFn テンプレートを扱っていると構文エラーチェックはパスしたものの、いざ動かしてみたらスタックの作成でエラーになってしまうといった経験をすることがあるかと思います。TaskCat は多くの方にとってあまり馴染みのないツールだと思いますが、実際に使ってみるととても手軽に CFn テンプレートの自動テストを実現できます。\n今回は Linux 上に開発環境を作るところからはじめて、簡素なサンプルを用いたテストの実行、テスト自動化を組み込んだシンプルな CI/CD パイプラインの構築まで紹介していきたいと思います。これから CFn テンプレート開発されている方で自動テストをやりたいと考えている方は参考にしてみてはいかがでしょうか。\nTaskCat とは # TaskCat とは、AWS CloudFormation (CFn) テンプレートの自動テストを行う Python 製のテストツールです。\nこのツールを利用することで、指定した各リージョンに CFn テンプレートから環境を一時的にデプロイ、各リージョンでのデプロイ可否結果のレポート生成、テストで一時的に作成した環境を削除、といった一連の流れを自動化できます。\nなお、AWS TackCat はローカルでテストを実行する際に Docker が必要となるため、Docker をサポートしていない AWS CloudShell では利用することができないのでご注意ください。\n TaskCat を使ってみよう # 冒頭ですでに述べたとおり、今回は Linux 上に開発環境を作るところからはじめて、簡素なサンプルを用いたテストの実行、テスト自動化を組み込んだシンプルな CI/CD パイプラインの構築まで紹介していきたいと思います。\nまずは開発環境の設定から # それでは Linux 上に開発環境を作っていきたいと思います。まず TaskCat をインストールする事前準備として Python の仮想環境を作成していきましょう。なお、今回の例で使用している Linux ディストリビューションは Amazon Linux 2 です。\n実行例）\n$ sudo yum install -y python3 $ python3 --version Python 3.7.10 $ python3 -m venv venv37 $ . venv37/bin/activate 次に、TaskCat をインストールします。\n実行例）\n$ python3 -m pip install taskcat $ taskcat --version _ _ _ | |_ __ _ ___| | _____ __ _| |_ | __/ _` / __| |/ / __/ _` | __| | || (_| \\__ \\  \u0026lt; (_| (_| | |_ \\__\\__,_|___/_|\\_\\___\\__,_|\\__| version 0.9.25 0.9.25 TaskCat のテストに必要な docker サービスやこの後のステップで利用する git を追加で設定します。\n実行例）\n$ sudo yum install -y docker git $ sudo systemctl start docker 最後に、AWS CLI の設定をして開発環境の構築は完了です。\n実行例）\n$ aws configure 手動でテストを実行してみよう # では簡単なサンプルを用いて TaskCat を使ったテストを行っていきましょう。今回は、東京リージョン(ap-northeast-1)と大阪リージョン(ap-northeast-3)の 2 つのリージョンに対して、同じテンプレートを使ってスタックの作成ができるかを確認していきたいと思います。\nStep1. テスト対象のテンプレートを作成しよう # 今回は VPC を作るだけのとてもシンプルなテンプレートを用意しました。\n作成例）my-vpc.yaml\nAWSTemplateFormatVersion:\u0026#34;2010-09-09\u0026#34;Description:Sample CloudFormation TemplateParameters:vpcIpv4CicdBlock:Type:StringDefault:10.0.0.0/16vpcNameTag:Type:StringResources:myVPC:Type:AWS::EC2::VPCProperties:CidrBlock:!Ref vpcIpv4CicdBlockEnableDnsSupport:trueEnableDnsHostnames:trueTags:- Key:NameValue:!Ref vpcNameTagOutputs:myVpcId:Description:VPC IDValue:!Ref myVPCExport:Name:myVpcIdStep2. TaskCat のテスト定義ファイルを作成しよう # 次に TaskCat のテスト定義ファイル .taskcat.yml を作成します。今回の例では東京リージョン(ap-northeast-1)と大阪リージョン(ap-northeast-3)でテストを実施するように定義しています。\n作成例）.taskcat.yml\nproject:name:sample-taskcat-projectregions:- ap-northeast-1- ap-northeast-3tests:test-my-vpc:parameters:vpcIpv4CicdBlock:10.255.0.0/16vpcNameTag:test-vpctemplate:my-vpc.ymlStep3. テストを実行してみよう # では次のコマンドでテストを実行していきましょう。テストを実行すると、スタック作成が東京リージョンと大阪リージョンに対して並列で実行され、各リージョンでのスタック作成の成否結果の収集、スタック削除まで自動的に行われます。\n実行例）\n$ taskcat test run _ _ _ | |_ __ _ ___| | _____ __ _| |_ | __/ _` / __| |/ / __/ _` | __| | || (_| \\__ \\  \u0026lt; (_| (_| | |_ \\__\\__,_|___/_|\\_\\___\\__,_|\\__| version 0.9.25 [INFO ] : Linting passed for file: /root/aws-taskcat-sample/my-vpc.yaml [S3: -\u0026gt; ] s3://tcat-sample-taskcat-project-XXXXXXX/sample-taskcat-project/my-vpc.yaml [INFO ] : ┏ stack Ⓜ tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f [INFO ] : ┣ region: ap-northeast-1 [INFO ] : ┗ status: CREATE_COMPLETE [INFO ] : ┏ stack Ⓜ tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f [INFO ] : ┣ region: ap-northeast-3 [INFO ] : ┗ status: CREATE_COMPLETE [INFO ] : Reporting on arn:aws:cloudformation:ap-northeast-1:\u0026lt;AWSアカウント名\u0026gt;:stack/tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f/XXXXXXX [INFO ] : Reporting on arn:aws:cloudformation:ap-northeast-3:\u0026lt;AWSアカウント名\u0026gt;:stack/tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f/XXXXXXX [INFO ] : Deleting stack: arn:aws:cloudformation:ap-northeast-3:\u0026lt;AWSアカウント名\u0026gt;:stack/tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f/XXXXXXX [INFO ] : Deleting stack: arn:aws:cloudformation:ap-northeast-1:\u0026lt;AWSアカウント名\u0026gt;:stack/tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f/XXXXXXX [INFO ] : ┏ stack Ⓜ tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f [INFO ] : ┣ region: ap-northeast-1 [INFO ] : ┗ status: DELETE_COMPLETE [INFO ] : ┏ stack Ⓜ tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f [INFO ] : ┣ region: ap-northeast-3 [INFO ] : ┗ status: DELETE_COMPLETE 実行後は taskcat_outputs ディレクトリにリージョンごとの実行ログが出力されますので、こちらからスタック作成の成否結果を確認できます。\n実行例）\n$ tree -a taskcat_outputs/ taskcat_outputs/ ├── index.html ├── tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f-ap-northeast-1-cfnlogs.txt └── tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f-ap-northeast-3-cfnlogs.txt 0 directories, 3 files $ cat taskcat_outputs/*-ap-northeast-3-*.txt ----------------------------------------------------------------------------- Region: ap-northeast-3 StackName: tCaT-sample-taskcat-project-test-my-vpc-d27c255f120c414fb370e9d2827a215f ***************************************************************************** ResourceStatusReason: Stack launch was successful ***************************************************************************** : 以上、ローカル環境で CFn テンプレートのテストを実行する方法のご紹介でした。\nCI/CD パイプラインに組み込んでみよう # 次はもう一歩進んで、ソースコードの更新をトリガーに自動でテストを実行する CI/CD パイプラインを構築し、実際に動かすところまで行ってみたいと思います。なお、今回は AWS リソース作成を簡略化するため AWS クイックスタートを利用して CI/CD パイプラインを構築していきたいと思います。\n なお、今回作成するパイプラインでは開発ブランチ（例では develop ブランチ）に対して更新が入ると、それをトリガーに AWS TackCat を活用した自動テストを実行し、テストに成功したら特定ブランチ(例では main ブランチ)へマージするという一連の流れを自動化しています。\n   Step1. GitHub にリポジトリを作ろう # まずは枠だけ作っておきましょう。中身は後続のステップで入れます。\n   Step2. GitHub でアクセストークンを作ろう # GitHub \u0026gt; Setting \u0026gt; Developer settings \u0026gt; Personal access tokens \u0026gt; Generate new tokens から、指定の通り「repo」と「admin:repo_hook」のスコープを選択したトークンを作ります。\n   作成に成功したら次のようにトークンが表示されます。スタック作成時に利用しますのでコピーしておきましょう。なお、トークン情報が漏れると大変なことになりますので絶対に漏らさないように注意しましょう。\n   Step3. クイックスタートを起動します # ではクイックスタートを活用して環境を構築していきましょう。まずはクイックスタートサイトの「クイックスタートを起動します」をクリックします。\n   リンクをクリックすると自動的にスタックの作成画面へ遷移します。作成先のリージョンがデフォルトだとオレゴンなので適宜変更して「次へ」をクリックします。\n   次にパラメータを入力していきましょう。\n   必要に応じてタグなどの設定を実施し、確認画面で入力ミスがないかを確認したら「スタックの作成」を実行します。\n   あとはステータスが「CREATE_COMPLETE」になるまで待つだけです。\n   以上で、CI/CD パイプラインに必要な AWS リソースの構築まで完了しました。\nStep4. CI/CD パイプラインの動作確認 # ではソースコードを GitHub に登録して CI/CD パイプラインが動作することを確認していきましょう。登録するソースコードは先ほど作成したこちらのファイルです。\n実行例）\n$ tree -a . ├── my-vpc.yaml └── .taskcat.yml 0 directories, 2 files スタック作成時に指定した GitHub の監視対象ブランチ(例では develop ブランチ)へソースコードをプッシュします。\n実行例）\n$ git init . $ git remote add origin \u0026lt;GitHub上のリポジトリ\u0026gt; $ git pull origin main $ git branch develop $ git checkout develop $ git add . $ git commit -m \u0026#34;initial commit\u0026#34; $ git push origin develop さてここからは再び AWS マネジメントコンソールへ移ります。先ほどのプッシュをトリガーにパイプラインが起動していることを確認するため、まずは CodePipeline 画面へ行きましょう。次のように CI/CD パイプラインが動作していることが確認できるかと思います。\n   ここからドリルダウンでビルドログなどを見ることができます。Build ステージの CodeBuild アクションボックスの「詳細」ボタンをクリックし、ビルドログを確認しましょう。出力例では指定したリージョンでのテストに成功していることがわかります。\n   以上、TaskCat によるテストを組み込んだ CI/CD パイプラインの作成のご紹介でした。\n終わりに # TaskCat はいかがだったでしょうか？\n今回はクイックスタートを使って環境を構築しましたが、すでにパイプラインを構築されている方はリポジトリに .taskcat.yml を追加して、pip install tackcat と taskcat test run をステップに追加するだけで簡単に CFn テンプレートの自動テストを組み込むことができます。気になった方はぜひ触ってみていただければと思います。\n以上、AWS CloudFormation テンプレートの自動テストを実現する「TaskCat」のご紹介でした。\n  AWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 GitHub は、GitHub Inc. の商標または登録商標です。 Linux は、Linus Torvalds 氏 の日本およびその他の国における登録商標または商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。  ","date":"October 20, 2021","permalink":"/posts/2021/10/aws-taskcat/","section":"記事一覧","summary":"みなさん、こんにちは。今回は TaskCat というオープンソースを利用した AWS CloudFormation (CFn) テンプレートの自動テストについてのお話です。","title":"TaskCatを使ってCloudFormationテンプレートの自動テストをしよう"},{"content":"みなさん、こんにちは。今回は「AWS Chalice」を活用したサーバレスアプリケーション開発についてのお話です。AWS Summit Online Japan 2021 の日立製作所の講演で軽く触れたこともあり、どこかで紹介したいと思っておりました。\nさて、AWS Chalice は多くの方にとってあまり馴染みのないツールだと思いますが、実際に使ってみるととても手軽に Amazon API Gateway と AWS Lambda を使用するサーバレスアプリケーションを作成してデプロイできます。もちろん、どんなツールにも得手不得手はあるので「すべてのプロジェクトで AWS Chalice の活用が最適か？」と言われればもちろん「No！」なのですが、ちょっと試しに動く Web API をサッと手軽に作りたい、といったケースにはとても良いソリューションだと思います。\n今回は AWS CloudShell 上に開発環境を作るところからはじめて、簡素なサンプルを用いた一連の開発の流れ、テスト自動化を組み込んだシンプルな CI/CD パイプラインの構築まで紹介していきたいと思います。これから実際に動くサーバレスアプリケーションを手軽に作りたいと思われている方は参考にしてみてはいかがでしょうか。\nAWS Chalice とは # AWS Chalice とは、Amazon AWS Gateway や AWS Lambda を用いたサーバレスアプリケーションを、お手軽に開発できるようにする Python 製のサーバレスアプリケーションフレームワークです。具体的には、Web API をシンプルで直感的なコードで実装できるようにする機能や、作成したコードからアプリケーションの作成やデプロイを実行するコマンドラインインタフェース(CLI)といった開発者にやさしい機能を提供してくれます。\n普段から Python に触れている方であれば似たような機能として Flask や Bottle をイメージされるかと思いますが、これらにサーバレス環境へデプロイする機能が追加で付与されたものが AWS Chalice、とイメージしていただくと良いのかなと思います。\n AWS Chalice を使ってみよう # 冒頭ですでに述べたとおり、今回は AWS CloudShell 上に開発環境を作るところからはじめて、簡素なサンプルを用いた一連の開発の流れ、テスト自動化を組み込んだシンプルな CI/CD パイプラインの構築まで紹介していきたいと思います。\nまずは開発環境の構築から # 今回は AWS CloudShell 上に開発環境を作っていきたいと思います。 それではまず AWS Chalice をインストールする事前準備として Python の仮想環境を作成していきましょう。\n実行例）\n$ sudo yum install -y python3 $ python3 --version Python 3.7.10 $ python3 -m venv venv37 $ . venv37/bin/activate 次に、AWS Chalice をインストールします。\n実行例）\n$ python3 -m pip install chalice $ chalice --version chalice 1.26.0, python 3.7.10, linux 4.14.243-185.433.amzn2.x86_64 最後に、AWS CLI の設定をして開発環境の構築は完了です。\n実行例）\n$ aws configure 簡単なアプリケーションを動かしてみよう # ここでは AWS Chalice を使ったアプリケーションの作成からデプロイまでの流れを、次のような簡単なサンプルを用いて紹介していきたいと思います。\n   Step1. 新規プロジェクトの作成 # まずは chalice new-project コマンドを実行して新しいプロジェクトを作成します。出力例のようにプロジェクトを新しく作るとデフォルトでサンプルプログラムも生成されます。\n実行例）\n$ chalice new-project \u0026lt;任意のプロジェクト名\u0026gt; $ cd \u0026lt;任意のプロジェクト名\u0026gt; $ tree -a . ├── app.py # APIの実装を行うファイル ├── .chalice │ └── config.json # Chaliceの設定を行うファイル ├── .gitignore └── requirements.txt # 利用するライブラリの定義を行うファイル 1 directory, 4 file Step2. ソースコードの編集 # 今回はデフォルトで作られたソースコードにちょっとだけ手を加えました。修正後のファイルの中身は次の通りです。実際の例を見ていただけるとわかる通り、AWS Chalice を用いた実装は Python を普段使わないという方でも直感的でわかりやすい構文になっているのではないでしょうか。\n作成例）app.py\nfrom chalice import Chalice app = Chalice(app_name=\u0026#39;\u0026lt;任意のプロジェクト名\u0026gt;\u0026#39;) app.log.setLevel(logging.INFO) @app.route(\u0026#39;/hello\u0026#39;) def hello(): app.log.debug(\u0026#34;Invoking from function hello\u0026#34;) return {\u0026#39;hello\u0026#39;: \u0026#39;world\u0026#39;} @app.route(\u0026#39;/hello\u0026#39;, methods=[\u0026#39;POST\u0026#39;], content_types=[\u0026#39;application/json\u0026#39;], cors=True) def hello_post(): app.log.debug(\u0026#34;Invoking from function hello_post\u0026#34;) request = app.current_request return {\u0026#39;result\u0026#39;: request.json_body[\u0026#39;payload\u0026#39;]} 作成例）requirements.txt\nchalice 作成例）.chalice/config.json\n{ \u0026#34;version\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;app_name\u0026#34;: \u0026#34;\u0026lt;任意のプロジェクト名\u0026gt;\u0026#34;, \u0026#34;stages\u0026#34;: { \u0026#34;dev\u0026#34;: { \u0026#34;api_gateway_stage\u0026#34;: \u0026#34;api\u0026#34; } } } Step3. ローカル環境で動作確認 # AWS 上へデプロイする前にローカル環境で軽く動作をしたい場合は、chalice local コマンドを実行します。 実行例のように期待通りのレスポンスが返ってくるようであれば、いよいよ AWS 上へデプロイをしていきましょう。\n実行例）Tarminal#1から実行\n$ chalice local Serving on http://127.0.0.1:8000 実行例）Tarminal#2から実行\n$ curl http://127.0.0.1:8000/hello {\u0026#34;hello\u0026#34;:\u0026#34;world\u0026#34;} $ curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;payload\u0026#34;:\u0026#34;hello, world\u0026#34;}\u0026#39; http://127.0.0.1:8000/hello {\u0026#34;result\u0026#34;:\u0026#34;hello, world\u0026#34;} Step4. AWS 上へデプロイ # AWS 上へアプリケーションのデプロイをする場合は、chalice deploy コマンドを実行します。実行例ではメッセージから新たに Lambda 関数、API Gateway と IAM ロールが作られていることがわかります。\n実行例）\n$ chalice deploy --stage dev Creating deployment package. Creating IAM role: \u0026lt;任意のプロジェクト名\u0026gt;-dev Creating lambda function: \u0026lt;任意のプロジェクト名\u0026gt;-dev Creating Rest API Resources deployed: - Lambda ARN: arn:aws:lambda:ap-northeast-1:\u0026lt;AWSアカウント名\u0026gt;:function:\u0026lt;任意のプロジェクト名\u0026gt;-dev - Rest API URL: https://\u0026lt;文字列\u0026gt;.execute-api.ap-northeast-1.amazonaws.com/api/ デプロイ完了後は期待するレスポンスが返ってくるか確認しましょう。\n実行例）\n$ curl https://\u0026lt;文字列\u0026gt;.execute-api.ap-northeast-1.amazonaws.com/api/hello {\u0026#34;hello\u0026#34;:\u0026#34;world\u0026#34;} $ curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;payload\u0026#34;:\u0026#34;hello, world\u0026#34;}\u0026#39; https://\u0026lt;文字列\u0026gt;.execute-api.ap-northeast-1.amazonaws.com/api/hello {\u0026#34;result\u0026#34;:\u0026#34;hello, world\u0026#34;} 以上、AWS Chalice を使ったアプリケーションの作成からデプロイまでの一連の流れでした。\nStepEX. デプロイしたアプリケーションの削除 # 不要になったアプリケーションは chalice delete コマンドを使って削除しておきましょう。\n実行例）\n$ chalice delete Deleting Rest API: \u0026lt;文字列\u0026gt; Deleting function: arn:aws:lambda:ap-northeast-1:\u0026lt;AWSアカウント名\u0026gt;:function:\u0026lt;任意のプロジェクト名\u0026gt;-dev Deleting IAM role: \u0026lt;任意のプロジェクト名\u0026gt;-dev ユニットテストを自動化しよう # 近頃はユニットテストの自動化は一般的に行われているかと思います。AWS Chalice でも Python の一般的なテストツール Pytest を使ってユニットテストを実装できます。例として、今回は次のサンプルに対するテストコードを実装してみます。\n作成例）app.py(テスト対象のプログラム)\nfrom chalice import Chalice app = Chalice(app_name=\u0026#39;\u0026lt;任意のプロジェクト名\u0026gt;\u0026#39;) app.log.setLevel(logging.INFO) @app.route(\u0026#39;/hello\u0026#39;) def hello(): app.log.debug(\u0026#34;Invoking from function hello\u0026#34;) return {\u0026#39;hello\u0026#39;: \u0026#39;world\u0026#39;} @app.route(\u0026#39;/hello\u0026#39;, methods=[\u0026#39;POST\u0026#39;], content_types=[\u0026#39;application/json\u0026#39;], cors=True) def hello_post(): app.log.debug(\u0026#34;Invoking from function hello_post\u0026#34;) request = app.current_request return {\u0026#39;result\u0026#39;: request.json_body[\u0026#39;payload\u0026#39;]} Step1. テストコードの実装 # では、まず必要なファイルを用意していきましょう。中身は後で入れるとしてここでは空ファイルだけ作ります。\n実行例）\n$ touch test_requirements.txt # テストプログラムの依存ライブラリ定義 $ mkdir tests $ touch tests/__init__.py $ touch tests/conftest.py # テストプログラム間で共通の処理を実装 $ touch tests/test_app.py # テストプログラムを実装 次に各ファイルを編集していきましょう。今回は HTTP レスポンスのステータスコードとボディの中身を確認するだけの簡単なテストプログラムを用意しました。\n作成例）test_requirements.txt（依存ライブラリの定義）\npytest-chalice pytest-cov 作成例）tests/init.py\n(EOF) 作成例）tests/conftest.py（共通処理の実装）\nimport pytest from app import app as chalice_app @pytest.fixture def app(): return chalice_app 作成例）tests/test_app.py（テストの実装）\nfrom http import HTTPStatus import json def test_hello_get(client): response = client.get(\u0026#39;/hello\u0026#39;) assert response.status_code == HTTPStatus.OK assert response.json == {\u0026#39;hello\u0026#39;: \u0026#39;world\u0026#39;} def test_hello_post(client): headers = {\u0026#39;Content-type\u0026#39;:\u0026#39;application/json\u0026#39;} payload = {\u0026#39;payload\u0026#39;:\u0026#39;hello, world\u0026#39;} response = client.post(\u0026#39;/hello\u0026#39;, headers=headers, body=json.dumps(payload)) assert response.status_code == HTTPStatus.OK assert response.json == {\u0026#39;result\u0026#39;: \u0026#39;hello, world\u0026#39;} def test_hello_put(client): response = client.put(\u0026#39;/hello\u0026#39;) assert response.status_code == HTTPStatus.METHOD_NOT_ALLOWED assert response.json == {\u0026#34;Code\u0026#34;:\u0026#34;MethodNotAllowedError\u0026#34;,\u0026#34;Message\u0026#34;:\u0026#34;Unsupported method: PUT\u0026#34;} Step2. ユニットテストの実行 # ではテストプログラムが用意できたので pytest コマンドを使ってユニットテストを実行しましょう。実行例では、出力メッセージからユニットテスト 3 件が実行され、3 件とも成功 (PASSED) して、C1 カバレッジが 100% 網羅されていることがわかります。\n実行例）\n$ python3 -m pip install -r test_requirements.txt $ pytest -v --cov --cov-branch =========================== test session starts ============================ platform linux -- Python 3.7.10, pytest-6.2.5, py-1.10.0, pluggy-1.0.0 -- /home/cloudshell-user/venv37/bin/python3 cachedir: .pytest_cache rootdir: /home/cloudshell-user/\u0026lt;任意のプロジェクト名\u0026gt; plugins: cov-3.0.0, chalice-0.0.5 collected 3 items tests/test_app.py::test_hello_get PASSED [ 33%] tests/test_app.py::test_hello_post PASSED [ 66%] tests/test_app.py::test_hello_put PASSED [100%] ---------- coverage: platform linux, python 3.7.10-final-0 ----------- Name Stmts Miss Branch BrPart Cover ----------------------------------------------------- app.py 11 0 0 0 100% tests/__init__.py 0 0 0 0 100% tests/conftest.py 4 0 0 0 100% tests/test_app.py 16 0 0 0 100% ----------------------------------------------------- TOTAL 31 0 0 0 100% ============================ 3 passed in 0.05s ============================= 以上、ちょっとしたユニットテストの自動化のご紹介でした。\nCI/CD パイプラインを構築しよう # 次はもう一歩進んで、ソースコードの更新をトリガーに自動でテストを実行してデプロイまで実施する CI/CD パイプラインを構築し、実際に動かすところまで行ってみたいと思います。\nStep1. CI/CD パイプラインの作成 # AWS Chalice には CI/CD パイプライン用の CloudFormation(CFn) テンプレートを自動生成してくれるとても便利な chalice generate-pipeline コマンドがあります。今回の例では、先ほど作ったユニットテストもパイプラインの中で実行するようにしますので、buildspec.yml を分離する -b オプションも付与して実行します。\n実行例）\n$ chalice generate-pipeline -b buildspec.yml \u0026lt;テンプレートファイル名\u0026gt; 作成された CFn テンプレートは中身が大きいのでここには貼り付けませんが、おおよそ次のような AWS リソース作成が定義されています。\n AWS CodeCommit リポジトリ AWS CodeBuild プロジェクト AWS CodePipeline パイプライン Amazon Simple Storage Service(S3) バケット AWS IAM ロールおよびポリシー  なお、今回はユニットテストの結果をレポート出力できるように作成されたテンプレートファイルを編集して CodeBuild に割り当てる権限を追加しました。\n作成例）CFnテンプレート（編集後）\n\u0026#34;CodeBuildPolicy\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;AWS::IAM::Policy\u0026#34;, \u0026#34;Properties\u0026#34;: { \u0026#34;PolicyName\u0026#34;: \u0026#34;CodeBuildPolicy\u0026#34;, \u0026#34;PolicyDocument\u0026#34;: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { + \u0026#34;Action\u0026#34;: [ + \u0026#34;codebuild:CreateReportGroup\u0026#34;, + \u0026#34;codebuild:CreateReport\u0026#34;, + \u0026#34;codebuild:UpdateReport\u0026#34;, + \u0026#34;codebuild:BatchPutTestCases\u0026#34;, + \u0026#34;codebuild:BatchPutCodeCoverages\u0026#34; + ], + \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, + \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; + }, { \u0026#34;Action\u0026#34;: [ では CFn テンプレートからリソースをデプロイしましょう。実行例のようにスタック作成の成功と出力されたら完了です。\n実行例）\n$ aws cloudformation deploy --stack-name \u0026lt;スタック名\u0026gt; --template-file \u0026lt;テンプレートファイル名\u0026gt; --capabilities CAPABILITY_IAM : Successfully created/updated stack - \u0026lt;スタック名\u0026gt; Step2. パイプラインジョブ定義の編集 # 次に chalice generate-pipeline で生成されたパイプラインジョブの定義を編集して、ユニットテストも自動で実行するようにしていきましょう。編集前のファイル内容はこちらです。\n作成例）buildspec.yml（編集前）\nartifacts: files: - transformed.yaml type: zip phases: install: commands: - sudo pip install --upgrade awscli - aws --version - sudo pip install \u0026#39;chalice\u0026gt;=1.26.0,\u0026lt;1.27.0\u0026#39; - sudo pip install -r requirements.txt - chalice package /tmp/packaged - aws cloudformation package --template-file /tmp/packaged/sam.json --s3-bucket ${APP_S3_BUCKET} --output-template-file transformed.yaml version: \u0026#39;0.1\u0026#39; 編集後のファイルは次のようにしてみました。記載順序の入れ替えやビルドフェーズの分割も併せて実施していてわかりにくくなっていますが、要約すると test_requirements.txt の読み込みと pytest コマンドの実行をステップとして追加しています。\n作成例）buildspec.yml（編集後）\nversion: 0.2 phases: install: commands: - python --version pre_build: commands: - sudo pip install --upgrade awscli pip - aws --version - sudo pip install -r requirements.txt -r test_requirements.txt build: commands: - pytest -v -junit-xml=test-result.xml --cov --cov-branch --cov-report=xml --cov-report=term post_build: commands: - chalice package /tmp/packaged - aws cloudformation package --template-file /tmp/packaged/sam.json --s3-bucket ${APP_S3_BUCKET} --output-template-file transformed.yaml reports: pytest_reports: files: - test-result.xml file-format: JUNITXML cobertura_reports: files: - coverage.xml file-format: COBERTURAXML artifacts: files: - transformed.yaml Step3. CI/CD パイプラインの動作確認 # 環境もパイプラインジョブの定義も整いましたので、あとは作成された CodeCommit リポジトリへソースコードを登録して、それをトリガーに自動でパイプラインが実行されるところまで見ていきましょう。\nまずはローカルに git リポジトリを作成して、ソースコードのコミットまでしていきましょう。今回の .gitignore ファイルは GitHub さんが公開する Python 向けテンプレートを使ってます。\n実行例）\n$ git init . $ curl https://raw.githubusercontent.com/github/gitignore/master/Python.gitignore \u0026gt; .gitignore $ git add . $ git commit -m \u0026#34;Initial commit\u0026#34; 次に、ソースコードを格納する AWS CodeCommit リポジトリの URL を確認します。\n実行例）\n$ aws cloudformation describe-stacks --stack-name \u0026lt;スタック名\u0026gt; --query \u0026#39;Stacks[0].Outputs\u0026#39; : - OutputKey: SourceRepoURL OutputValue: https://git-codecommit.ap-northeast-1.amazonaws.com/v1/repos/\u0026lt;プロジェクト名\u0026gt; : 先ほど調べたリポジトリ情報を元に、リモートリポジトリを追加します。\n実行例）\n$ git remote add codecommit https://git-codecommit.ap-northeast-1.amazonaws.com/v1/repos/\u0026lt;プロジェクト名\u0026gt; CodeCommit へ Push できるように認証情報ヘルパーを設定します。\n実行例）\n$ git config --global credential.helper \u0026#39;!aws codecommit credential-helper $@\u0026#39; $ git config --global credential.UseHttpPath true ではソースコードをリモートレポジトリへ push してみましょう。\n実行例）\n$ git push codecommit master さてここからは AWS マネジメントコンソールに移ります。先ほどの git push をトリガーにパイプラインが起動していることを確認するためまずは CodePipeline 画面へ。次のように CI/CD パイプラインが動作していることが確認できるかと思います。\n   では次に、ビルド処理に追加したユニットテストが動いているかを確認しましょう。Build ステージの CodeBuild アクションボックスの「成功しました」メッセージのすぐ下にある「詳細」ボタンをクリックして CodeBuild 画面へ進みましょう。次のようにビルドログからユニットテストが実行されていることが確認できるかと思います。\n   また、テストとカバレッジのレポートについてはレポートグループの方にも登録されていることが確認できるかと思います。\n   では CodePipeline 画面へ戻りましょう。Beta ステージで実施しているアプリケーションのデプロイにも成功していることがわかりますね。ExcecuteChangeSet アクションボックスの「詳細」ボタンをクリックし、CloudFormation 画面へ進みましょう。\n   CloudFormation 画面では出力の一覧から EndpointURL 値を確認しましょう。\n   最後に、期待するレスポンスが返ってくるか生成された EndpointURL に向けて HTTP リクエストを発行して確認しましょう。実行例では期待するレスポンスが返ってきてますね。\n実行例）\n$ curl https://\u0026lt;文字列\u0026gt;.execute-api.ap-northeast-1.amazonaws.com/api/hello {\u0026#34;hello\u0026#34;:\u0026#34;world\u0026#34;} $ curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;payload\u0026#34;:\u0026#34;hello, world\u0026#34;}\u0026#39; https://\u0026lt;文字列\u0026gt;.execute-api.ap-northeast-1.amazonaws.com/api/hello {\u0026#34;result\u0026#34;:\u0026#34;hello, world\u0026#34;} 本来はもっとパイプラインに条件分岐を持たせたりすると思いますが、ひとまずこれでソースコードのプッシュを契機にテストを走らせてからデプロイを行う、という最低限の流れを自動化することができました。\n以上、ちょっとした CI/CD パイプラインの作成でした。\nちゃんとしたアプリケーションを開発するには # 公式ドキュメントを活用しよう # ちゃんとしたアプリケーションにするにはもちろん色々と機能を実装しないといけませんよね。こんなときにいつも私がお世話になっているのが公式ドキュメントです。トピックごとに簡単なサンプルを交えた実装方法について記載がありますのでとても参考になるかと思います。\n サービス別資料を参考にしよう # もちろん AWS さんのサービス別資料もとても参考になります。こちらも目を通していただけると良いかと思います。\n 余談ですが、、、 # 本記事執筆のきっかけとなった AWS Summit Online Japan 2021 で行った日立の講演についてもご紹介させてください。\nこの講演では、IoT やサーバレス技術を活用した異なるタイプ(データ分析系とアプリケーション開発系)の事例を 1 件ずつお話させていただきました。AWS Chalice については、2 つ目のアプリケーション開発系の事例にて本当に軽くですが触れておりますのでよろしければご参照いただけると幸いです。\nこの講演を通じて「ほほう、日立ってこんなこともしてたんだ」と多少なりとも良いイメージを抱いてもらえるきっかけになってくれると嬉しいなと思ってます σ(,,´∀ ｀,,)\n 終わりに # AWS Chalice はいかがだったでしょうか？\nAWS には、AWS SAM (Serverless Application Model)といった別のサーバレスアプリケーション開発用フレームワークがありますが、こちらと比較すると機能がかなり限定されることもあって、今回ご紹介した AWS Chalice は多くの方にとって馴染みもなければ、なかなかに触れる機会も少ないのかもしれません。\nただ、実際に触ってみていただけばわかる通り、とても簡単・手軽にサーバレスアプリケーションを作ることができますので、迅速性を求められる PoC(概念実証)や PoV(価値実証)といった場面でとくに有用なのでは、と考えています。気になった方はぜひ触ってみて、そのお手軽さを実際に体験していただければと思います。\n以上、AWS 上でサーバレスアプリケーションを手軽に開発できるようにする「AWS Chalice」のご紹介でした。\n  AWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。  ","date":"October 18, 2021","permalink":"/posts/2021/10/aws-chalice/","section":"記事一覧","summary":"みなさん、こんにちは。今回は「AWS Chalice」を活用したサーバレスアプリケーション開発についてのお話です。AWS Summit Online Japan 2021 の日立製作所の講演で軽く触れたこともあり、どこかで紹介したいと思っておりました。","title":"AWS Chaliceを使ってサーバレスアプリケーションを開発しよう"},{"content":"今更ですが Microsoft Build 2021 にて AKS(Azure Kubernetes Service) on Azure Stack HCI の一般提供が発表されておりました。\n この機能によって Kubernetes のコントロールプレーンは Azure マネージドで提供されつつも、ワークロードはオンプレの Azure Stack HCI 上で動かせるようになります。\nそのため、PoC などは Azure 上の AKS でフットワーク軽くやって、でも本番はオンプレの AKS on Azure Stack HCI でカッチリ、っていうシナリオも描けるようになってきた感じでしょうか。Azure Stack HCI が世に出てから2年越し(?)でやっと単なるサーバ製品じゃなく、Azure を冠した意味/メリットがでてきてくれそうですね。\n","date":"July 1, 2021","permalink":"/posts/2021/07/azure-aks-on-hci-entered-ga/","section":"記事一覧","summary":"今更ですが Microsoft Build 2021 にて AKS(Azure Kubernetes Service) on Azure Stack HCI の一般提供が発表されておりました。","title":"AKS on Azure Stack HCIの一般提供(GA)が開始しました"},{"content":"Microsoft 認定 Azure Data Scientist Associate (DP-100) を 2021/5/28 に受験していたのでその時のメモです。ほんとは違う試験を受けに行く予定だったので情報少ないです。すんません。\n受験メモ # 受験者情報 #  AWS 資格 12 種コンプ Azure 資格 4 種 (AZ-900、AZ-104、AZ-400、AZ-303/304) GCP 資格 3 種 (ACE、PCA、PDE)  結果 #  781 点 (合格)  試験対策例 #  合格者のブログを漁って勉強の計画などを立てる (うっかりで 0 時間) 試験のアウトラインを読んで不安な箇所がないかを確認する (うっかりで 0 時間) 自信のない部分を Microsoft Learn などでお勉強する (うっかりで 0 時間) Udemy などで評価の高い問題集をやる (うっかりで 0 時間)  感想 #  Azure Machine Learning の知識が無くても答えられる設問が多かった印象でした。たとえば、Python のコードが出てきて実装コードの穴あきを埋めよとか、トレーニングパイプライン/推論パイプラインの穴あきを埋めよとか、xx のケースだとモデルデプロイ先のアーキテクチャとして最適なものはどれか？みたいな。いやほんと、Azure 以外でも通用するような一般知識を問われるケースが多かったです。  アドバイスなど #  DP-100 は、Azure Machine Learning に特化した試験なのではっきり言って範囲は広くないです。ワークスペースをどうやってセットアップするか、セットアップ後はどうやってモデルのトレーニングを始めるのか、モデルの精度がイマイチだった場合にどういう手段が取れるのか、トレーニングが終わったモデルを実際にアプリから呼び出して使う際はどうするか、とかいった一連の流れを問われます。ここら辺の分野がまったくの素人ですって方は、時間効率は悪いですが MS Learn でもひととおり学べるので目を通しておくと良いでしょう。 もし仮に勉強する時間が取れていたら、Udemy で評価の高い英語の問題集を購入してやっていたかと思います。Solutions Architect とかの人気試験ならまだしも、この手の試験は日本語で質の良い問題集はまずないと思った方がいいですし。  最後に # 試験を申し込むときは似たような名前の試験があるので最新の注意を払いましょう！\n試験を申し込むときは似たような名前の試験があるので最新の注意を払いましょう！\n重要なことなので 2 回書きました。本当は私も Power BI の試験 (D A -100) を申し込んだつもりだったんですよね、、、試験を開始して 5 問くらいやったあたりで「はわわわ、こいつはおかしい！Azure Machine Learning に関する問題しか出てこないんですけどーーーー！」と内心めちゃくちゃ焦ってました。でも、なんとか合格できたのでラッキーでしたね^^;\nということで、Microsoft 認定 Azure Data Scientist Associate (DP-100) を受験したときのお話でした。\n","date":"June 5, 2021","permalink":"/posts/2021/06/azure-data-scientist-associate-dp-100-exam/","section":"記事一覧","summary":"Microsoft 認定 Azure Data Scientist Associate (DP-100) を 2021/5/28 に受験していたのでその時のメモです。ほんとは違う試験を受けに行く予定だったので情報少ないです。すんません。","title":"Microsoft 認定 Azure Data Scientist Associate (DP-100) 受験メモ"},{"content":"あれは Azure を業務で扱いだして 2~3 か月くらい経った頃、そろそろ Solutions Architect Expert を取りに行かねばな、と思い立って AZ-303 (2021/1/5) と AZ-304 (2020/11/25) の試験を受けに行った時のメモです。\n受験メモ # 受験者情報 #  AWS 資格 5 種 (CLF、SAA、SAP、DOP、SCS) Azure 資格 3 種 (AZ-900、AZ-104、AZ-400) GCP 資格 1 種 (PCA)  結果 #  AZ-303: 760 点 (合格) AZ-304: 895 点 (合格)  試験対策例 #  合格者のブログを漁って勉強の計画などを立てる (サボったので 0 時間) 試験のアウトラインを読んで不安な箇所がないかを確認する (約 1 時間) 自信のない部分を Microsoft Learn などでお勉強する (サボったので 0 時間) Udemy などで評価の高い問題集をやる (サボったので 0 時間)  感想 #  AZ-303 は、AZ-304 よりも偏りなくまんべんなくいろんな分野の問題が出た印象。テクノロジーと銘打つ試験の通り、AZ-304 よりも個々のサービスについて深くて細かいノウハウを要求された印象。 AZ-304 は幅広く、いろいろな分野の問題が出てくるが、Azure AD のハイブリッドクラウドの設計が感覚的に 1 割くらいと多かった印象。デザインと銘打つ試験の通り、細かい仕様よりもシステムを設計する上での考え方などが重要な印象。上流設計をメインでされている方は AZ-303 よりも余裕をもって臨めるかと。 Azure は AZ-303/AZ-304 の 2 つの試験をパスしないと SA になれないので正直 AWS や GCP よりも手間がかかりましたね。試験の難易度は感覚的に AWS \u0026raquo;\u0026gt; Azure = GCP の印象。※あくまで個人的な意見なのであしからず。  アドバイスなど #  AZ-303 と AZ-304 の出題範囲はおおよそ同じなので、2 つの試験はあまり間隔を開けずに受験することをオススメします。 AZ-303 は、たとえば、SQL サーバを冗長化するには RG は同じである必要があるか？SQL サーバをリージョン間で冗長化するための前提条件は？みたいな感じの細かい知識を求める問題が多いです。知らないと勘になってしまう(=選択肢は絞れるけど、どんなに考えても導き出せない)ので試験ガイダンスにしたがって穴を埋めるのが良いかと思います。 AZ-304 は、FgCF (Financial-grade Cloud Fundamentals) や Azure Well-Architecture フレームワークの知識を叩き込んでいけばどうにかなると思います。また、他クラウドのべスプラが頭に入っている方であれば、Azure についてそのままズバリを知らなくてもこうあるべきだ、を考えれば答えが導き出せるかも！？     本試験を受けた知人が以下の問題集から似たような問題がちょいちょい出てきたので点数の底上げに役立った、とのこと。評価も高いですし勉強素材の選択肢の1つとして検討してみても良いかも！？    終わりに # 繰り返しになりますが、Microsoft 赤間氏が GitHub で公開してくれている FgCF (Financial-grade Cloud Fundamentals) のコンテンツは Azure を設計する上で重要なエッセンスがギュッと詰まっていて、金融って銘打ってますけど他業界でも十分使える内容となっています。試験対策とか置いといて、Azure って何から勉強すればいいの？という方には「まずは FgCF を見んしゃい！」と言わせていただきたいですね。\nということで、Microsoft 認定 Solutions Architect Expert (AZ-303/AZ-304) を受験したときのお話でした。\n、、、余談ですが、テクノロジー (AZ-303) の方はノーガード戦法だと辛いだろうなと思って試験前に大型連休を挟んだ　はず　だったんですけどね、、、いやー、合格できてラッキーでした^^;\n","date":"June 5, 2021","permalink":"/posts/2021/06/azure-solutions-architect-expert-az-303-304-exam/","section":"記事一覧","summary":"あれは Azure を業務で扱いだして 2~3 か月くらい経った頃、そろそろ Solutions Architect Expert を取りに行かねばな、と思い立って AZ-303 (2021/1/5) と AZ-304 (2020/11/25) の試験を受けに行った時のメモです。","title":"Microsoft 認定 Azure Solutions Architect Expert (AZ-303/AZ-304) 受験メモ"},{"content":"みなさん、こんにちは。ちょっと前に AWS さんのブログを漁ってたら「AWS SaaS Boost がオープンソースとしてリリースされました」っていう記事が目に入ってきたんですよね。\n 普通は AWS SaaS Boost をサービスとして利用すればいいだけですし、へー、コードが公開されたんだ、でスルーしようとも思ったんですが、なんとなく気になったんです。もしかするとリポジトリをフォークして魔改造して使うときが来るかもしれない。いや、来ないだろうけど、、、\nということで、そんな時に備えるためちゃちゃっと改造して、改造したものを動かしてみるところまで試せたらいいなーと思って筆をとってみた次第です。\nそもそも \u0026ldquo;AWS SaaS Boost\u0026rdquo; って何者よ？ # AWS SaaS Boost というのは、自分の持っているアプリケーションを SaaS 化したいなー！というときに強い味方となってくれるサービスです。\n AWS さんのブログに書いてある内容をそのまま載せちゃいますが、要は、こちらのサービスが SaaS 化する際に作りこまないといけない機能を補完してくれる ってことなんですね。\n 既存のアプリケーションを SaaS Boost にインストールするだけで、テナント管理、デプロイ、テナントごとの分析、ビリング（請求）、メータリングがすべてセットアップされ、すぐに使用できるようになります。これにより、ソリューションを再構築するコストをかけることなく、より迅速に、製品を SaaS モデルで市場に提供することが可能になります。\n  ということでユーザは、よりアプリのコア機能開発に時間を割くことができるようになる ってことなので、使うメリットは十分にあるんじゃなかろうかと思います。\nまずはオリジナルのまま環境を作ってみる # ん？改造するんじゃないの？と思った方は正解！でも、改造する前に絶対やっておくべきプロセスです。とくに若いオープンソースにあるあるだと思いますけど、「手順通りにやったけど動かないぜーー！」ということがあります。ということで、改造云々の前に導入するにあたって追加の前提条件や手順がないのかを確認しておきたいと思います。\nということで、こちらのドキュメントに基本沿って作っていきたいと思います。ちなみに、この記事の Step とドキュメント上の Step の数字は一致するように書いてます。題名は若干違いますが、、、\n Step0: 環境の準備 # 今回は Amazon Linux 2 の EC2 インスタンスを用意しました。メモリは 4GB 以上必要なようです。\nStep1: 前提パッケージの導入 # 公式ドキュメントにリスト掲載されているパッケージをインストールしていきましょう。\n  Java 11 Amazon Corretto 11 Apache Maven AWS Command Line Interface version 2 Git Node 14.15 (LTS) Yarn   公式ドキュメントにはコマンドラインまでは書かれてませんけど Amazon Linux 2 だと下の例のような感じです。補足ですが、筆者の環境だと OpenJDK 8 がデフォルトの java のバージョンになってたので、必要に応じてコマンド例のように java バージョンを切り替えて使ってください。永続化はしてないので注意。\n実行例）Amazon Linux 2 の場合\n$ sudo yum update -y $ sudo yum install -y java-11-amazon-corretto maven $ sudo alternatives --config java $ export JAVA_HOME=/usr/lib/jvm/java-11-amazon-corretto.x86_64/ $ curl --silent --location https://rpm.nodesource.com/setup_14.x | sudo bash - $ sudo yum install -y nodejs $ curl --silent --location https://dl.yarnpkg.com/rpm/yarn.repo | sudo tee /etc/yum.repos.d/yarn.repo $ sudo yum install -y yarn AWS CLI v2 は最初から入っている環境であったため手順を省いてしまいましたが、もし自分で導入される際はこちらをご参照くださいね。\n Step2: ソースコードを取得 # AWS SaaS Boost のソースコードを GitHub からダウンロードします。 ここは公式ドキュメントにコマンドラインが書かれているのでそのまま実行すれば OK なんですが、今回はオリジナルからフォークしてきた個人のリポジトリを使うようにしましたよ、と。\n実行例）Amazon Linux 2 の場合\n$ git clone https://github.com/chacco38/aws-saas-boost.git ./aws-saas-boost Step3: アプリをビルド＆デプロイ # AWS CLI の設定をした上で対話型インストーラを用いて AWS SaaS Boost を導入していきましょう、、、って、このタイミングでシステム要件出してくるのか、まぁいいけどｗ\n The system where you run the installation should have at least 4 GB of memory.\n 気を取り直して、コマンドを実行していきましょう。こんな感じです。ちなみに、出力形式を「YAML」にすると install.sh がエラーになったので「JSON」に設定してます。\n実行例）Amazon Linux 2 の場合\n$ aws configure $ aws configure set output json $ cd aws-saas-boost $ sh -x install.sh install.sh スクリプトの処理が進むとインストーラ起動オプションの入力を迫られます。今回は「1」を選択。\n出力例）インストーラ起動オプション入力時\n=========================================================== Welcome to the AWS SaaS Boost Installer Setting version to v0 as it is missing from the git properties file. Installer Version: d64277c-dirty, Commit time: 2021-05-19T21:15:02+0000 Checking maven, yarn and AWS CLI... Environment Checks for maven, yarn, and AWS CLI PASSED. =========================================================== 1. New AWS SaaS Boost install. 2. Install Metrics and Analytics in to existing AWS SaaS Boost deployment. 3. Update Web Application for existing AWS SaaS Boost deployment. 4. Update existing AWS SaaS Boost deployment. 5. Delete existing AWS SaaS Boost deployment. 6. Exit installer. Please select an option to continue (1-6): 1 次に各パラメータの入力を求められるので適当に設定していき、、、\n出力例）パラメータ入力時\nDirectory path of saas-boost download (Press Enter for \u0026#39;/home/ssm-user/aws-saas-boost\u0026#39;) : ***** Enter name of the AWS SaaS Boost environment to deploy (Ex. dev, test, uat, prod, etc.): ***** Enter the email address for your AWS SaaS Boost administrator: ***** Enter the email address address again to confirm: ***** Would you like to setup a domain in Route 53 as a Hosted Zone for the AWS SaaS Boost environment (y or n)? ***** Would you like to install the metrics and analytics module of AWS SaaS Boost (y or n)? ***** If your application requires a FSX for Windows Filesystem, an Active Directory is required. Would you like to provision a Managed Active Directory to use with FSX for Windows Filesystem (y or n)? ***** 最後にパラメータの確認をしてから「continue」を意味する「y」を入力したら、あとは何もせずに完了を待つだけです。筆者の環境だと完了まで 15 分弱かかりました。\n出力例）入力パラメータ確認時\nWould you like to continue the installation with the following options? AWS SaaS Boost Environment Name: ***** Admin Email Address: ***** Route 53 Domain for AWS SaaS Boost environment: ***** Install Metrics and Analytics: ***** Amazon Quicksight user for setup of Metrics and Analytics: ***** Setup Active Directory for FSX for Windows: ***** Enter y to continue or n to cancel: y install.sh スクリプトが完了したらこちらの EC2 インスタンス上での操作はいったん終了のようです。ということで、管理者 Email アドレスに新しいメールが届いていると思うのでメールボックスを見に行きましょう。\nStep4: AWS SaaS Boost への初回ログイン # ここからは Web ブラウザを中心に作業を行っていきます。本ステップでは管理者へ届いたメール本文に書かれている URL へアクセスする、ログインする、そしてパスワード変える、それだけです。入れたことを確認したら終わりです。\n公式ドキュメントでは Step5 以降で AWS SaaS Boost アプリ上の設定をしていくわけですが、今回は導入までの流れを確認したかっただけなので以降の手順は割愛します。\n余談、導入成功までの過程で出たエラー # ということで、今回導入するにあたって出てきたエラー 2 件のご紹介をしたいと思います。\nCase1: 「Error with build of Java installer for SaaS Boost」というエラーメッセージを出力してスクリプト停止 # Step1 で必要なパッケージをインストールだけしてスクリプトを実行したらこんな感じでエラーになったんです。mvn コマンドがエラーになったように見えるんだけど mvn はなんもエラーはいてないぞ、と。\n実行例）\n$ sh -x install.sh (省略) + cd /home/ssm-user/aws-saas-boost/installer + echo \u0026#39;Build installer jar with maven\u0026#39; Build installer jar with maven + mvn + echo \u0026#39;Error with build of Java installer for SaaS Boost\u0026#39; Error with build of Java installer for SaaS Boost + exit 2 そこでスクリプトの中身を見てみたんです。すると、mvn の標準出力も標準エラー出力も/dev/null にリダイレクトしてましたよ、と。\n実行例）\n$ less install.sh (/mvn) (n) if ! mvn \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 ; then echo \u0026#34;Error with build of Java installer for SaaS Boost\u0026#34; exit 2 fi ということで、mvn コマンドが何を出力するのかを見たくて手動で実行しましたよ、と。 以下のようにちゃんとメッセージでてくれました。「javac: invalid target release: 11」か、、、Java のターゲットリリースが 11 じゃないと怒られてた^^;\n実行例）\n$ cd /home/ssm-user/aws-saas-boost/installer $ mvn (省略) [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.6.0:compile (default-compile) on project SaaSBoostInstall: Compilation failure [ERROR] javac: invalid target release: 11 [ERROR] Usage: javac \u0026lt;options\u0026gt; \u0026lt;source files\u0026gt; [ERROR] use -help for a list of possible options [ERROR] -\u0026gt; [Help 1] [ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch. [ERROR] Re-run Maven using the -X switch to enable full debug logging. [ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles: [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException 確認すると、たしかに OpenJDK のバージョン 8 が設定されておりました。凡ミス。\n実行例）\n$ java -version openjdk version \u0026#34;1.8.0_282\u0026#34; $ mvn -version Java version: 1.8.0_282, vendor: Red Hat, Inc. ということで、Corretto のバージョン 11 で動くように設定して、再度 mvn を実行すると成功しましたよ、と。\n実行例）\n$ sudo alternatives --config java $ java -version openjdk version \u0026#34;11.0.11\u0026#34; 2021-04-20 LTS $ export JAVA_HOME=/usr/lib/jvm/java-11-amazon-corretto.x86_64/ $ mvn -version Java version: 11.0.11, vendor: Amazon.com Inc. $ mvn ということで install.sh から実行してもちゃんとビルドは通りました。ちゃんちゃん。\nCase2: 「Could not execute \u0026lsquo;aws sts get-caller-identity\u0026rsquo;, please check AWS CLI configuration.」というエラーメッセージを出力してスクリプト停止 # でも、すぐに次のエラーはやってきんです。「aws sts get-caller-identity」の実行ができなかったぞ、と。\n実行例）\n$ sh -x install.sh ： + echo \u0026#39;Launch Java Installer for SaaS Boost\u0026#39;Launch Java Installer for SaaS Boost + java -Djava.util.logging.config.file=logging.properties -jar /home/ssm-user/aws-saas-boost/installer/target/SaaSBoostInstall-1.0.0-shaded.jar WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance. Setting version to v0 as it is missing from the git properties file. =========================================================== Welcome to the AWS SaaS Boost Installer Setting version to v0 as it is missing from the git properties file. Installer Version: d64277c-dirty, Commit time: 2021-05-19T21:15:02+0000 Checking maven, yarn and AWS CLI... Could not execute \u0026#39;aws sts get-caller-identity\u0026#39;, please check AWS CLI configuration. 実行できなかったってどういう意味なのかさっぱり分からないので、まずは手動で実行してみるとあっさり原因判明。YAML という知らない出力タイプが設定されているって言われてました、、、なるほど aws configure の設定値が悪いのか。\n実行例）\n$ aws sts get-caller-identity Unknown output type: yaml ということで出力フォーマットを JSON 形式にして実行すると、ちゃんとインストーラのメニュー表示まで進んでくれました。ちゃんちゃん。\n実行例）\n$ aws configure set output json $ sh install.sh ： =========================================================== Welcome to the AWS SaaS Boost Installer Setting version to v0 as it is missing from the git properties file. Installer Version: d64277c-dirty, Commit time: 2021-05-19T21:15:02+0000 Checking maven, yarn and AWS CLI... Environment Checks for maven, yarn, and AWS CLI PASSED. =========================================================== 1. New AWS SaaS Boost install. 2. Install Metrics and Analytics in to existing AWS SaaS Boost deployment. 3. Update Web Application for existing AWS SaaS Boost deployment. 4. Update existing AWS SaaS Boost deployment. 5. Delete existing AWS SaaS Boost deployment. 6. Exit installer. Please select an option to continue (1-6): ということでエラー集のご紹介でした。\nでは改造して使ってみよう # なんですが、もうオリジナルを導入しただけおなかいっぱいだな、、、ってことで、またどこか時間が取れるときにでも更新しようと思います^^;\n(まぁ後回しにした場合は大抵やらないんだけどね、、、)\n終わりに # ということで、今回は「AWS SaaS Boost がオープンソースとして公開されたのでさっそくいじってみよう（と思っただけ）」でした。\n、、、えっ！？タイトルが違う？まぁいいじゃないかｗ\n","date":"May 29, 2021","permalink":"/posts/2021/05/aws-saas-boost-installation-steps/","section":"記事一覧","summary":"みなさん、こんにちは。ちょっと前に AWS さんのブログを漁ってたら「AWS SaaS Boost がオープンソースとしてリリースされました」っていう記事が目に入ってきたんですよね。","title":"AWS SaaS Boost がオープンソースとして公開されたのでさっそくいじってみよう"},{"content":"開始から少し時間が経ってしまいましたが、2021年7月いっぱいまでの期間限定かつ自宅からのリモート受験限定で、2回目の再受験が無料になるキャンペーンが開催されています。\nもちろん、来るかもわからない2回目の試験に保険をかけるの？っていう方もいるとは思いますが、ひとまず「普段は会場派という方も一考の余地あり」かとは思います。\n    ","date":"May 28, 2021","permalink":"/posts/2021/05/aws-exam-free-retry-campaign/","section":"記事一覧","summary":"開始から少し時間が経ってしまいましたが、2021年7月いっぱいまでの期間限定かつ自宅からのリモート受験限定で、2回目の再受験が無料になるキャンペーンが開催されています。","title":"AWS 認定試験の1回目が不合格でも2回目の再受験が無料になるキャンペーンが開催されています"},{"content":"最近も出会ってしまった。こんな誤解。\nセキュリティグループ、正確に言えば Network Security Groups(NSG)。AWS のセキュリティグループと同じで、ネットワークインターフェースに割り当てて使う仮想ファイアウォール。こちらの NSG なんですが、ちょっと AWS と違うのは、NSG の割当先として NIC とサブネットの 2 つから選べる点。\nそして MS から推奨されているのは、運用管理をシンプルにするということで個別の NIC ごとに NSG を割り当てるではなく、サブネットに NSG を割り当てる、です。なので、ふつうに設計を進めていくと用途ごとにサブネットを分けて、サブネットに NSG を割り当てていくことになるんですよね、、、と、ここで誤解が生じるんです。\nそう、サブネットに NSG を割り当てるってそのまま聞くと動き的には、いわゆる AWS でいうネットワーク ACL のような感じになるんじゃなかろうか、と。ちなみに、AWS のネットワーク ACL っていうのは、サブネットへの入出力のところでチェックをかけるっていうやつですよね。なので、サブネット内通信はネットワーク ACL を制御できないので、サブネット内は何でも OK なノーガード状態になるわけです。\nでも Azure でサブネットに NSG を割り当てた時の正しい動きは、「サブネットに NSG を割り当てると自動的にサブネット配下の NIC すべてに当該 NSG が適用される」なので、サブネット内通信もしっかり NSG でガードされます。Azure 有識者だと当たり前なんですが言葉通りにサブネットへ NSG を割り当てるだと、サブネット内はザルの方を想像される方が多いようです。とくにネットワークに詳しいエンジニアさんの方が誤解される傾向にあるかもしれないです。\nもう頭の中ではサブネット内通信はザルと思っているから、サブネットに NSG を割り当てて実際に動かしてみると「あれ、通信できないんですけどーーーー！！」ということで慌てられるケースもあったりなかったり。ということで、Azure を利用する際は少しだけお気を付けを。\n","date":"May 19, 2021","permalink":"/posts/2021/05/azure-misanderstanding-about-security-group/","section":"記事一覧","summary":"最近も出会ってしまった。こんな誤解。","title":"たまに出会う Azure のセキュリティグループに関する誤解"},{"content":"Google Cloud認定資格を受けたことがある方はご存じの通り、専用の申し込みサイト(Google Cloud Webassessor)から試験の予約をするのですが、英語試験の予約と日本語試験の予約では異なるアカウント（Emailアドレスは同じ)が必要になります。\nなぜ異なるアカウントが必要なのか # 理由は「日本語試験用アカウントでログインすると英語の試験を選択できない仕様だから」です。もちろん逆もまた然り、です。\nこれを知らないと、英語版の申し込みサイトに飛んで（日本語専用とは知らずに日本語専用の）アカウントでログインすると、（英語試験の申し込みサイトではなくて日本語試験の申し込みサイトに飛んでしまってて）英語での試験項目が一覧に出てこなくて「なぜだ!?」となります。\nそして再び英語版の申し込みサイトへ戻って、再度同じアカウントでログインしなおして、再度同じ事象に陥る、、、のループを繰り返してムダな時間をしばらく過ごしてしまう方も、きっと少なからずいらっしゃることかと思います。少なくとも筆者はムダな時間を過ごしました^^;\nということで、過去に日本語試験を受けたことがある方で、これから英語試験を受けようかなって思われている方はご注意ください。\nここからは完全に雑談です # そういえば英語版のGoogle Cloud Webassessorアカウントを作成する際に、次のように「Eメールアドレスはログイン名に使うなよ！」という注意書きが出てきて、めちゃくちゃドキっとしたんですよね。\n   なんでかというと、私、日本語版のアカウント名にEメールアドレスを使っていたから、なんです、、、\nまじかー、完全にやらかしたわー、と思いつつ日本版のアカウント作る時にも「Eメールアドレスはダメよ！」って注意書きが出てきたのにそれを見逃したんだなきっと、、、と自分を責めつつも、念のため日本語のアカウント作成画面へ確認しにいったんです。\nすると、あろうことか日本語サイトくんはこう言っているわけです。\n   おいおいおいおい、さすがにこの翻訳はイケてなさすぎるでしょう、、、そらー、Eメールアドレスをアカウント名に使っちゃうわ、っていうね。はい、以上どうでもいい小ネタでしたｗ\n","date":"May 17, 2021","permalink":"/posts/2021/05/gcp-exam-account-required-for-each-language/","section":"記事一覧","summary":"Google Cloud認定資格を受けたことがある方はご存じの通り、専用の申し込みサイト(Google Cloud Webassessor)から試験の予約をするのですが、英語試験の予約と日本語試験の予約では異なるアカウント（Emailアドレスは同じ)が必要になります。","title":"Google Cloud資格試験は言語ごとに異なるアカウントが必要なのでご注意を"},{"content":"みなさん、こんにちは。今回は Azure Active Directory(Azure AD) でアプリケーションと SAML 連携する際に使う SSL 証明書に関するお話です。\n最近は Azure AD と SaaS アプリケーション、たとえば GitHub や JFrog Artifactory など、とを SAML 連携してシングルサインオン(SSO)を実現しています、といった事例もそこそこあるんかなと推測してます。\nただご存じの通り、Azure AD で作成した証明書って期限が 3 年なんですよね。ん？証明書の期限切れになったら何が起きるか、ですか？答えはシングルサインオンに失敗します。つまりサインインできません。もちろん救済処置を用意している SaaS アプリケーションもあるんですけどね、、、\nということで、まぁ 3 年って個人的には優しい方だと思うんですが、証明書のロールオーバー作業を定期的に実施する必要が出てくるわけです。面倒なのですがね^^;\nただ最近、「そもそも証明書の期限をもっと長くできないの？」って聞かれたので、世の中の流れに反するので正直あまりオススメはしたくないんですが、タイトルに書いた通り期限を長くする方法について記載していきたいと思います。\nで、どうすればいいの？ # 結論を言うと、 期限の長い証明書を Azure AD の外部で作って、それを Azure AD へインポートする です。具体的な手順としては、、、\n Azure AD 外部の適切なところで SAML 署名証明書を作成する。詳細は割愛。 対象アプリケーションの SSO 設定画面から「証明書のインポート」を選択する。    インポートする証明書ファイルを選択する。    インポートした証明書をアクティブ化する。    証明書がアクティブになっていることを確認する。     てな感じで有効期限の長ーーーーい証明書にすることができましたｗ\n終わりに # ということで、今回は Azure AD でアプリケーションのシングルサインオンをする際に使用する SSL 証明書の期限を 3 年よりも長くする方法のご紹介でした。\n世の中の流れ（SSL 証明書の有効期限は徐々に短縮されていく）に反するので、あまり大きな声でできますというのは忍ばれるのですが、、、「どうしても証明書のロールオーバーは手間がかかるんでやりたくないんです！」というわがままバディさんには一応こういう方法もあるんですよ、とだけ言っておきたいと思います。\nちなみに補足ですが、Azure AD と SaaS アプリケーションの SSO 設定については Microsoft 社の公式ドキュメントに詳細がありますので、こちらをご参照いただければと思います。\n ","date":"May 10, 2021","permalink":"/posts/2021/05/azure-ad-with-long-lived-saml-certificate/","section":"記事一覧","summary":"みなさん、こんにちは。今回は Azure Active Directory(Azure AD) でアプリケーションと SAML 連携する際に使う SSL 証明書に関するお話です。","title":"Azure AD でアプリケーションと SAML 連携する際に証明書の期限を長くする方法"},{"content":"すでに Professional Cloud Architect だけは持っていたのですが、ACE の受験バウチャーを無償でいただく機会があったので「使わないなんてもったいない！」ということで 2021/05/06 に他の試験を受けるついでに受験したのでその時のメモです。\nAWS や Azure などをよく知っている人には多少は役に立つかもしれませんが、逆に、これからクラウドを一から勉強するという方にとってはあまり参考にならない情報かもしれませんのであしからず。\nまぁいっぱい情報転がっていると思うのでこんな記事は読まずに素直に他の記事を読みましょう。\n受験メモ # 受験者情報 #  AWS 資格 12 種コンプ Azure 資格 4 種 (AZ-900、AZ-104、AZ-400、AZ-303/304) GCP 資格 1 種 (PCA)  結果 #  合格 (点数非公開)  準備期間 #  4 時間強くらい  内訳 #  前準備、パートナーキックスタートに登録して Qwiklabs クエスト 2 種をクリアして無料で受験バウチャーをゲット (実質 3h、実際はバウチャー送付待ちなどで数 days) 試験ガイドを読んで不安な箇所がないかを確認する (0.2h)    公式の模擬試験(無料)を実施し、解説を読みこむ (1h)   感想 #  PCA を持っていたためか公式模擬試験を初見で 9 割くらい取れたのでそのまま GO で OK でした。とくに対策してません。参考にならんくてすんません。 試験時間は 2 時間ですが開始 40 分くらいの時点で試験会場を出てきたので、かなり時間には余裕があるかと思います。 個人的には AWS や Azure などのアソシ試験と比較して楽だったかな、と。AWS よりも出てくるサービスが少ないし、Azure のように細かい手順を問われたりもしないので。  アドバイスなど #  まずは模擬試験(30 問)をやってどこが弱いかを把握し、不足を補うように勉強しましょう。 試験ガイドに載っている各サービスがどんなことできるのかは把握しておきましょう。深い知識は不要。 AWS や Azure に詳しい方ならあらかじめ以下を見ておくといいかもしれないです。    べスプラに沿った回答を求められるケースが多いが、こちらは AWS や Azure の知識がだいたいそのまま流用可能です。  終わりに # ということで Associate Cloud Engineer (ACE)を取ってきました、、、だがしかし！！\nこちらの資格は上位資格である Professional Cloud Architect (PCA)の前提条件というわけでもないですし、PCA も感覚的には AWS などと比較してそこまで難しくはないので、普通の人はアソシをすっ飛ばして最初からプロを目指しちゃってもいいのかなと思います^^;\n  Google Cloud は、Google LLC の商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。  ","date":"May 7, 2021","permalink":"/posts/2021/05/gcp-associate-cloud-engineer-exam/","section":"記事一覧","summary":"すでに Professional Cloud Architect だけは持っていたのですが、ACE の受験バウチャーを無償でいただく機会があったので「使わないなんてもったいない！」ということで 2021/05/06 に他の試験を受けるついでに受験したのでその時のメモです。","title":"Google Cloud 認定 Associate Cloud Engineer 受験メモ"},{"content":"最近、待望の Google Cloud 認定 Professional Data Engineer のパートナー認定資格キックスタートプログラムが開始されました。ってことで、さっそくバウチャーを取得して 2021/05/06 に受験してきたので、その時のメモです。\nちなみに、参考までに費やした時間なども記載してますが個人差が大きいと思うのであくまでご参考です。\n受験メモ # 受験者情報 #  AWS 資格 12 種コンプ Azure 資格 4 種 (AZ-900、AZ-104、AZ-400、AZ-303/304) GCP 資格 1 種 (PCA)  結果 #  合格 (点数非公開)  準備期間 #  12 時間くらい  内訳 #  前準備、パートナーキックスタートに登録して Qwiklabs クエスト 2 種をクリアして無料で受験バウチャーをゲット (実質 4h、実際はバウチャー送付待ちなどで数 days) 合格者のブログを漁って勉強の計画などを立てる (1h) 試験ガイドを読んで不安な箇所がないかを確認する (0.2h)    公式の模擬試験(無料)を実施し、解説を読みこむ (1h)    高評価な問題集をやる (5-6h)  感想 #  模擬試験は初見で 6 割くらいだったので少し勉強すりゃ大丈夫かなと思い、評価のよさそうな問題集(英語)を 1 つ選んでやりました。 2 時間の試験でしたが 1 時間強くらいで見直し含めて終わりました。 日本語はしっかりしているから普通に受ける分には問題はないのだけど、英語で勉強した手前、AWS や Azure のように英語へ切り替えられない仕様なのがちょいとつらかった。  アドバイスなど #  まずは模擬試験(30 問)をやってどこが弱いかを把握し、不足を補うように勉強しましょう。 以下のブログにて試験のテクニック的なこと書いてくれてて結構参考になった気がします。    今回利用した教材は次の Udemy のこちら、キャンペーンで 1500 円くらいでした。こちらの問題集から似たような問題がちょいちょい本番でも出てきたので点数の底上げに役立つかと思います。余談ですが筆者は ★4.5 を付けました。有用だけど日本語じゃないから－★0.5 です。   終わりに # ということで Professional Data Engineer を取ってきました、、、しかもタダで！！ あ、Udemy で教材買ったんだった、、、\nもし Google さんとパートナーの企業のエンジニアの方は、ぜひパートナー認定資格キックスタートプログラムを利用してタダでチャレンジしてみてはいかがでしょうか。\n  Google Cloud は、Google LLC の商標または登録商標です。 その他、記載されている会社名および商品・製品・サービス名は、各社の商標または登録商標です。  ","date":"May 7, 2021","permalink":"/posts/2021/05/gcp-professional-data-engineer-exam/","section":"記事一覧","summary":"最近、待望の Google Cloud 認定 Professional Data Engineer のパートナー認定資格キックスタートプログラムが開始されました。ってことで、さっそくバウチャーを取得して 2021/05/06 に受験してきたので、その時のメモです。","title":"Google Cloud 認定 Professional Data Engineer 受験メモ"},{"content":"いやー最近(2021 年 4 月頃)、Professional Data Engineer の取得を目指して Dataflow を触って遊んでたら、こんな現象が何回も出てきて困ったんすよね、、、っていうことで共有します。\n現象 # Dataflow でジョブを流したら速攻でエラー終了するんです。そして、次のようなエラーメッセージがポロンって出てくるわけです。むーん。\nWorkflow failed. Causes: There was a problem refreshing your credentials. Please check: 1. Dataflow API is enabled for your project. 2. Make sure both the Dataflow service account and the controller service account have sufficient permissions. If you are not specifying a controller service account, ensure the default Compute Engine service account [PROJECT_NUMBER]-compute@developer.gserviceaccount.com exists and has sufficient permissions. If you have deleted the default Compute Engine service account, you must specify a controller service account. For more information, see: https://cloud.google.com/dataflow/docs/concepts/security-and-permissions#security_and_permissions_for_pipelines_on_google_cloud_platform. , There is no cloudservices robot account for your project. Please ensure that the Dataflow API is enabled for your project. チェックしてみてって言われてるから調べてみる、、、 # まずは「あれ、API 有効化してなかったけなー？」なんて思いながら「1. Dataflow API is enabled for your project.」を確認してみるわけですね。\n   、、、はい、もちろん有効化済みですよーっと。そらそうだ。\nでは次に「じゃあ、サービスアカウントの設定が変だったんだな、きっと！」なんて思いつつ「2. Make sure both the Dataflow service account and the controller service account have sufficient permissions.」の方も確認していきましょう。\n   、、、はい、Editor 権限を付与しておりとくに問題はなさそうに見えますよーっと。\nんー、オワタ、速攻で手詰まってしまったｗ やっぱり、Google Cloud のエラーメッセージはあてにならんことが多いよなぁ、、、\nってことで Stack Overflow を見に行くわけだよね # 、、、とここで、何かがフッと降りてきて、次の行動をとってみた、、、\nおもむろにジョブを再実行してみる # 「設定は正しいはずなんだけどなぁ」なんてことを思いながら、おもむろに再実行してみると、、、なんということでしょう。\nさっきはエラーになったジョブが動くんだ、なぜかこれが。そらぁ設定ちゃんとされてるもんね。むしろ、じゃあ、なんでさっきエラーになったんだよ！っていうね^^;\n終わりに # ということで今回も内容のない記事ですいませんでした。とはいえ、同じ現象に陥って困っている方はちょっと時間をおいて再実行してみても損はないのかな、と^^;\n","date":"April 30, 2021","permalink":"/posts/2021/04/gcp-dataflow-failed-refleshing-credentials/","section":"記事一覧","summary":"いやー最近(2021 年 4 月頃)、Professional Data Engineer の取得を目指して Dataflow を触って遊んでたら、こんな現象が何回も出てきて困ったんすよね、、、っていうことで共有します。","title":"Google Cloud の Dataflow でクレデンシャルの更新に失敗したというエラーが発生する件について"},{"content":"いやー最近(2021 年 3 月頃)、AWS と GCP で遊んでいたらこんな現象が出てきて困ったんすよね、、、ってことで共有します。ちなみに、2021/3/25 2021/7/21 時点でもまだ同じ現象は出ますね。\n現象 # 日本リージョン(東京/大阪)にデプロイした Google Cloud Functions から、日本からの受付のみ許可するように地域制限をかけた Amazon CloudFront にアクセスをしました。期待はもちろん成功なんですが、、、\n   ジャジャーン！はい、キタコレ、拒否！！、、、きっと仲が良くないん（ry\nCloudFront ってどうやって地理を判定しているの？ # 「Amazon CloudFront デベロッパーガイド」を見ると、サードパーティの GeoIP データベースを使っていますよってことです。あー、なるほど正確性は「99.8%」なのか。\n   てか、サードパーティの GeoIP データベースってどこよ？と思ったら「Amazon CloudFront API Reference」の方には MaxMind GeoIP databases って明記されておりました。\n   なるほど、じゃあ、ここのデータベースに Google Cloud Functions で利用されている IP アドレス範囲が入ってくればいいのか、、、まぁ AWS を疑ってたわけだけど AWS は全然悪くないのね。\n余談、どんなコード書いたの？ # 「Cloud Functions でどんなコード書いて試したの？」って聞かれたので「ん、こんな感じの雑なので試しましたよ」って答えておく。（でも、ほんとはググってほしい）\n作成例）main.py\ndef hello_world(request): import requests response = requests.get(\u0026#39;http://XXXXXXXXXXXXX/index.html\u0026#39;) return response.text 作成例）requirements.txt\nrequests 最後に # AWS から見るとやっぱ Google Cloud って外国扱いなんかな、と思ったら AWS 全然悪くないじゃない。疑ってすいませんでした。ということで、Google Cloud Functions の IP アドレスも MaxMind GeoIP データベースにいつの日か登録されることを祈っております。\n","date":"March 25, 2021","permalink":"/posts/2021/03/access-denied-from-cloud-functions-to-cloudfront/","section":"記事一覧","summary":"いやー最近(2021 年 3 月頃)、AWS と GCP で遊んでいたらこんな現象が出てきて困ったんすよね、、、ってことで共有します。ちなみに、2021/3/25 2021/7/21 時点でもまだ同じ現象は出ますね。","title":"Google Cloud Functionsから地域制限をかけたAmazon CloudFrontへアクセスすると拒否される件について"},{"content":"Google Cloud Certified Professional Cloud Architect 資格取得に向けた学習の一環で、Google Courses powered by Qwiklabs の「Cloud Architecture: Design, Implement, and Manage」のチャレンジラボクエストに挑戦しました。チャレンジラボの中にはクリア条件が不明瞭で手こずったラボもありましたので同様に詰まっている方の助けになればいいなぁと思います。\n Qwiklabs クエストを攻略する # 次の 7 つのチャレンジラボすべてをクリアするとクエスト攻略となります。\n   No. チャレンジラボ名     1 Google Cloud の基本スキル: チャレンジラボ（GSP101）   2 リモート起動スクリプトを使用した Compute インスタンスのデプロイ（GSP301）   3 Deployment Manager を使用したファイアウォールと起動スクリプトの構成（GSP302）   4 Windows の要塞ホストを使用したセキュアな RDP の構成（GSP303）   5 Windows の要塞ホストを使用したセキュアな RDP の構成（GSP303）   6 Kubernetes クラスタでのコンテナ化されたアプリケーションのスケールアウトと更新（GSP305）   7 MySQL データベースの Google Cloud SQL への移行（GSP306）    Lab1. Google Cloud の基本スキル: チャレンジラボ（GSP101） # 「GSP101」は、Compute Engine の VM インスタンスを作成して、手動でゲスト OS 上に apache2 をインストールするだけのラボです。苦労するところはとくにないかな、と。\n解答   Lab2. リモート起動スクリプトを使用した Compute インスタンスのデプロイ（GSP301） # 「GSP301」は、前のラボと同じ内容を起動スクリプトを使って行うラボです。具体的には、Compute Engine の VM インスタンスを作成する際に、startup-script-url オプションにスクリプトを配置した Cloud Storage の URL を指定させて自動で WEB サーバを構築します。\n注意点としては、スクリプトを格納した Cloud Storage へアクセスできるように Compute Engine へ権限付与するのを忘れずに、です。\n解答   Lab3. Deployment Manager を使用したファイアウォールと起動スクリプトの構成（GSP302） # 「GSP302」は、Deployment Manager を使って Compute Engine の VM インスタンスと Firewall をデプロイするラボです。\n筆者は、最後のチェックポイント「Check that Deployment manager includes startup script and firewall resources」がなかなか条件を満たせずに苦労しました。いや、環境自体はデプロイ成功して、外部から WEB アクセスも通っているんですがクリア扱いにならないという…。\n結論としては、.jinja ファイルに上記 2 つのリソース定義を含めろよって言っていたようで、Firewall のリソース定義を.yaml から.jinja ファイルへ持っていく必要がありました。（逆に.jinja の VM インスタンス定義を.yaml へ移動でもよかったのかも）\n解答   Lab4. Windows の要塞ホストを使用したセキュアな RDP の構成（GSP303） # 「GSP303」は、踏み台サーバ（Bastion）を用意してその踏み台を経由してサーバをデプロイしていくラボです。\n文中にリージョンを制限される記載はないものの東日本リージョンではサブネット構築のチェックポイントをクリアできず、米国リージョンに変更する必要がありました。筆者は us-central1 を選択しました。\nちなみに、ラボを進めるにあたり GCP 上の VM インスタンスへの RDP 接続が必要になりますのでプロキシ環境化のクライアントからの実施はやや厳しいかもしれないです。\n解答   Lab5. Kubernetes クラスタへの Docker イメージのビルドとデプロイ（GSP304） # 「GSP304」は、Docker イメージのビルド、Container Registory へのイメージプッシュ、Kubernetes クラスタにアプリをデプロイという一覧の操作をしていくラボです。\nContainer Registory にプッシュする際はイメージ名にgcr.io/[PROJECT ID]/[IMAGE NAME]:[TAG NAME]とつける点に注意しましょう。\nk8s へのアプリデプロイは全部コマンドを手打ちし終わってから気づきましたが、manifests ディレクトリにサンプル yaml が入っていたのでこちらを編集してkubectl create -fしてもよかったのかもしれませんね。\n解答   Lab6. Kubernetes クラスタでのコンテナ化されたアプリケーションのスケールアウトと更新（GSP305） # 「GSP305」は、k8s へデプロイされているアプリのコンテナイメージ更新およびポッド数の変更操作をしていくラボです。\n操作については特筆する点はないかと思います。イメージ変更やレプリカ数の変更が簡単に Cloud Console（GUI）でもできてしまうのは他のメガクラウドと比べて GCP の優秀なところですね。\n解答   Lab7. MySQL データベースの Google Cloud SQL への移行（GSP306） # 「GSP306」は、Google Cloud SQL デプロイ、既存 MySQL サーバからのデータ移行、アプリで利用するデータベース切替の一連操作をしていくラボです。\nデータ移行のところは mysqldump コマンドでデータベースをエクスポートして Cloud Storage にアップロード、Cloud Console（GUI）を使って SQL へデータをインポートしていきましょう。それ以外の操作はチェックポイントの条件を満たしていけば特筆しなくてもクリアできるかと思います。\n解答   終わりに # やはりクラウドは触って試してみるのがお勉強として一番効率いいと思いますので、ぜひ皆さんも触ってみてください。いやーしかし、GCP もそうなんですがメガクラウドベンダーから提供されている教材は充実していますね。これらが無償で受けられるとは…まいったね、こりゃ。\nしかも、Google さんが提供するパートナー認定資格キックスタートプログラムを通じて、今回紹介した「Cloud Architecture: Design, Implement, and Manage」と、その前段のクエスト「Cloud Architecture」の 2 つをクリアすると Professional Cloud Architect 認定資格試験の無料バウチャーまでもらえます。太っ腹だなぁ＾＾；\n  ","date":"October 30, 2020","permalink":"/posts/2020/10/qwiklabs-quest-google-cloud-architecture/","section":"記事一覧","summary":"Google Cloud Certified Professional Cloud Architect 資格取得に向けた学習の一環で、Google Courses powered by Qwiklabs の「Cloud Architecture: Design, Implement, and Manage」のチャレンジラボクエストに挑戦しました。チャレンジラボの中にはクリア条件が不明瞭で手こずったラボもありましたので同様に詰まっている方の助けになればいいなぁと思います。","title":"Qwiklabs クエストの Cloud Architecture: Design, Implement, and Manage を攻略する"},{"content":"みなさん、こんにちは。今回は社内プロキシ環境などから eksctl コマンドや kubectl コマンドを使って Amazon EKS を操作するためのクライアント側の設定方法を記載していきたいと思います。\nプロキシ環境下でクラウドベースの開発をしているエンジニアさんはプロキシに泣かされるってこと多いんじゃないかなと思います。いや、ほんとプロキシ関連の調査で時間をとられるのつらいですよね^^;\n設定手順 # まずは AWS CLI のセットアップをします # 1. AWS CLI をインストールします # AWS 公式ドキュメントの通りに MSI インストーラをダウンロード＆実行してください。\n2. AWS 認証情報を設定します # 次のコマンドを実行して認証情報を設定します。\nPS C:\\\u0026gt; aws configure 3. AWS CLI のプロキシ設定をします # 次のコマンドを実行してから設定を適用するために PowerShell アプリを起動しなおします。\nPS C:\\\u0026gt; setx HTTP_PROXY http://\u0026lt;proxy username\u0026gt;:\u0026lt;proxy password\u0026gt;@\u0026lt;proxy hostname or ip address\u0026gt;:\u0026lt;proxy port\u0026gt; PS C:\\\u0026gt; setx HTTPS_PROXY http://\u0026lt;proxy username\u0026gt;:\u0026lt;proxy password\u0026gt;@\u0026lt;proxy hostname or ip address\u0026gt;:\u0026lt;proxy port\u0026gt; 次に EKS CLI ツールのセットアップをします # 次にパッケージマネージャの Chocolatey を使って eksctl や kubectl などをインストールしていきましょう。\n1. Chocolatey をインストールします # 公式サイトからインストールスクリプトをダウンロードし、PowerShell を管理者として起動して環境変数を設定した上でインストールスクリプトを実行します。\nPS C:\\\u0026gt; $env:chocolateyProxyLocation = \u0026#34;\u0026lt;proxy hostname or ip address\u0026gt;:\u0026lt;proxy port\u0026gt;\u0026#34; PS C:\\\u0026gt; $env:chocolateyProxyUser = \u0026#34;\u0026lt;proxy username\u0026gt;\u0026#34; PS C:\\\u0026gt; $env:chocolateyProxyPassword = \u0026#34;\u0026lt;proxy password\u0026gt;\u0026#34; PS C:\\\u0026gt; Set-ExecutionPolicy RemoteSigned PS C:\\\u0026gt; .\\$downloadPath\\install.ps1 2. Chocolatey のプロキシ設定をします # 次のコマンドを実行してプロキシ設定をします。\nPS C:\\\u0026gt; chocolatey config set proxy \u0026#34;\u0026lt;proxy hostname or ip address\u0026gt;:\u0026lt;proxy port\u0026gt;\u0026#34; PS C:\\\u0026gt; chocolatey config set proxyUser \u0026#34;\u0026lt;proxy username\u0026gt;\u0026#34; PS C:\\\u0026gt; chocolatey config set proxyPassword \u0026#34;\u0026lt;proxy password\u0026gt;\u0026#34; 3. eksctl などをインストールします # 次のコマンドを実行して eksctl などをインストールしていきましょう。\nPS C:\\\u0026gt; chocolatey install -y eksctl aws-iam-authenticator kubernetes-cli ということでここまででひとまずセットアップは終了です。\nでは既存の EKS クラスタへの接続確認をしましょう # 1. まずは kubeconfig を作成します # 次のコマンドを実行して kubeconfig（kubectl 接続設定ファイル）を作成します。\nPS C:\\\u0026gt; aws eks --region $region update-kubeconfig --name $cluster    $region、$cluster部分は自分の環境に沿った値へ変換して実行してください。  2. kubectl を実行してみよう # 次のコマンドなどを実行して情報が取得できたら終わりです。\nPS C:\\\u0026gt; kubectl get svc PS C:\\\u0026gt; kubectl get pods    「error: You must be logged in to the server (Unauthorized)」で kubectl がエラーになった場合は RABC ではじかれている可能性が高いです。スイッチロールするなりしてアクセスできる状態にした上で再実行してください。  終わりに # ということで、今回は Amazon EKS をプロキシ環境下の Windows 10 マシンから CLI 操作する方法でした。プロキシ環境で戦うエンジニアさんの力に少しでもなれれば幸いです。\n","date":"May 2, 2020","permalink":"/posts/2020/05/aws-eks-cli-through-proxy-from-windows10-client/","section":"記事一覧","summary":"みなさん、こんにちは。今回は社内プロキシ環境などから eksctl コマンドや kubectl コマンドを使って Amazon EKS を操作するためのクライアント側の設定方法を記載していきたいと思います。","title":"プロキシ環境下の Windows 10 マシンから Amazon EKS を CLI で操作する"},{"content":"みなさん、こんにちは。今回は社内プロキシ環境下から Amazon EC2 などの外部環境へ SSH でアクセスしたいといった場合の接続方法です。なお、今回は TeraTerm を利用した手順となっています。\n接続手順 # 1. TeraTerm をインストールする際に「TTProxy」にチェックして入れます\n   2. TeraTerm を起動して「設定 \u0026gt; プロキシ」を選択します\n   3. プロキシ情報を入力します\n   4. あとは「ファイル \u0026gt; 新しい接続」からいつも通りアクセスすれば OK です\n   終わりに # 今更感はありますが「プロキシ環境下の Windows 10 マシンから外部へ SSH でアクセスする方法」でした。ちなみに今のご時世、EC2 などへのアクセスであれば SSH ポートは開けずに System Manager セッションマネージャなどを使う、が正解だと思いますけどね。\n","date":"May 1, 2020","permalink":"/posts/2020/05/ssh-through-proxy-from-windows10-client/","section":"記事一覧","summary":"みなさん、こんにちは。今回は社内プロキシ環境下から Amazon EC2 などの外部環境へ SSH でアクセスしたいといった場合の接続方法です。なお、今回は TeraTerm を利用した手順となっています。","title":"プロキシ環境下の Windows 10 マシンから外部へ SSH でアクセスする"}]