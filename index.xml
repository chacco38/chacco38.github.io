<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>クラウドCoEの何でも屋</title><link>https://chacco38.github.io/</link><description>Recent content on クラウドCoEの何でも屋</description><generator>Hugo -- gohugo.io</generator><language>ja-jp</language><copyright>&amp;copy; 2022 Satoshi Matsuzawa</copyright><lastBuildDate>Wed, 16 Feb 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://chacco38.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>AWS Controllers for Kubernetesを使って各種AWSサービスをマニフェストファイルで管理しよう</title><link>https://chacco38.github.io/posts/2022/02/aws-controllers-for-kubernetes/</link><pubDate>Wed, 16 Feb 2022 00:00:00 +0000</pubDate><guid>https://chacco38.github.io/posts/2022/02/aws-controllers-for-kubernetes/</guid><description>はじめに みなさん、こんにちは。今回はさまざまなAWSサービスをKubernetesから管理できるようにするAWS Controllers for Kubernetesのお話です。
みなさんはAmazon EKSを活用してKubernetesクラスタをAWS上で動かすとなった際に、他のマネージドサービスの利用はどうされていますか。もちろんすべてKubernetes上で動かしてシステムを完結させるという選択肢もあるかと思いますが、やはり多くの方が他のAWSのマネージドサービスの併用も検討されるのではないでしょうか。その一方で、これら併用環境のコード化 (IaC、Infrastructure as Code) を実現しようとすると、Kubernetesアプリケーションの管理はHelm、AWSリソースの管理はTerraform、などという別々のツールでの管理になってしまいがちです。
そんな悩みを解決する1つの手段がAWS Load Balancer ControllerやAWS Controllers for KubernetesといったKubernetesクラスタ機能を拡張する各種コントローラの活用です。これらのコントローラを利用することで、AWSリソースについてもKubernetesマニフェストファイルで定義できるようになり、Kubernetes側に運用管理を寄せてシンプル化することが可能です。
今回はそのうちの1つ、さまざまなAWSサービスを管理できるようにするAWS Controllers for Kubernetesについて、簡単なサンプルを交えて紹介していきたいと思います。これからAmazon EKS上にアプリケーションを展開しようと考えている方は参考にしてみてはいかがでしょうか。
AWS Controllers for Kubernetesとは AWS Controllers for Kubernetesは、さまざまなAWSサービスをKubernetesクラスタから管理するためのKubernetes API拡張コントローラ群の総称です。このコントローラ群を活用することで、Kubernetesクラスタから直接AWSサービスの定義、作成を行うことが可能になり、アプリケーションとその依存関係にあるデータベース、メッセージキュー、オブジェクトストレージなどのマネージドサービスを含むすべてをKubernetesにて一元管理することが可能となります。なお、現時点のAWS Controllers for Kubernetesでは、次のAWSサービス向けコントローラがディベロッパープレビュー機能として利用可能となっています。
Amazon API Gateway V2 Amazon Application Auto Scaling Amazon DynamoDB Amazon ECR Amazon EKS Amazon ElastiCache Amazon EC2 Amazon MQ Amazon OpenSearch Service Amazon RDS Amazon SageMaker Amazon SNS AWS Step Functions Amazon S3 https://github.</description></item><item><title>Terraformを使ってGKE+ASMのマルチクラスタメッシュ環境を構築してみた</title><link>https://chacco38.github.io/posts/2022/02/gcp-deploy-asm-with-terraform/</link><pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate><guid>https://chacco38.github.io/posts/2022/02/gcp-deploy-asm-with-terraform/</guid><description>はじめに みなさん、こんにちは。以前に「複数リージョンのGKEクラスタとAnthos Service Meshでマルチクラスタメッシュ環境を構築してみた」という記事を書いたのですが、今回はその環境をTerraformを使って構築してみました。もしこれから「ASM環境をTerraformで」と検討している方は参考にしてみてはいかがでしょうか。
とはいえ、本記事の執筆時点(2022年1月末)ではTerraform公式モジュールがASMのv1.11以降に対応しておらず正直使いモノにならなかったこともあり、やや苦しい実装になってしまっています。素直にASMの導入以降はTerraform以外を使うのが良いかと思いますが、あくまで本記事はご参考ということでその点ご承知おきいただけると幸いです。
構築するシステムについて 次の図に示すように限定公開クラスを有効化した複数リージョンのGKEクラスタに対してAnthos Service Mesh(マネージドコントロールプレーン)を導入した環境となっています。なお、アプリケーションのコンテナについてはインフラとは異なるリポジトリで管理するのが一般的かと思うので今回は除外しています。
Terraformのサンプルコードを書いてみた それでは今回作成したTerraformのサンプルコードを紹介していきたいと思います。まずはディレクトリ構造ですが、今回はenvironmentsディレクトリ配下へ環境ごとにサブディレクトリを作成し、Workspaceは使わずに別ファイルとして管理する形を想定した作りにしてます。
ディレクトリ構成 . |-- environments | `-- poc | |-- backend.tf | |-- main.tf | `-- variables.tf `-- modules |-- networks | |-- main.tf | |-- variables.tf | `-- outputs.tf |-- gke | |-- main.tf | |-- variables.tf | `-- outputs.tf `-- asm |-- main.tf |-- variables.tf |-- scripts | |-- install.</description></item><item><title>AWSマネジメントコンソールの表示言語を変更する方法</title><link>https://chacco38.github.io/posts/2022/01/aws-console-language-settings/</link><pubDate>Thu, 13 Jan 2022 00:00:00 +0000</pubDate><guid>https://chacco38.github.io/posts/2022/01/aws-console-language-settings/</guid><description>はじめに みなさん、こんにちは。AWS マネジメントコンソールを使っていると、ごくごく稀に表示言語を代えたくなることはありませんか。私は日本語⇔英語を切り替えたくなるケースがたまにあります。
ただ、個人的に「設定」と言えばなんとなく右上の項目からできそうなイメージがあり、「あれ、設定ってどこから変えるんだっけ、、、アカウントへ飛んだ先にあったっけ、、、(答え、アカウントへ飛んだ先にはない)」みたいにウッカリ設定方法を忘れて途方にくれてしまうことがあります。みなさんはそんな経験ございませんかね？ということで自分への備忘も込め、今回は表示言語の設定方法について紹介していきたいと思います。
表示言語の変更方法 では早速答えですが、言語の設定方法はAWS マネジメントコンソールの左下部分にございます。(なるほど、ここだったか、、、)
終わりに いまさらの情報でしたがいかがだったでしょうか。こんな記事でもだれかの役に立っていただければ幸いです。以上、AWS マネジメントコンソールの表示言語を変更する方法でした。
AWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。</description></item><item><title>AWS CloudShell上のviエディタでインデントの自動挿入なしでペーストする方法</title><link>https://chacco38.github.io/posts/2021/12/aws-cloudshell-autoindent-settings/</link><pubDate>Mon, 20 Dec 2021 00:00:00 +0000</pubDate><guid>https://chacco38.github.io/posts/2021/12/aws-cloudshell-autoindent-settings/</guid><description>はじめに みなさん、こんにちは。AWS CloudShellを使っているとファイルにペーストした際、勝手にインデントが入って「あー！」っとなったことの一度くらいはあるのではないでしょうか。今回はそんなときの解決方法を紹介していきたいと思います。
インデントの自動挿入なしでペーストする方法 いくつか方法はありますが、解の1つは「ペーストモードを使う」です。編集モードへ入る前に :set paste もしくは :set paste! を実行しましょう。ペーストモードにすると出力例のようにインデントが追加されずに期待通りの動きになりますね。
起動時に自動で設定する方法 とはいえ、エディタを起動するたびに毎回ペーストモードの設定をするのは面倒です。そんなときは vim の設定ファイル ~/.vimrc を作成しましょう。これでエディタが起動した際に自動で適用されるようになります。めでたしめでたし。
~/.vimrc set paste 終わりに いまさらの情報でしたがいかがだったでしょうか。もちろんコードを編集するときは自動でインデントを追加してくれるのはうれしいのですが、個人的にはAWS CloudShell上ではクリップボードからコピーしてくることも結構多いのでデフォルトペーストモードにしておき、必要に応じて :set nopaste もしくは :set paste! しております。よろしければ参考にしていただければと思います。
以上、AWS CloudShell上のviエディタでインデントの自動挿入なしでペーストする方法でした。
AWS は、米国その他の諸国における Amazon.com, Inc. またはその関連会社の商標です。 その他、本資料に記述してある会社名、製品名は、各社の登録商品または商標です。</description></item><item><title>GKE クラスタ作成時のオプションで Anthos Service Mesh を有効化できるようになりました</title><link>https://chacco38.github.io/posts/2021/12/gcp-new-asm-installation-options/</link><pubDate>Thu, 16 Dec 2021 00:00:00 +0000</pubDate><guid>https://chacco38.github.io/posts/2021/12/gcp-new-asm-installation-options/</guid><description>はじめに みなさん、こんにちは。今回はタイトルに掲げたとおり、最近の更新で Google Cloud コンソール(GUI)から Google Kubernetes Engine(GKE) Standard クラスタを作成する際に、次のようなすてきなオプションがプレビュー機能として追加されました。今回はこのオプションを使ってどのような構成が作られるのか実験したので共有したいと思います。
いきなりですが、結論です！ In-cluster とマネージドコントロールプレーンのどちらで構築される？ 答え、マネージドコントロールプレーンにて構築されます。現時点でこの設定を変更することはできません。
Anthos Service Mesh のバージョンは？ 答え、GKE でリリースチャンネルを採用した場合は GKE と同じチャンネルになります。GKE で静的リリースを採用した場合は Reguler チャンネルとなります。現時点でこの設定を変更することはできません。
カスタム CA を扱うことはできるか？ 答え、扱えません。マネージドコントロールプレーンでの導入となるため、カスタム CA を扱うことができる Istio CA を選択することはできません。
限定クラスタにした場合は別途 15017/TCP を許可する必要があるか？ 答え、不要です。ルールを追加しなくてもサイドカー自動インジェクションは問題なく動きます。
Ingress ゲートウェイはデフォルトで作られるか？ 答え、Ingress ゲートウェイはデフォルトでは作られません。別途ユーザにてデプロイする必要があります。
終わりに 今回は GKE クラスタ作成時の Anthos Service Mesh 有効化オプションの実験結果の共有でしたがいかがだったでしょうか。
GKE と Anthos Service Mesh で採用するリリースチャンネルを変えたい、カスタム CA を使いたいといったケースでは従来どおり CLI を利用する必要がありますが、これらの要件がなければ GUI からポチポチするだけでとっても簡単に環境を作れるようになりそうですね。
warn 2021 年 12 月時点ではGUI の Anthos Service Mesh 有効化オプションはプレビュー段階であり、今回紹介した挙動から変わる可能性がありますのでご注意ください。 {</description></item><item><title>AWS Load Balancer Controller を使って ELB を Kubernetes のマニフェストファイルで管理しよう</title><link>https://chacco38.github.io/posts/2021/12/aws-load-balancer-controller/</link><pubDate>Tue, 14 Dec 2021 00:00:00 +0000</pubDate><guid>https://chacco38.github.io/posts/2021/12/aws-load-balancer-controller/</guid><description>はじめに みなさん、こんにちは。今回は Amazon Elastic Kubernetes Service(EKS) を利用する際に併せて利用したい AWS Load Balancer Controller のお話です。
みなさんは Amazon EKS を活用して Kubernetes クラスタを AWS 上で動かすとなった際に、他のマネージドサービスの利用はどうされていますか。もちろんすべて Kubernetes 上で動かしてシステムを完結させるという選択肢もあるかと思いますが、やはり多くの方が他の AWS のマネージドサービスの併用も検討されるのではないでしょうか。その一方で、これら併用環境のコード化 (IaC、Infrastructure as Code) を実現しようとすると、Kubernetes アプリケーションの管理は Helm で、AWS リソースの管理は Terraform で、などという別々のツールでの管理になってしまいがちです。
そんな悩みを解決する一つの手段が AWS Load Balancer Controller や AWS Controllers for Kubernetes といった Kubernetes クラスタ機能を拡張する各種コントローラの活用です。これらのコントローラを利用することで、AWS リソースについても Kubernetes マニフェストファイルで定義できるようになり、Kubernetes 側に運用管理を寄せてシンプル化することができます。
今回はそのうちの一つ、Elastic Load Balancing(ELB) を Kubernetes クラスタで管理できるようにする AWS Load Balancer Controller について、簡単なサンプルアプリケーションを交えて紹介していきたいと思います。これから Amazon EKS 上にアプリケーションを展開しようと考えている方は参考にしてみてはいかがでしょうか。
AWS Load Balancer Controller とは AWS Load Balancer Controller (旧AWS ALB Ingress Controller) は、ELB を Kubernetes クラスタから管理するためのコントローラです。このコントローラを活用することで、Kubernetes Ingress リソースとして L7 ロードバランサの Application Load Balancer(ALB) を、Kuberntes Service リソースとして L4 ロードバランサの Network Load Balancer(NLB) を利用することができるようになります。</description></item><item><title>単一リージョンの複数 GKE クラスタと Anthos Service Mesh でマルチクラスタメッシュ環境を構築してみた</title><link>https://chacco38.github.io/posts/2021/12/gcp-multi-asm-cluster/</link><pubDate>Thu, 09 Dec 2021 00:00:00 +0000</pubDate><guid>https://chacco38.github.io/posts/2021/12/gcp-multi-asm-cluster/</guid><description>はじめに みなさん、こんにちは。今回は単一リージョンに展開した複数 GKE クラスタを単一の Anthos Service Mesh 環境に追加し、GKE クラスタ間で負荷分散を行う方法についてご紹介していきたいと思います。
複数 GKE クラスタでマルチクラスタメッシュを構築することにより、片方の GKE クラスタを先にバージョンアップし、サービスメッシュのトラフィック制御を使ってバージョンアップしたクラスタ側に少量のトラフィックを流して問題がないことを確認しながら段階的に比重をあげていく、といった「基盤部分も含めたカナリアリリース」のユースケースも容易に実現できるようになる見込みです。
もちろん公式ドキュメントにもマルチクラスタメッシュの構築に関する記載はあるのですが、単にクラスタ間で分散されたことを確認しただけで終わっており、ルーティングの設定やメッシュの外からの通信に関する記載はなかったため、今回はここら辺も含めて一気通貫でご紹介したいと思います。もしこれから Anthos Service Mesh 環境の利用を検討している方は参考にしてみてはいかがでしょうか。
構築するシステムについて 次の図に示すように限定公開クラスタおよび承認済みネットワーク機能を有効化した単一リージョンの複数 GKE クラスタに対して Anthos Service Mesh (マネージドコントロールプレーン)を導入し、サービスメッシュ上でサンプルアプリケーションを動かしていきたいと思います。なお、今回の例では GKE、Anthos Service Mesh のいずれのリリースチャンネルについても安定性重視の Stable チャンネルを採用しています。
それでは構築していきましょう 公式ドキュメントを参考にしつつ、公式ドキュメントに書かれていない部分を補足しながら構築をしていきたいと思います。
https://cloud.google.com/service-mesh/docs/unified-install/gke-install-multi-cluster
Step1. VPC ネットワークの作成 まずは GKE ノードを配置する VPC ネットワークおよび東京リージョンにサブネットを作成します。今回の例では GKE ノードからプライベートネットワーク経由で Artifact Registry などの他のマネージドサービスへアクセスできるように限定公開の Google アクセスをオンにしています。
VPCネットワークの作成 # 環境変数の設定 export NETWORK=&amp;#34;matt-vpc&amp;#34; export SUBNET=&amp;#34;matt-private-vm&amp;#34; export LOCATION=&amp;#34;asia-northeast1&amp;#34; export IP_RANGE=&amp;#34;172.16.0.0/16&amp;#34; # VPC ネットワークの作成 gcloud compute networks create ${NETWORK} --subnet-mode=custom # サブネットの作成 gcloud compute networks subnets create ${SUBNET} \ --network=${NETWORK} --range=${IP_RANGE} --region=${LOCATION} \ --enable-private-ip-google-access プライベートネットワーク経由でインターネット上の Docker Hub などへ接続できるよう Cloud NAT も作成しておきます。</description></item><item><title>複数リージョンの GKE クラスタと Anthos Service Mesh でマルチクラスタメッシュ環境を構築してみた</title><link>https://chacco38.github.io/posts/2021/12/gcp-multi-region-asm-cluster/</link><pubDate>Thu, 09 Dec 2021 00:00:00 +0000</pubDate><guid>https://chacco38.github.io/posts/2021/12/gcp-multi-region-asm-cluster/</guid><description>はじめに みなさん、こんにちは。今回は複数のリージョンに展開する各 GKE クラスタを単一の Anthos Service Mesh 環境に追加し、GKE クラスタ間で負荷分散を行う方法についてご紹介していきたいと思います。
複数リージョンの GKE クラスタでマルチクラスタメッシュを構築することにより、予期しない大規模災害の発生にも耐えうる高い可用性と回復力の実現、エンドユーザからより近い位置への振り分けによるレイテンシの改善といったことが期待できるようになる見込みです。
もちろん公式ドキュメントにもマルチクラスタメッシュの構築に関する記載はあるのですが、単にクラスタ間で分散されたことを確認しただけで終わっており、ローカリティを意識したルーティングの設定やメッシュの外からの通信に関する記載はなかったため、今回はここら辺も含めて一気通貫でご紹介したいと思います。もしこれからリージョンをまたがった Anthos Service Mesh 環境の利用を検討している方は参考にしてみてはいかがでしょうか。
構築するシステムについて 次の図に示すように限定公開クラスタおよび承認済みネットワーク機能を有効化した複数リージョンの GKE クラスタに対して Anthos Service Mesh (マネージドコントロールプレーン)を導入しています。サービスメッシュ上ではサンプルアプリケーションを動かし、ローカリティを意識した負荷分散についても設定をしていきたいと思います。なお、今回の例では GKE、Anthos Service Mesh のいずれのリリースチャンネルについても安定性重視の Stable チャンネルを採用しています。
それでは構築していきましょう いつも通り公式ドキュメントを参考にしつつ、公式ドキュメントに書かれていない部分を補足しながら構築をしていきたいと思います。
https://cloud.google.com/service-mesh/docs/unified-install/gke-install-multi-cluster
Step1. VPC ネットワークの作成 まずは GKE ノードを配置する VPC ネットワークおよび東京リージョンと大阪リージョンにサブネットを作成します。今回の例では GKE ノードからプライベートネットワーク経由で Artifact Registry などの他のマネージドサービスへアクセスできるように限定公開の Google アクセスをオンにしています。
VPCネットワークの作成 # 環境変数の設定 export NETWORK=&amp;#34;matt-vpc&amp;#34; export SUBNET=&amp;#34;matt-private-vm&amp;#34; export LOCATION_1=&amp;#34;asia-northeast1&amp;#34; export LOCATION_2=&amp;#34;asia-northeast2&amp;#34; export IP_RANGE_1=&amp;#34;172.16.0.0/16&amp;#34; export IP_RANGE_2=&amp;#34;172.24.0.0/16&amp;#34; # VPC ネットワークの作成 gcloud compute networks create ${NETWORK} --subnet-mode=custom # サブネットの作成 (東京リージョン) gcloud compute networks subnets create ${SUBNET} \ --network=${NETWORK} --range=${IP_RANGE_1} --region=${LOCATION_1} \ --enable-private-ip-google-access # サブネットの作成 (大阪リージョン) gcloud compute networks subnets create ${SUBNET} \ --network=${NETWORK} --range=${IP_RANGE_2} --region=${LOCATION_2} \ --enable-private-ip-google-access プライベートネットワーク経由でインターネット上の Docker Hub などへ接続できるよう Cloud NAT も作成しておきます。</description></item><item><title>GHEC Audit Log CLI を使って GitHub Enterprise Cloud の監査ログを取得してみた</title><link>https://chacco38.github.io/posts/2021/12/ghec-audit-log-cli/</link><pubDate>Tue, 07 Dec 2021 00:00:00 +0000</pubDate><guid>https://chacco38.github.io/posts/2021/12/ghec-audit-log-cli/</guid><description>はじめに みなさん、こんにちは。今回は GitHub Enterprise Cloud(GHEC) の監査ログ(Audit Log) の取得方法についてのお話です。
GHEC 監査ログの取得方法としてはいくつか方法はあるのですが、この記事では GHEC の監査ログを取得するためのコマンドラインインタフェースである GHEC Audit Log CLI を使った方法をご紹介していきたいと思います。
https://github.com/github/ghec-audit-log-cli
GHEC Audit Log CLI を使ってみよう 今回は Linux(AWS CloudShell) 上に環境を作って試しに実行してみるところからはじめて、定期的に監査ログ取得を行う自動化フローの構築まで紹介していきたいと思います。
ローカル環境で実行してみよう Step1. 前提パッケージのインストール GHEC Audit Log CLI の前提パッケージとして Node.js が必要となります。GitHub からソースコードを入手する必要があるため、git コマンドと併せてインストールしましょう。
前提パッケージの導入 $ curl --silent --location https://rpm.nodesource.com/setup_16.x | sudo bash - $ sudo yum install -y nodejs git Step2. GHEC Audit Log CLI のインストール GitHub からソースコードを取得し、npm コマンドを使って GHEC Audit Log CLI をインストールします。最後の ghce-audit-log-cli -v コマンドにてバージョン情報が出力されれば CLI のインストールは完了です。</description></item><item><title>GKE Autopilot と Anthos Service Mesh を使ってフルマネージドなサービスメッシュ環境を構築してみた</title><link>https://chacco38.github.io/posts/2021/12/gcp-asm-with-gke-autopilot/</link><pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate><guid>https://chacco38.github.io/posts/2021/12/gcp-asm-with-gke-autopilot/</guid><description>はじめに みなさん、こんにちは。今回は Google Cloud が提供するマネージドサービスメッシュサービスの Anthos Service Mesh に関するお話です。
Anthos Service Mesh はここ半年で「マネージドコントロールプレーン機能の一般公開」、「マネージドデータプレーン機能のプレビュー公開」と Google マネージドの範囲を徐々に広げてきましたが、2021 年 11 月 19 日の更新でプレビュー段階ではありますが「Google Kubernetes Engine(GKE) Autopilot 上でも Anthos Service Mesh を利用できる」ようになりました。
今回はそんなプレビュー公開されたばかりの GKE Autopilot と Anthos Service Mesh(ASM) を使った Kubernetes 部分も含めてフルマネージドなサービスメッシュ環境を構築していきたいと思います。
構築するシステムについて 次の図に示すように限定公開クラスタおよび承認済みネットワーク機能を有効化した GKE Autopilot クラスタに対して Anthos Service Mesh を導入し、サービスメッシュ上でサンプルアプリケーションを動かしていきたいと思います。
それでは構築していきましょう いつも通り公式ドキュメントを参考にしつつ、公式ドキュメントに書かれていない部分を補足しながら構築をしていきたいと思います。
https://cloud.google.com/service-mesh/docs/unified-install/managed-asmcli-experimental
Step1. VPC ネットワークの作成 まずは GKE ノードを配置する VPC ネットワークおよび東京リージョンにサブネットを作成します。今回の例では GKE ノードからプライベートネットワーク経由で Artifact Registry などの他のマネージドサービスへアクセスできるように限定公開の Google アクセスをオンにしています。
プライベートネットワークからインターネット上の Docker Hub などへ接続できるよう Cloud NAT リソースも作成しておきたいと思います。</description></item><item><title>Microsoft Sentinel を使って GitHub Enterprise Cloud のセキュリティを強化しよう (Azure Logic Apps コネクタ編)</title><link>https://chacco38.github.io/posts/2021/11/azure-sentinel-logicapps-data-connector-for-ghec/</link><pubDate>Fri, 26 Nov 2021 00:00:00 +0000</pubDate><guid>https://chacco38.github.io/posts/2021/11/azure-sentinel-logicapps-data-connector-for-ghec/</guid><description>はじめに みなさん、こんにちは。今回は GitHub Enterprise Cloud(GHEC) の各種ログを SIEM1 マネージドサービスである Microsoft Sentinel (旧称 Azure Sentinel) に集約し、サイバー攻撃の兆候を検知できるようにする方法をご紹介していきたいと思います。
Microsoft Sentinel と GHEC との連携方法はいくつかあるのですが、この記事では Microsoft Sentinel コミュニティで開発している Azure Logic Apps(ロジックアプリ)、Azure Functions(関数アプリ)の 2 種類のカスタムデータコネクタの内、「Azure Logic Apps コネクタ」を使った方法をご紹介していきたいと思います。
これから GHEC の利用を検討している方や、GHEC は利用しているけれど SIEM システムの導入まではしていないという方は、セキュリティ強化策のひとつとして参考にしてみてはいかがでしょうか。
構築するシステムについて 今回は Azure Logic Apps(ロジックアプリ)を定期的に起動し、GHEC から監査ログなどを取得して Microsoft Sentinel ワークスペースへ格納、格納されたログに対して Microsoft Sentinel が自動的に相関分析をかけていく、といった流れで処理を行うシステムを構築していきたいと思います。
Azure Sentinel とのコネクタについては、今回は Microsoft Sentinel コミュニティで公開されている次のカスタムデータコネクタを利用していきます。
https://github.com/Azure/Azure-Sentinel/tree/master/DataConnectors/GitHub
本カスタムコネクタをデプロイすると、次の 3 種類の Azure Logic Apps リソースが動作するようになります。
リソース種別 説明 Audit Playbook 監査ログを定期的に収集する自動ワークフロー (デフォルト 5 分間隔) Repo Playbook 各リポジトリに対するフォーク、クローン、コミットなどの操作ログを定期的に収集する自動ワークフロー (デフォルト 1 時間間隔) Vulnerability Alert Playbook 各リポジトリに対するセキュリティ脆弱性診断ログを定期的に収集する自動ワークフロー (デフォルト 1 日間隔) また、本カスタムコネクタで取得した各種ログデータについては、Log Analytics ワークスペースの次のカスタムテーブルへ格納されるようになります。</description></item><item><title>Microsoft Sentinel を使って GitHub Enterprise Cloud のセキュリティを強化しよう (Azure Functions コネクタ編)</title><link>https://chacco38.github.io/posts/2021/11/azure-sentinel-functions-data-connector-for-ghec/</link><pubDate>Thu, 25 Nov 2021 00:00:00 +0000</pubDate><guid>https://chacco38.github.io/posts/2021/11/azure-sentinel-functions-data-connector-for-ghec/</guid><description>はじめに みなさん、こんにちは。今回は GitHub Enterprise Cloud(GHEC) の各種ログを SIEM1 マネージドサービスである Microsoft Sentinel (旧称 Azure Sentinel) に集約し、サイバー攻撃の兆候を検知できるようにする方法をご紹介していきたいと思います。
Microsoft Sentinel と GHEC との連携方法はいくつかあるのですが、この記事では Microsoft Sentinel コミュニティで開発している Azure Logic Apps(ロジックアプリ)、Azure Functions(関数アプリ)の 2 種類のカスタムデータコネクタの内、「Azure Functions コネクタ」を使った方法をご紹介していきたいと思います。
これから GHEC の利用を検討している方や、GHEC は利用しているけれど SIEM システムの導入まではしていないという方は、セキュリティ強化策のひとつとして参考にしてみてはいかがでしょうか。
構築するシステムについて 今回は Azure Functions(関数アプリ)を定期的に起動し、GHEC から監査ログなどを取得して Microsoft Sentinel ワークスペースへ格納、格納されたログに対して Microsoft Sentinel が自動的に相関分析をかけていく、といった流れで処理を行うシステムを構築していきたいと思います。
Azure Sentinel とのコネクタについては、今回は Microsoft Sentinel コミュニティで公開されている次のカスタムデータコネクタを利用していきます。
https://github.com/Azure/Azure-Sentinel/blob/master/DataConnectors/GithubFunction
本カスタムコネクタで取得した各種ログデータについては、Log Analytics ワークスペースの次のカスタムテーブルへ格納されるようになります。
テーブル名 説明 GitHub_CL 監査ログのデータを格納するテーブル GitHubRepoLogs_CL 各リポジトリに対するフォーク、クローン、コミットなどの操作ログやリポジトリに対するセキュリティ脆弱性診断ログのデータを格納するテーブル それでは構築していきましょう 今回は GitHub Enterprise Cloud → Microsoft Sentinel ワークスペース → カスタムコネクタ → Microsoft Sentinel の順で設定していきます。</description></item><item><title>TaskCatを使ってCloudFormationテンプレートの自動テストをしよう</title><link>https://chacco38.github.io/posts/2021/10/aws-taskcat/</link><pubDate>Wed, 20 Oct 2021 00:00:00 +0000</pubDate><guid>https://chacco38.github.io/posts/2021/10/aws-taskcat/</guid><description>はじめに みなさん、こんにちは。今回は TaskCat というオープンソースを利用した AWS CloudFormation (CFn) テンプレートの自動テストについてのお話です。
CFn テンプレートを扱っていると構文エラーチェックはパスしたものの、いざ動かしてみたらスタックの作成でエラーになってしまうといった経験をすることがあるかと思います。TaskCat は多くの方にとってあまり馴染みのないツールだと思いますが、実際に使ってみるととても手軽に CFn テンプレートの自動テストをすることができます。
今回は Linux 上に開発環境を作るところからはじめて、簡素なサンプルを用いたテストの実行、テスト自動化を組み込んだシンプルな CI/CD パイプラインの構築まで紹介していきたいと思います。これから CFn テンプレート開発されている方で自動テストをやりたいと考えている方は参考にしてみてはいかがでしょうか。
TaskCat とは TaskCat とは、AWS CloudFormation (CFn) テンプレートの自動テストを行う Python 製のテストツールです。
このツールを利用することで、指定した各リージョンに CFn テンプレートから環境を一時的にデプロイ、各リージョンでのデプロイ可否結果のレポート生成、テストで一時的に作成した環境を削除、といった一連の流れを自動化することができます。
なお、AWS TackCat はローカルでテストを実行する際に Docker が必要となるため、Docker をサポートしていない AWS CloudShell では利用することができないのでご注意ください。
https://github.com/aws-quickstart/taskcat
TaskCat を使ってみよう 「はじめに」で既に述べたとおり、今回は Linux 上に開発環境を作るところからはじめて、簡素なサンプルを用いたテストの実行、テスト自動化を組み込んだシンプルな CI/CD パイプラインの構築まで紹介していきたいと思います。
まずは開発環境の設定から それでは Linux 上に開発環境を作っていきたいと思います。まず TaskCat をインストールする事前準備として Python の仮想環境を作成していきましょう。なお、今回の例で使用している Linux ディストリビューションは Amazon Linux 2 です。
$ sudo yum install -y python3 $ python3 --version Python 3.</description></item><item><title>AWS Chaliceを使ってサーバレスアプリケーションを開発しよう</title><link>https://chacco38.github.io/posts/2021/10/aws-chalice/</link><pubDate>Mon, 18 Oct 2021 00:00:00 +0000</pubDate><guid>https://chacco38.github.io/posts/2021/10/aws-chalice/</guid><description>はじめに みなさん、こんにちは。今回は「AWS Chalice」を活用したサーバレスアプリケーション開発についてのお話です。AWS Summit Online Japan 2021 の日立製作所の講演で軽く触れたこともあり、どこかで紹介したいと思っておりました。
さて、AWS Chalice は多くの方にとってあまり馴染みのないツールだと思いますが、実際に使ってみるととても手軽に Amazon API Gateway と AWS Lambda を使用するサーバレスアプリケーションを作成してデプロイすることができます。もちろん、どんなツールにも得手不得手はあるので「すべてのプロジェクトで AWS Chalice の活用が最適か？」と言われればもちろん「No！」なのですが、ちょっと試しに動く Web API をサッと手軽に作りたい、といったケースにはとても良いソリューションだと思います。
今回は AWS CloudShell 上に開発環境を作るところからはじめて、簡素なサンプルを用いた一連の開発の流れ、テスト自動化を組み込んだシンプルな CI/CD パイプラインの構築まで紹介していきたいと思います。これから実際に動くサーバレスアプリケーションを手軽に作りたいと思われている方は参考にしてみてはいかがでしょうか。
改めて AWS Chalice とは AWS Chalice とは、Amazon AWS Gateway や AWS Lambda を用いたサーバレスアプリケーションを、お手軽に開発できるようにする Python 製のサーバレスアプリケーションフレームワークです。具体的には、Web API をシンプルで直感的なコードで実装できるようにする機能や、作成したコードからアプリケーションの作成やデプロイを実行するコマンドラインインタフェース(CLI)といった開発者にやさしい機能を提供してくれます。
普段から Python に触れている方であれば似たような機能として Flask や Bottle をイメージされるかと思いますが、これらにサーバレス環境へデプロイする機能が追加で付与されたものが AWS Chalice、とイメージしていただくと良いのかなと思います。
https://github.com/aws/chalice
ではさっそく AWS Chalice を使ってみよう 「はじめに」で既に述べたとおり、今回は AWS CloudShell 上に開発環境を作るところからはじめて、簡素なサンプルを用いた一連の開発の流れ、テスト自動化を組み込んだシンプルな CI/CD パイプラインの構築まで紹介していきたいと思います。
まずは開発環境の構築から 今回は AWS CloudShell 上に開発環境を作っていきたいと思います。 それではまず AWS Chalice をインストールする事前準備として Python の仮想環境を作成していきましょう。</description></item></channel></rss>